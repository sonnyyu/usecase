actions_UBASeverity,cardinalityTest,change_type,description,examples_name,label,outlierPeerGroup,outlierValueTracked1,outlierValueTracked2,outlierVariable,outlierVariableSubject,prereqs_field,prereqs_greaterorequalto,prereqs_name,prereqs_override_auto_finalize,prereqs_resolution,prereqs_test,value
,index=*  sourcetype=cisco:esa* OR sourcetype=MSExchange*:MessageTracking OR tag=email src_user=* | bucket _time span=1d | stats dc(src_user) as count by _time ,,"First we pull in our email dataset.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Finally, we can count and aggregate per src_ip, per day.",Spike in Email from Address - Live,Spike in Email from Address - Live,,,,count,sender,count,1,Must have Email Data,,"This search requires Email data. The out of the box field extractions support the Common Information Model, including Cisco ESA/Ironport and Microsoft Exchange. If you don't have this data today, we highly recommend ingesting it with the <a href=""https://splunkbase.splunk.com/app/1761/"">Cisco ESA TA</a> or the <a href=""https://splunkbase.splunk.com/app/3225/"">Splunk Add-on for Microsoft Exchange</a>. For best performance, accelerate the email data model from the <a href=""https://splunkbase.splunk.com/app/1621/"">Common Information Model</a>!",| tstats count where index=* sourcetype=cisco:esa* OR sourcetype=MSExchange*:MessageTracking OR tag=email earliest=-4h,"index=* sourcetype=cisco:esa* OR sourcetype=MSExchange*:MessageTracking OR tag=email src_user=*
| bucket _time span=1d 
| stats count by src_user, _time"
,"index=* sourcetype=""WinEventLog:Security"" Logon_Type=2 OR Logon_Type=10 OR Logon_Type=11 Logon Type TaskCategory=Logon Audit Success | bucket _time span=1d | stats dc(dest) as count by _time",,"First we pull in our dataset of Windows Authentication events specifying interactive logon types.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Finally, we can count and aggregate per system, per day.",Increase in Interactively Logged In Users - Live,Increase in Interactively Logged In Users - Live,,,,count,dest,count,1,Must have Windows Security data,,"This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.","| metasearch earliest=-2h latest=now index=* sourcetype=""WinEventLog:Security"" | head | stats count ","index=* sourcetype=""WinEventLog:Security"" Logon_Type=2 OR Logon_Type=10 OR Logon_Type=11 Logon Type TaskCategory=Logon Audit Success 
| bucket _time span=1d 
| stats dc(user) as count by _time dest"
,| tstats summaries_only=t allow_old_summaries=t count from datamodel=Email by _time span=1d | eval count=1,,"Here, tstats is pulling in one command a super-fast count of emails where the subject contains ""Password Reset"" per src_ip, per day.
We're adding a Password Reset tag to this. You could also expand this out for multiple items, including new phishing campaigns.",Spike in Password Reset Emails - Accelerated,Spike in Password Reset Emails - Accelerated,,,,count,Detect_Type,"count
count
count","1
1
1","Must have an Email data model
Must have an Accelerated Email data model
Must have Subjects (All_Email.subject) in your Accelerated Email data model",,"This search requires an Email data. This is dependent on the <a href=""https://splunkbase.splunk.com/app/1621/"">Common Information Model</a> being present, and also having your data mapped to CIM via appropriate TAs, usually with the out of the box field extractions from the <a href=""https://splunkbase.splunk.com/app/1761/"">Cisco ESA TA</a>, the <a href=""https://splunkbase.splunk.com/app/3225/"">Splunk Add-on for Microsoft Exchange</a>, etc.
This search requires an accelerated Email data. In order to run a fast accelerated search, you should accelerate your data model. (<a href=""https://docs.splunk.com/Documentation/Splunk/latest/HadoopAnalytics/Configuredatamodelacceleration#Accelerate_the_data_model"">docs</a>)
This search assumes that you have actual source email addresses -- check your field extractions for src_user and then rebuild your data models if not.","| tstats summaries_only=f allow_old_summaries=t count from datamodel=Email where earliest=-1h
| tstats summaries_only=t allow_old_summaries=t count from datamodel=Email where earliest=-1h
| tstats summaries_only=t allow_old_summaries=t dc(All_Email.subject) as count from datamodel=Email where earliest=-1h","| tstats summaries_only=t allow_old_summaries=t count from datamodel=Email where All_Email.subject=""*Password Reset*"" by _time span=1d 
| eval Detect_Type=""Password Reset"""
,index=* sourcetype=Cerner | bucket_time span=1d | stats dc(prsnl_name) as count by  _time,,"First we pull in our Cerner audit log dataset.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Finally, we can count and aggregate per user, per day.",Number of Unique Patient Records Viewed Per Day - Live,Number of Unique Patient Records Viewed Per Day - Live,,,,count,prsnl_name,"count
count
count","1
1
1","Must have Cerner data (or similar healthcare data)
Must have a prsnl_id defined in your data
Must have a ""event_list.participants.person_id"" defined in your data",,"While this use case applies to any similar data, our sample search is looking for a sourcetype of Cerner somewhere. 
You should have a field called ""prsnl_id"" defined in your Cerner logs. If that's not currently extracted, build an extraction for it (or do an inline rex in the SPL below to work around this).
You should have a field called ""event_list.participants.person_id"" defined in your Cerner logs. If that's not currently extracted, build an extraction for it (or do an inline rex in the SPL below to work around this).","| metasearch earliest=-24h latest=now index=* sourcetype=Cerner | head 100 | stats count 
earliest=-2h latest=now index=* sourcetype=Cerner| head 100 | stats dc(prsnl_id) as count 
earliest=-2h latest=now index=* sourcetype=Cerner| head 100 | stats dc(""event_list.participants.person_id"") as count ","index=* sourcetype=Cerner 
| bucket_time span=1d 
| stats dc(""event_list.participants.person_id"") as count by prsnl_name  _time"
,"| `Load_Sample_Log_Data(""Email Logs"")`| bucket _time span=1d | stats values(eval(""1"")) by _time ",,"First we pull in our demo dataset.
Based on the message subject, tag it with a value for Detect_Type
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Finally, we can count and aggregate per detection type tag, per day.",Spike in Password Reset Emails - Demo,Spike in Password Reset Emails - Demo,,,,count,Detect_Type,Anonymized_Email_Logs.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| `Load_Sample_Log_Data(""Email Logs"")`
| eval Detect_Type="""", Detect_Type=case(LIKE(Subject, ""%Password Reset%""), ""Password Reset"", LIKE(Subject, ""%Validate Credentials%""), ""Validate Credentials"",1=1, null) 
| bucket _time span=1d 
| stats count by _time Detect_Type "
,index=* sourcetype=aws:cloudtrail eventName=ConsoleLogin OR eventName=CreateImage OR eventName=AssociateAddress OR eventName=AttachInternetGateway OR eventName=AttachVolume OR eventName=StartInstances OR eventName=StopInstances OR eventName=UpdateService OR eventName=UpdateLoginProfile | bucket _time span=1d | stats dc(user) as count by _time,,"First we bring in our basic dataset, AWS CloudTrail logs that are filtered for interesting APIs.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Next we use stats to summarize the number of events per user per day.",AWS APIs Called More Often Than Usual Per Account - Live,AWS APIs Called More Often Than Usual Per Account - Live,,,,count,user,count,1,Must have AWS CloudTrail data,,"In order to run this search, you must have AWS CloudTrail data onboard. Visit the <a href=""/app/Splunk_Security_Essentials/data_source?technology=AWS%20CloudTrail"">data onboarding guide for AWS CloudTrail in this app</a>, or browse to <a href=""https://splunkbase.splunk.com/app/1876/"">apps.splunk.com</a> for more information.",| tstats count where earliest=-2h latest=now index=* sourcetype=aws:cloudtrail,"index=* sourcetype=aws:cloudtrail eventName=ConsoleLogin OR eventName=CreateImage OR eventName=AssociateAddress OR eventName=AttachInternetGateway OR eventName=AttachVolume OR eventName=StartInstances OR eventName=StopInstances OR eventName=UpdateService OR eventName=UpdateLoginProfile 
| bucket _time span=1d 
| stats count by user _time"
,"| `Load_Sample_Log_Data(""SFDC Data"")` | search EVENT_TYPE=DocumentAttachmentDownloads | bucket _time span=1d | stats values(eval(""1"")) as count by _time ",,"First we pull in our demo SFDC dataset.
Then we filter for what we're looking for in this use case, specifically the DocumentAttachmentDownloads EVENT_TYPE.
Then we enrich to convert the SFDC USER_ID into a friendly username via a lookup.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Finally, we can count and aggregate per user, per day.",Spike in SFDC Documents Downloaded - Demo,Spike in SFDC Documents Downloaded - Demo,,,,count,USER_NAME,SFDC_Sample_Data_Anon.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| `Load_Sample_Log_Data(""SFDC Data"")` 
| search EVENT_TYPE=DocumentAttachmentDownloads 
| lookup SFDC_User_Lookup USER_ID 
| bucket _time span=1d 
| stats count by USER_NAME _time"
,"|  `Load_Sample_Log_Data(""Windows Logon Activity"")`| bucket _time span=1d | stats dc(user) as count by _time ",,"First we pull in our demo dataset.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Finally, we can count and aggregate per user, per day.",Unique Hosts Logged Into Per Day - Demo,Unique Hosts Logged Into Per Day - Demo,,,,count,user,Sampled_AnonymizedLogonActivity.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","|  `Load_Sample_Log_Data(""Windows Logon Activity"")`
| bucket _time span=1d 
| stats dc(anonymized_ComputerName) as count by user _time"
,| inputlookup UC_smb_spike_detection | search (dest_port=139 OR dest_port=445) | bucket _time span=1d | stats dc(src_ip) as count by _time ,,"First we pull in our demo dataset of Firewall logs
Next we filter for just SMB connections.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the last day.
Now we are looking at the number of unique destinations per source IP, per day.",Detect Spike in SMB Traffic - Demo,Detect Spike in SMB Traffic - Demo,,,,count,src_ip,UC_smb_spike_detection.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| inputlookup UC_smb_spike_detection 
| search (dest_port=139 OR dest_port=445) 
| bucket _time span=1d 
| stats dc(dest_ip) as count by src_ip, _time"
,(tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)  | bucket _time span=1d | stats dc(src_ip) as count by _time ,,"First we pull in our Firewall dataset (we should specify a single index and sourcetype in production environments).
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Finally, we can count and aggregate per src_ip, per day.",Distinct Hosts Communicated With Per Day - Live,Distinct Hosts Communicated With Per Day - Live,,,,count,src_ip,"count
count","1
1","Must have Firewall data
Must have a src_ip and dest_ip field",,"This search requires Firewall or Netflow data to run. By default, we're checking for Common Information Model compliant data, and then also manually specifying the standard sourcetypes for Check Point, Palo Alto Networks, and Cisco ASAs. You should specify your particular index and sourcetype in the actual search to improve performance (or better yet, accelerate with the common information model!)
This search is also looking for firewall logs, but with the added filter of making sure that a src_ip and dest_ip is defined.","(tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)| head 100 | stats count 
((tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)) src_ip=* dest_ip=* | head 100 | stats count ","(tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)  
| bucket _time span=1d 
| stats dc(dest_ip) as count by src_ip, _time"
7,"| `Load_Sample_Log_Data(""Printer Logs"")` | bucket _time span=1d | stats dc(User) as count by _time ",,"First we pull in our demo dataset.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Finally, we can count and aggregate per user, per day.",Pages Printed Per User Per Day - Demo,Pages Printed Per User Per Day - Demo,,,,Pages,User,uniflow_printer_log_sample.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| `Load_Sample_Log_Data(""Printer Logs"")`  
| bucket _time span=1d 
| stats sum(Page_Count) as Pages by User _time"
,| tstats summariesonly=t allow_old_summaries=t dc(All_Traffic.src_ip) as count from datamodel=Network_Traffic by _time span=1d,,"Here, tstats is pulling in one command a super-fast count per user, per day.
I usually like to rename data model fields after my tstats, as it becomes much easier to use.",Unique Hosts Logged Into Per Day - Accelerated,Unique Hosts Logged Into Per Day - Accelerated,,,,count,user,"count
count
count","1
1
1","Must have an accelerated Authentication data model
Authentication data model must have a field called dest defined
Authentication data model must have the user field defined",,"This search requires an accelerated authentication data model to run. If it is not present, consider ingesting Windows Security data via the Splunk Universal Forwarder, and then accelerating it with the Common Information App from <a href=""http://apps.splunk.com/"">apps.splunk.com</a>.
You should have a field called ""dest"" defined in your accelerated Authentication data model (referenced in tstats as Authentication.dest). If it is not present, consider ingesting Windows Security data via the Splunk Universal Forwarder, and then accelerating it with the Common Information App from <a href=""http://apps.splunk.com/"">apps.splunk.com</a>.
You should have a field called ""user"" defined in your accelerated Authentication data model (referenced in tstats as Authentication.user). If it is not present, consider ingesting Windows Security data via the Splunk Universal Forwarder, and then accelerating it with the Common Information App from <a href=""http://apps.splunk.com/"">apps.splunk.com</a>.","| tstats summariesonly=t allow_old_summaries=t count  from datamodel=Authentication where earliest=-2h 
| tstats summariesonly=t allow_old_summaries=t dc(Authentication.dest) as count  from datamodel=Authentication where earliest=-2h
| tstats summariesonly=t allow_old_summaries=t dc(Authentication.user) as count from datamodel=Authentication where earliest=-2h","| tstats summariesonly=t allow_old_summaries=t dc(Authentication.dest) as count  from datamodel=Authentication  groupby _time span=1d, Authentication.user 
| rename ""Authentication.user"" as user"
,"| `Load_Sample_Log_Data(AWS CloudTrail)` | bucket _time span=1d | stats values(eval(""1"")) as count by _time ",,"First we bring in our basic demo dataset. In this case, anonymized AWS CloudTrail logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Next we use stats to summarize the number of events per user per day.",AWS APIs Called More Often Than Usual Per Account - Demo,AWS APIs Called More Often Than Usual Per Account - Demo,,,,count,user,aws-cloudtrail-data-anon.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| `Load_Sample_Log_Data(AWS CloudTrail)` 
| bucket _time span=1d 
| stats count by  user _time"
,index=* sourcetype=aws:cloudtrail eventName=AuthorizeSecurityGroup* | bucket _time span=1d | stats dc(user) as count by _time,,"First we bring in our basic dataset, AWS CloudTrail logs that are filtered for ACL modification events.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Next we use stats to summarize the number of events per user per day.",AWS Unusual Amount of Modifications to ACLs - Live,AWS Unusual Amount of Modifications to ACLs - Live,,,,count,user,count,1,Must have AWS CloudTrail data,,"In order to run this search, you must have AWS CloudTrail data onboard. Visit the <a href=""/app/Splunk_Security_Essentials/data_source?technology=AWS%20CloudTrail"">data onboarding guide for AWS CloudTrail in this app</a>, or browse to <a href=""https://splunkbase.splunk.com/app/1876/"">apps.splunk.com</a> for more information.",| tstats count where earliest=-2h latest=now index=* sourcetype=aws:cloudtrail,"index=* sourcetype=aws:cloudtrail eventName=AuthorizeSecurityGroup* 
| bucket _time span=1d 
| stats count by user _time"
,"|  `Load_Sample_Log_Data(""Interactive Logins"")`| bucket _time span=1d | stats dc(user) as count by _time  ",,"First we pull in our demo dataset.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Finally, we can count and aggregate per user, per day.",Increase in Interactive Logons - Demo,Increase in Interactive Logons - Demo,,,,count,user,anon_interactive_logons.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","|  `Load_Sample_Log_Data(""Interactive Logins"")`
| bucket _time span=1d 
| stats dc(dest) as count by _time user "
,| tstats summariesonly=t allow_old_summaries=t dc(All_Traffic.src_ip) as count from datamodel=Network_Traffic by _time span=1d ,,"Here, tstats is pulling in one command a super-fast count per src_ip, per day.",Distinct Hosts Communicated With Per Day - Accelerated,Distinct Hosts Communicated With Per Day - Accelerated,,,,count,All_Traffic.src_ip,"count
count
count","1
1
1","Must have data in Network Traffic data model
Must have an accelerated Network Traffic data model
Network Traffic data model must have src_ip and dest_ip fields",,"This search requires Firewall or Netflow data to run. We are searching here for the common information model network traffic data model.
In addition to searching for the common information model network traffic data model, we are telling Splunk to only visit accelerated data models.
In addition to searching for the accelerated common information model network traffic data model, we are telling Splunk to verify that there is a src_ip and dest_ip in this data set.","| tstats count from datamodel=Network_Traffic where earliest=-1h 
| tstats summariesonly=t allow_old_summaries=t count from datamodel=Network_Traffic where earliest=-1h 
| tstats summariesonly=t allow_old_summaries=t dc(All_Traffic.dest_ip) as dest dc(All_Traffic.src_ip) as src from datamodel=Network_Traffic where earliest=-1h | eval count = dest * src",| tstats summariesonly=t allow_old_summaries=t dc(All_Traffic.dest_ip) as count from datamodel=Network_Traffic by All_Traffic.src_ip _time span=1d 
,"|  `Load_Sample_Log_Data(""Interactive Logins"")`| bucket _time span=1d | stats dc(dest) as count by _time  ",,"First we pull in our demo dataset.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Finally, we can count and aggregate per user, per day.",Increase in Interactively Logged In Users - Demo,Increase in Interactively Logged In Users - Demo,,,,count,dest,anon_interactive_logons.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","|  `Load_Sample_Log_Data(""Interactive Logins"")`
| bucket _time span=1d 
| stats dc(user) as count by _time dest "
,"|  `Load_Sample_Log_Data(""Windows Run As Logs (Event ID 4648)"")`| makemv Account_Name delim="","" | bucket _time span=1d | stats dc(Unprivileged_Account_Name) as count by _time  ",,"First we pull in our demo dataset.
This line won't exist outside of demo data.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Finally, we can count and aggregate per user, per day.",Increase in Windows Privilege Escalation - Demo,Increase in Windows Privilege Escalation - Demo,,,,count,Unprivileged_Account_Name,event_id_4648_runas.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","|  `Load_Sample_Log_Data(""Windows Run As Logs (Event ID 4648)"")`
| makemv Account_Name delim="","" 
| bucket _time span=1d 
| stats count by _time Unprivileged_Account_Name"
,index=sfdc EVENT_TYPE=DocumentAttachmentDownloads | bucket _time span=1d | stats dc(USER_ID) as count by _time ,,"First we pull in our SFDC dataset and filter for what we're looking for in this use case, specifically the DocumentAttachmentDownloads EVENT_TYPE.
Then we enrich to convert the SFDC USER_ID into a friendly username via a lookup.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Finally, we can count and aggregate per user, per day.",Spike in SFDC Documents Downloaded - Live,Spike in SFDC Documents Downloaded - Live,,,,count,USER_NAME,"count
count","1
1","Must have Salesforce Data (assumes index=SFDC)
Must have Download Data (assumes index=SFDC)",,"This search requires data from the Salesforce Event Log File API. This is an additional fee from Salesforce, and can be effectively ingested and analyzed with the <a href=""https://splunkbase.splunk.com/app/1931"">Splunk App for Salesforce</a>.
This search requires data from the Salesforce Event Log File API, and specifically DocumentAttachmentDownloads EVENT_TYPEs. It is assumed that will always be present if you are ingesting data from the Salesforce Event Log File, this check just validates that.","| metasearch index=sfdc  earliest=-24h | head 100 | stats count
| metasearch index=sfdc DocumentAttachmentDownloads earliest=-24h | head 100| stats count","index=sfdc EVENT_TYPE=DocumentAttachmentDownloads 
| lookup SFDC_User_Lookup USER_ID 
| bucket _time span=1d 
| stats count by USER_NAME _time"
,"index=* sourcetype=""WinEventLog:Security"" (4624 OR 4647 OR 4648 OR 551 OR 552 OR 540 OR 528 OR 4768 OR 4769 OR 4770 OR 4771 OR 4768 OR 4774 OR 4776 OR 4778 OR 4779 OR 672 OR 673 OR 674 OR 675 OR 678 OR 680 OR 682 OR 683) | bucket _time span=1d | stats dc(user) as count by  _time",,"First we pull in our Windows Security log dataset, filtering to logon Event IDs.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Finally, we can count and aggregate per user, per day.",Unique Hosts Logged Into Per Day - Live,Unique Hosts Logged Into Per Day - Live,,,,count,user,"count
count
count","1
1
1","Must have Windows Security data
Must have Logon Success Data
Must have the user field defined",,"This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.
You should log logon events. There are many event IDs that we look for in the underlying logs, but they should all fall into the Audit Successful (or Failed) Logon events in your Windows Audit Policy. (<a href=""https://technet.microsoft.com/en-us/library/cc431373.aspx"">docs</a>)
You should have a field called ""user"" defined in your Windows Security logs. This is provided by the Splunk TA for Windows. Consider adding that TA to make for a better experience!","| metasearch earliest=-2h latest=now index=* sourcetype=""WinEventLog:Security"" | head 100 | stats count 
sourcetype=""WinEventLog:Security"" index=* (4624 OR 4647 OR 4648 OR 551 OR 552 OR 540 OR 528 OR 4768 OR 4769 OR 4770 OR 4771 OR 4768 OR 4774 OR 4776 OR 4778 OR 4779 OR 672 OR 673 OR 674 OR 675 OR 678 OR 680 OR 682 OR 683) | head 100 | stats count
sourcetype=""WinEventLog:Security"" earliest=-2h index=* (4624 OR 4647 OR 4648 OR 551 OR 552 OR 540 OR 528 OR 4768 OR 4769 OR 4770 OR 4771 OR 4768 OR 4774 OR 4776 OR 4778 OR 4779 OR 672 OR 673 OR 674 OR 675 OR 678 OR 680 OR 682 OR 683) | head 100 | stats dc(user) as count","index=* sourcetype=""WinEventLog:Security"" (4624 OR 4647 OR 4648 OR 551 OR 552 OR 540 OR 528 OR 4768 OR 4769 OR 4770 OR 4771 OR 4768 OR 4774 OR 4776 OR 4778 OR 4779 OR 672 OR 673 OR 674 OR 675 OR 678 OR 680 OR 682 OR 683) 
| bucket _time span=1d 
| stats dc(host) as count by user _time"
,index=sfdc ROWS_PROCESSED>0 EVENT_TYPE=API OR EVENT_TYPE=BulkAPI OR EVENT_TYPE=RestAPI| bucket _time span=1d | stats dc(USER_ID) as count by _time ,,"First we pull in our SFDC dataset and filter for what we're looking for in this use case, specifically export EVENT_TYPEs with at least one ROWS_PROCESSED.
Then we enrich to convert the SFDC USER_ID into a friendly username via a lookup.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Finally, we can count and aggregate per user, per day.",Spike in SFDC Records Exported - Live,Spike in SFDC Records Exported - Live,,,,rows,USER_NAME,"count
count","1
1","Must have Salesforce Data (assumes index=SFDC)
Must have Export Data (assumes index=SFDC)",,"This search requires data from the Salesforce Event Log File API. This is an additional fee from Salesforce, and can be effectively ingested and analyzed with the <a href=""https://splunkbase.splunk.com/app/1931"">Splunk App for Salesforce</a>.
This search requires data from the Salesforce Event Log File API, and specifically the ROWS_PROCESSED by the API / BulkAPI / RestAPI EVENT_TYPEs. It is assumed that will always be present if you are ingesting data from the Salesforce Event Log File, this check just validates that.","| metasearch index=sfdc  earliest=-24h | head 100 | stats count
| metasearch index=sfdc API OR BulkAPI OR RestAPI earliest=-24h | head 100| stats count","index=sfdc ROWS_PROCESSED>0 EVENT_TYPE=API OR EVENT_TYPE=BulkAPI OR EVENT_TYPE=RestAPI
| lookup SFDC_User_Lookup USER_ID
| bucket _time span=1d 
| stats sum(ROWS_PROCESSED) as rows by _time USER_NAME"
,index=*  sourcetype=cisco:esa* OR sourcetype=MSExchange*:MessageTracking OR tag=email src_user=* | bucket _time span=1d | stats dc(src_user) as count by _time ,,"First we pull in our email dataset, with filters for Password Reset somewhere in the message.
Based on the message subject, tag it with a value for Detect_Type
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Finally, we can count and aggregate per detection type tag, per day.",Spike in Password Reset Emails - Live,Spike in Password Reset Emails - Live,,,,count,Detect_Type,count,1,Must have Email Data,,"This search requires Email data. The out of the box field extractions support the Common Information Model, including Cisco ESA/Ironport and Microsoft Exchange. If you don't have this data today, we highly recommend ingesting it with the <a href=""https://splunkbase.splunk.com/app/1761/"">Cisco ESA TA</a> or the <a href=""https://splunkbase.splunk.com/app/3225/"">Splunk Add-on for Microsoft Exchange</a>. For best performance, accelerate the email data model from the <a href=""https://splunkbase.splunk.com/app/1621/"">Common Information Model</a>!",| tstats count where index=* sourcetype=cisco:esa* OR sourcetype=MSExchange*:MessageTracking OR tag=email earliest=-4h,"index=* sourcetype=cisco:esa* OR sourcetype=MSExchange*:MessageTracking OR tag=email Password Reset 
| eval Detect_Type=case(LIKE(Subject, ""%Password Reset%""), ""Password Reset"", LIKE(Subject, ""%Validate Credentials%""), ""Validate Credentials"",1=1, null) 
| bucket _time span=1d 
| stats count by _time Detect_Type "
,index=* sourcetype=uniflow OR (sourcetype=Win*system EventCode=307) | bucket _time span=1d stats dc(User) by _time ,,"First we pull in our printer dataset.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Finally, we can count and aggregate per user, per day.",Pages Printed Per User Per Day - Live,Pages Printed Per User Per Day - Live,,,,Pages,User,"count
pages
users","1
1
1","Must have Printer data
Must have a field called Page_Count defined
Must have the user field defined",,"This search requires Printer data. By default we are looking for either Uniflow logs (used from our demo data sample) or Windows Print Server logs. If you don't have this data right now, consider <a href=""https://blogs.technet.microsoft.com/askperf/2008/08/12/two-minute-drill-enabling-print-queue-logging/"">ingesting it</a>! If you have other printer logs, go ahead and substitute the sourcetype below.
You should have a field called ""Page_Count"" defined in your printer logs. If that's not currently extracted, build an extraction for it (or do an inline rex in the SPL below to work around this). Or just choose a different field below.
You should have a field called ""User"" defined in your printer logs. If that's not currently extracted, build an extraction for it (or do an inline rex in the SPL below to work around this). Or just choose a different field below.","earliest=-6h latest=now sourcetype=uniflow OR (sourcetype=Win*system EventCode=307) index=* | head 100 | stats count dc(Page_Count) as pages dc(User) as users
earliest=-6h latest=now sourcetype=uniflow OR (sourcetype=Win*system EventCode=307) index=* | head 100 | stats count dc(Page_Count) as pages dc(User) as users
earliest=-6h latest=now sourcetype=uniflow OR (sourcetype=Win*system EventCode=307) index=* | head 100 | stats count dc(Page_Count) as pages dc(User) as users","index=*  sourcetype=uniflow OR (sourcetype=Win*system EventCode=307) 
| bucket _time span=1d 
| stats sum(Page_Count) as Pages by User _time"
,index=* ((tag=network tag=communicate) OR (sourcetype=pan*traffic OR sourcetype=opsec OR sourcetype=cisco:asa OR sourcetype=stream* )) (dest_port=139 OR dest_port=445) | bucket _time span=1d | stats dc(src_ip) as count by _time ,,"First we pull in our basic dataset, which comes from Firewall Logs for SMB connections.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the last day.
Now we are looking at the number of unique destinations per source IP, per day.",Detect Spike in SMB Traffic - Live,Detect Spike in SMB Traffic - Live,,,,,,count,1,Must have network traffic.,,"Ingest network traffic logs, consider using Splunk Stream.",index=* ((tag=network tag=communicate) OR (sourcetype=pan*traffic OR sourcetype=opsec OR sourcetype=cisco:asa OR sourcetype=stream*)) earliest=-1h latest=now | stats count,"index=* ((tag=network tag=communicate) OR (sourcetype=pan*traffic OR sourcetype=opsec OR sourcetype=cisco:asa OR sourcetype=stream* )) (dest_port=139 OR dest_port=445) 
| bucket _time span=1d 
| stats dc(dest_ip) as count by src_ip, _time "
,| tstats summariesonly=t allow_old_summaries=t dc(user) as count from datamodel=printer where nodename=Print_Jobs by _time span=1d,,"Here, tstats is pulling in one command a super-fast count per user, per day.
(self-explanatory)",Pages Printed Per User Per Day - Accelerated,Pages Printed Per User Per Day - Accelerated,,,,Pages,User,"count
count
count","1
1
1","Must have a Printer data model (not default)
Printer data model must have a field called Page_Count defined
Printer data model must have the user field defined",,"This search requires Printer data. There is no default printer data model in Splunk, so you will need to define one in order to use an accelerated search. May we suggest building a data model with the fields user and Page_Count and then accelerate it?
You should have a field called ""Page_Count"" defined in your printer data model (referenced in tstats as Printer.Page_Count). If that's not currently present and accelerated, do so. If it's a different field name, provide that below.
You should have a field called ""User"" defined in your printer data model (referenced in tstats as Printer.User). If that's not currently present and accelerated, do so. If it's a different field name, provide that below.","| tstats summariesonly=t allow_old_summaries=t count from datamodel=Printer where nodename=Print_Jobs earliest=-6h
| tstats summariesonly=t allow_old_summaries=t dc(Printer.Page_Count) as count from datamodel=Printer where nodename=Print_Jobs earliest=-6h
| tstats summariesonly=t allow_old_summaries=t dc(Printer.Page_Count) as count from datamodel=Printer where nodename=Print_Jobs earliest=-6h","| tstats summariesonly=t allow_old_summaries=t count from datamodel=Printer where nodename=Print_Jobs groupby user, _time span=1d 
| eval comment=""<--- We don't have a standard data model that includes pages printed, so you will need to build one to leverage data model acceleration... that said this is usually low volume enough to not be a big deal""  "
,| tstats summariesonly=t allow_old_summaries=t dc(user) as count from datamodel=Git where nodename=Git_View by _time span=1d,,"Here, tstats is pulling in one command a super-fast count per user, per day.
(self-explanatory)",Git File Views or Downloads Per Day - Accelerated,Git File Views or Downloads Per Day - Accelerated,,,,count,user,"count
count","1
1","Must have an accelerated Git data model (non-default)
Must have a user field in accelerated Git data model",,"This search accelerated Git data. There is no formal CIM data model for Source Code checkins or checkouts, so we are presuming a custom data model called Git.
The Git data model must have a user field defined.","| tstats summariesonly=t allow_old_summaries=t count from datamodel=Git where earliest=-24h latest=now nodename=Git_View 
| tstats summariesonly=t allow_old_summaries=t dc(user) as count from datamodel=Git where earliest=-24h latest=now nodename=Git_View","| tstats summariesonly=t allow_old_summaries=t count from datamodel=Git where nodename=Git_View groupby user, _time span=1d 
| eval comment=""<--- We don't have a standard data model that includes git repos, so you will need to build one to leverage data model acceleration""   "
,"|  `Load_Sample_Log_Data(""Source Code Access Logs"")`| bucket _time span=1d | stats dc(user) as count by _time ",,"First we pull in our demo dataset.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Finally, we can count and aggregate per user, per day.",Git File Views or Downloads Per Day - Demo,Git File Views or Downloads Per Day - Demo,,,,count,user,anonymized_git_history.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","|  `Load_Sample_Log_Data(""Source Code Access Logs"")`
| bucket _time span=1d 
| stats count by user _time "
,"| `Load_Sample_Log_Data(""Email Logs"")`| bucket _time span=1d | stats dc(Sender) as count by _time ",,"First we pull in our demo dataset.
Then we filter for where the sending email address is @mycompany.com
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Finally, we can count and aggregate per user, per day.",Spike in Email from Address - Demo,Spike in Email from Address - Demo,,,,count,Sender,Anonymized_Email_Logs.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| `Load_Sample_Log_Data(""Email Logs"")`
| search Sender=*@mycompany.com 
| bucket _time span=1d 
| stats count by Sender, _time"
,"| `Load_Sample_Log_Data(""SFDC Data"")` | search ROWS_PROCESSED>0 EVENT_TYPE=API OR EVENT_TYPE=BulkAPI OR EVENT_TYPE=RestAPI| bucket _time span=1d | stats values(eval(""1"")) as count by _time ",,"First we pull in our demo SFDC dataset.
Then we filter for what we're looking for in this use case, specifically export EVENT_TYPEs with at least one ROWS_PROCESSED.
Then we enrich to convert the SFDC USER_ID into a friendly username via a lookup.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Finally, we can count and aggregate per user, per day.",Spike in SFDC Records Exported - Demo,Spike in SFDC Records Exported - Demo,,,,rows,USER_NAME,SFDC_Sample_Data_Anon.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| `Load_Sample_Log_Data(""SFDC Data"")`  
| search ROWS_PROCESSED>0 EVENT_TYPE=API OR EVENT_TYPE=BulkAPI OR EVENT_TYPE=RestAPI
| lookup SFDC_User_Lookup USER_ID
| bucket _time span=1d 
| stats sum(ROWS_PROCESSED) as rows by _time USER_NAME"
,"index=* source=""*/atlassian-bitbucket-access.log"" | bucket _time span=1d | stats dc(user) as count by _time ",,"First we pull in our Atlassian Git dataset.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Finally, we can count and aggregate per user, per day.",Git File Views or Downloads Per Day - Live,Git File Views or Downloads Per Day - Live,,,,count,user,"count
count","1
1","Must have BitBucket / Git data
Must have a user defined in your data",,"In tests so far, Atlassian BitBucket git logs are stored in a file called atlassian-bitbucket-access.log. We're looking for that here.
You should have a field called ""user"" defined in your bitbucket logs. If that's not currently extracted, build an extraction for it (or do an inline rex in the SPL below to work around this).","| metasearch earliest=-24h latest=now index=* source=""*/atlassian-bitbucket-access.log"" | head 100 | stats count 
earliest=-2h latest=now index=* source=""*/atlassian-bitbucket-access.log"" | head 100 | stats dc(user) as count ","index=* source=""*/atlassian-bitbucket-access.log"" 
| bucket _time span=1d 
| stats count by user _time "
,"|  `Load_Sample_Log_Data(""Aggregated Cerner EMR Logs"")`| stats dc(EmployeeName) as count by _time ",,"First we pull in our demo dataset.
We would normally need to aggregate now per user per day, but in this case the demo dataset is already aggregated (pulling from a summary index, as is often done in this scenario).",Number of Unique Patient Records Viewed Per Day - Demo,Number of Unique Patient Records Viewed Per Day - Demo,,,,NumOpens,EmployeeName,healthcare_cerner_patient_records.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","|  `Load_Sample_Log_Data(""Aggregated Cerner EMR Logs"")`
| table _time EmployeeName NumOpens Role YearsAtCompany City Username"
,"index=* sourcetype=""WinEventLog:Security"" Logon_Type=2 OR Logon_Type=10 OR Logon_Type=11 Logon Type TaskCategory=Logon Audit Success | bucket _time span=1d | stats dc(user) as count by _time",,"First we pull in our dataset of Windows Authentication specifying Interactive logon types.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Finally, we can count and aggregate per user, per day.",Increase in Interactive Logons - Live,Increase in Interactive Logons - Live,,,,count,user,count,1,Must have Windows Security data,,"This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.","| metasearch earliest=-2h latest=now index=* sourcetype=""WinEventLog:Security"" | head | stats count ","index=* sourcetype=""WinEventLog:Security"" Logon_Type=2 OR Logon_Type=10 OR Logon_Type=11 Logon Type TaskCategory=Logon Audit Success 
| bucket _time span=1d 
| stats dc(dest) as count by _time user"
,"|  `Load_Sample_Log_Data(""Sample Firewall Data"")`| bucket _time span=1d | stats dc(src_ip) as count by _time ",,"First we pull in our demo dataset.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Finally, we can count and aggregate per src_ip, per day.",Distinct Hosts Communicated With Per Day - Demo,Distinct Hosts Communicated With Per Day - Demo,,,,count,src_ip,od_splunklive_fw_data.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","|  `Load_Sample_Log_Data(""Sample Firewall Data"")`
| bucket _time span=1d 
| stats dc(dest_ip) as count by src_ip, _time"
,"| `Load_Sample_Log_Data(AWS CloudTrail)`  | search eventName=AuthorizeSecurityGroup*  | bucket _time span=1d | stats values(eval(""1"")) as count by _time ",,"First we bring in our basic demo dataset. In this case, anonymized AWS CloudTrail logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
With our dataset onboard, we then filter down to just the events indicating a modification of ACLs
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Next we use stats to summarize the number of events per user per day.",AWS Unusual Amount of Modifications to ACLs - Demo,AWS Unusual Amount of Modifications to ACLs - Demo,,,,count,user,aws-cloudtrail-data-anon.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| `Load_Sample_Log_Data(AWS CloudTrail)` 
| search eventName=AuthorizeSecurityGroup* 
| bucket _time span=1d 
| stats count by user _time"
,"index=* sourcetype=""WinEventLog:Security"" 4648 EventCode=4648 | search NOT Account_Name=*$ | eval Unprivileged_Account_Name=mvindex(Account_Name, 1) | bucket _time span=1d | stats dc(Unprivileged_Account_Name) as count by _time ",,"First we pull in our dataset, consisting of Windows Security logs with Event ID 4648, showing ""Run As"" events.
Next we filter out the Windows System usernames, where this can occur frequently
Windows Security logs often include two usernames -- the acting username, and the target username. We want the latter (note that this hasn't been proven to work uniformly across all log sources, but it seems to work well for this scenario).
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Finally, we can count and aggregate per user, per day.",Increase in Windows Privilege Escalation - Live,Increase in Windows Privilege Escalation - Live,,,,count,user,"count
count","1
1","Must have Windows Security data
Must have Privileged Escalation Events (EventCode=4648)",,"This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.
Windows Security Event ID 4648 tracks the explicit use of credentials, as in a runas event or batch login from a scheduled task. You can enable this from your Windows Logon Event policy configuration.","| metasearch earliest=-2h latest=now index=* sourcetype=""WinEventLog:Security"" | head | stats count 
| metasearch earliest=-30d sourcetype=""WinEventLog:Security"" index=* TERM(eventcode=4648)  | head | stats count","index=* sourcetype=""WinEventLog:Security"" 4648 EventCode=4648 
| search NOT Account_Name=*$ 
| eval Unprivileged_Account_Name=mvindex(Account_Name, 1) 
| bucket _time span=1d 
| stats count by _time Unprivileged_Account_Name"
7,,,"First we bring in our basic demo dataset, Symantec Endpoint Protection Risks. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
While there are several approaches to grouping events, and stats is the fastest, we're using transaction because it's the easiest. This will let us group all the events based on the Computer_Name.
Finally we can filter for if there are at least three events and they spanned at least a few minutes.",Multiple Infections on Host - Demo,Multiple Infections on Host - Demo,,,,,,anon_interactive_logons.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(Symantec Endpoint Protection Risks)` 
| transaction maxpause=1h Computer_Name 
| where eventcount >=3 AND duration>240"
,,,"First we load our Sysmon demo data
From sysmon data, we care primarily about file writes (code 11) or timestamp changes (code 2), so we filter for that
Splunk has a capability of looking up data in a CSV file through the lookup command. This will take the filename, ""look it up"" in the csv file, and then add any new fields.
The field from the lookup is ""status"" so we can now search for any true Name field.
And finally we can pull out all the filenames and put them into a usable format via the stats command.",Ransomware Notes - Demo,Ransomware Notes - Demo,,,,,,UC_ransomware_notes.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| inputlookup UC_ransomware_notes 
| search EventCode=2 OR EventCode=11 
| lookup ransomware_notes_lookup ransomware_notes as TargetFilename 
| search status=True 
| stats values(TargetFilename) AS ""RansomNotes Detected"" by _time, host, Image"
,,,"First we load our basic demo data
Next we filter for the specific message that NetBackup sends for successful backups
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the day.
Finally we can look at the hosts that are successfully backed up over time, thanks to stats.",Succesful Backups - Demo,Succesful Backups - Demo,,,,,,UC_successful_backups.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| inputlookup UC_successful_backups 
|search MESSAGE=""Disk/Partition backup completed successfully."" 
| bucket _time span=1d 
| stats values(COMPUTERNAME) as ""Systems Backed Up"" by _time, MESSAGE"
7,,,"First we bring in our basic demo dataset. In this case, Windows logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same hour.
Next we use the magic of stats+eval to count how many events there are where the action is success, or the action is failure
Finally we filter for where there is at least one success, and more than 100 failures.",Basic Brute Force - Demo,Basic Brute Force - Demo,,,,,,anon_interactive_logons.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(Windows Brute Force)` 
| bucket _time span=1h 
| stats count(eval(action=""success"")) as successes count(eval(action=""failure"")) as failures by src _time 
| where successes>0 AND failures>100"
7,,,"First we pull in our demo dataset.
This line won't exist in production, it is just so that we can format the demo data (coming from a CSV file) correctly.
Next we filter to make sure we're looking for just account creation events or account deletions.
Transaction will now group everything together so that we can see multiple events occurring to the same username.
We can now filter for users where both event IDs occurred.
Finally we can display everything in a nice table for the user to consume.",Short Lived Accounts - Demo,Short Lived Accounts - Demo,,,,,,Local_Short_Lived_Account.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","|`Load_Sample_Log_Data(""Local Short-Lived Account"")`  
| rex mode=sed field=Account_Name ""s/
/;/g"" | makemv Account_Name delim="";"" 
 | search  EventCode=4726 OR EventCode=4720 
| transaction Target_Account_Name maxspan=180m 
| search EventCode=4720 EventCode=4726
| table _time EventCode Account_Name Target_Account_Name Message"
7,,,"First we pull in our demo dataset. This could be any EDR data source that provides process launch information.
From line one we have our process launch logs, now we need to filter that down to just the potential discovery tools. We do this via a subsearch. A subsearch goes and runs another search, and then takes those results and inserts them into the main search. You can copy-paste that subsearch into a new search window and see what the results look like -- there's a single field called ""search"" that has a bunch of file names with ORs between them. That will effectively be inserted into our main search, giving us a really long search string without having to maintain a really long search.
From Line 1-2, we have a list of suspicious process launches. Now we want to see if many of those fire around the same time. Transaction is great for that -- it lets us group together events that all have the same value for a field, in this case the same host. maxpause=5m lets us continue grouping together any events that have no more than 5 minutes between each one.
From line 1-3, we have grouping of suspicious process launches, now we're going to look and see how many different unique programs were launched using mvcount, which gives us the # of events for a multi-value field.
Finally we clean up a few fields that transaction adds, so that we get a nice clean display.",Series of Discovery Filenames - Demo,Series of Discovery Filenames - Demo,,,,,,generic_sysmon_service_launch_logs.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| `Load_Sample_Log_Data(""Generic Sysmon Process Launches"")` 
 |search [|inputlookup tools.csv | search discovery_or_attack=discovery | eval filename=""Image=\""*\\\\"" . filename . ""\"""" | stats values(filename) as search | eval search=mvjoin(search, "" OR "")] 
| transaction host maxpause=5m
 | where mvcount(Image)>=6
| fields - _raw closed_txn field_match_sum linecount"
7,,,"First we bring in our basic demo dataset. In this case, AWS CloudTrail logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
Next we pull the last src_ip for the same user using streamstats (sorted based on the user).
Next we look up the AccountId in the GDPR categorization lookup. Because we only care about GDPR Accounts for this example, we filter for only the hosts that are in scope for GDPR. (You could also categorize based on any other field in AWS.)
Here we filter for logins that are GDPR in scope, and in a short enough time range that it would be difficult to travel to distant parts of the globe.
Here we resolve the Last src_ip to a physical location, and stick that in a field so that we can conveniently use it.
Now we resolve the *current* src_ip
Now we calculate the distance using an approximation for the curvature of the earth. Easy, right? I do not understand it, I copy-pasted from https://answers.splunk.com/answers/317935/calculating-distances-between-points-with-geoip-us.html#answer-568451
Here we pull the date of the event, to make this easier to run over longer time windows.
Finally we use stats to collect all of the values into one line, per user, per day, and per set of locations. We're using some specific AWS data fields here -- if you're using a log source like VPN, then you might choose other fields.",Land Speed GDPR - Demo,Land Speed GDPR - Demo,,,,,,anon_interactive_logons.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(AWS CloudTrail)`
| sort 0 user, _time | streamstats window=1 current=f values(_time) as last_time values(src_ip) as last_src_ip by user 
| lookup gdpr_aws_category accountId 
| where isnotnull(category) AND last_src_ip != src_ip AND _time - last_time < 8*60*60 
| iplocation last_src_ip | rename lat as last_lat lon as last_lon | eval location = City . ""|"" . Country . ""|"" . Region 
| iplocation src_ip 
| eval rlat1 = pi()*last_lat/180, rlat2=pi()*lat/180, rlat = pi()*(lat-last_lat)/180, rlon= pi()*(lon-last_lon)/180 | eval a = sin(rlat/2) * sin(rlat/2) + cos(rlat1) * cos(rlat2) * sin(rlon/2) * sin(rlon/2) | eval c = 2 * atan2(sqrt(a), sqrt(1-a)) | eval distance = 6371 * c, time_difference_hours = round((_time - last_time) / 3600,2), speed=round(distance/ ( time_difference_hours),2) | fields - rlat* a c 
| eval day=strftime(_time, ""%m/%d/%Y"")
| stats values(accountId) values(awsRegion) values(eventName) values(distance) values(eval(mvappend(last_Country, Country))) as Country values(eval(mvappend(last_City, City))) as City values(eval(mvappend(last_Region, Region))) as Region  values(lat) values(lon)  values(userAgent) max(speed) as max_speed_kph min(time_difference_hours) as min_time_difference_hours by day user distance"
7,,,"First we pull in our basic dataset, which consists of Process Launch Logs (in this case coming from Windows Security Event ID 4688, but could come from any). We are filtering here to just process launches in the Users directory, so that we are focusing in on what unprivileged users can run (and not getting noise from things like software updates).
Next we use the Shannon Entropy algorithm provided by the free app URL Toolbox to calculate a very basic randomness score for this string.
Shannon Entropy gives a numeric score, you will usually want to filter on values above of 4 or 4.5.
Finally we use stats to put everything in a convenient table.
And of course we use rename to provide field names that will make sense to analysts.",Processes With High Entropy Names in Users Directory - Live,Processes With High Entropy Names in Users Directory - Live,,,,,,"count
count
count","1
1
1","Must have Windows Security Logs
Must have Process Launch Logs (Event ID 4688)
Must have URL Toolbox Installed (provides Shannon Entropy Checking)",,"Begin ingesting Windows Security Logs
Turn on Process Tracking in your Windows Audit logs (<a href=""https://technet.microsoft.com/en-us/library/cc976411.aspx"">docs</a>)
Splunk's URL Toolbox app, written by Cedric Le Roux, not only provides effective URL Parsing but also Levenshtein similarity checking (e.g., typo detection) and Shannon Entropy Detection. Download <a href=""https://splunkbase.splunk.com/app/2734/"">here</a>.","| metasearch index=* earliest=-2h latest=now sourcetype=""WinEventLog:Security"" | stats count 
earliest=-2h latest=now index=* sourcetype=""WinEventLog:Security"" EventCode=4688 | head 100 | stats count 
| rest /services/apps/local | search disabled=0 label=""URL Toolbox"" | stats count","index=* sourcetype=""WinEventLog:Security"" EventCode=4688 New_Process_Name=C:\\Users*
| lookup ut_shannon_lookup word as New_Process_Name 
| where ut_shannon > 4.5 
| stats  values(ut_shannon)  as ""Shannon Entropy Score"" by New_Process_Name,host 
| rename  New_Process_Name as Process,host as Endpoint | sort  -""Shannon Entropy Score"" "
7,,,"First we bring in our basic dataset. This is the output of an exceedingly complicated macro that pulls from Splunk's authorization information. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
Next we enrich the index field with the GDPR category.
Because this is a GDPR search, we search for just the GDPR in scope indexes.
Next we enrich the user field with the User's GDPR status.
Because a user or index can belong to many different categories, we use mvexpand to split them into a multi-value field.
Finally we look for users who don't have a matching GDPR category, or who aren't authorized for any GDPR information at all.",Splunk Role Check GDPR - Demo,Splunk Role Check GDPR - Demo,,,,,,anon_interactive_logons.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(Splunk Index Provisioning)`  
| lookup gdpr_splunk_index_category index as accessible_indexes OUTPUT category as index_category 
| search index_category=* 
| lookup gdpr_user_category user OUTPUT category as user_category
 | makemv delim=""|"" index_category | makemv delim=""|"" user_category 
| where isnull(user_category) OR user_category != index_category"
7,,,"First we pull in our dataset, of Windows Security Logs with account creation events or account changes with group membership events.
Transaction will now group everything together so that we can see multiple events occurring to the same username.
Now we can filter to just transactions with both event IDs
Finally we can display everything in a nice table for the user to consume.",New Local Admin - Live,New Local Admin - Live,,,,,,"count
count
count","1
1
1","Must have Windows Security Logs
Must have Local Account Management Logs (Event ID 4720)
Must have Local Group Management Logs (Event ID 4732)",,"Begin ingesting Windows Security Logs
Turn on Account Management Audit Logs in your Local Windows Security Policy (<a href=""http://www.thewindowsclub.com/track-user-activity-windows"">docs</a>)
Turn on Group Management Audit Logs in your Local Windows Security Policy (<a href=""http://whatevernetworks.com/?p=21"">docs</a>)","| metasearch index=* earliest=-2h latest=now sourcetype=""WinEventLog:Security"" | stats count 
| metasearch earliest=-30d latest=now index=* sourcetype=""WinEventLog:Security"" TERM(eventcode=4720) | head | stats count 
| metasearch earliest=-30d latest=now index=* sourcetype=""WinEventLog:Security"" TERM(eventcode=4732) | head | stats count ","index=* sourcetype=""WinEventLog:Security"" EventCode=4720 OR (EventCode=4732 Administrators) 
| transaction Security_ID maxspan=180m 
| search EventCode=4720 (EventCode=4732 Administrators) 
 | table _time EventCode Account_Name Target_Account_Name Message"
,,,"First we load our Sysmon EDR (though any other process launch logs with the full command line would suffice) data. We look for any instances of WMIC (Windows Management Instrumentation Command-line) being launched (EventCode 1 indicates a process launch), and filter to make sure our suspicious fields are in the CommandLine string.
Then we put the data into a table because that's the easiest thing to use.",Detecting WMI Remote Process Creation - Live,Detecting WMI Remote Process Creation - Live,,,,,,"count
count","1
1","Must have Microsoft sysmon logs
Must have process start events (EventCode=1)",,"Sysmon is a free Microsoft tool that provides all kinds of great value. Consider pulling the data in via our <a href=""https://splunkbase.splunk.com/app/1914/"">Splunk App</a>. Check out the <a href=""http://conf.splunk.com/files/2016/slides/splunking-the-endpoint-hands-on.pdf"">Splunking the Endpoint</a> .conf presentation to see what you can do with this data!
Check your sysmon configuration file to ensure you are not filtering out EventCode 1 events.","| metasearch index=* sourcetype=""xmlwineventlog:microsoft-windows-sysmon/operational""  earliest=-1h latest=now | stats count
sourcetype=""xmlwineventlog:microsoft-windows-sysmon/operational"" EventCode=1 index=* | head 100 | stats count","index=* sourcetype=XmlWinEventLog:Microsoft-Windows-Sysmon/Operational EventCode=1 Image=*wmic* CommandLine=*node* CommandLine=""*process call create*"" 
| table _time host Image CommandLine"
7,,,"First we run our LDAP query. This requires that you've properly configured the app, and often that you've got some basic understanding of how your Active Directory is configured. When in doubt, consult with an AD admin about where your user and service accounts are located.
Active Directory gives us timestamps in a couple of formats, none of which allow us to do comparisons. To solve this, we use the convert search command to convert those other time formats into Unix epoch time (a nice and clean number) that we can do math on.
Now we look for any accounts where the password is more than 120 days old, and the login is in the last 30 days. Why 120 days? If someone's password expires while they're on vacation, we don't want to alert, so this gives us a month of buffer. We also are filtering for only accounts that are actively used, so that we don't trigger on old disabled accounts (though that's also a worthwhile detection!). 
We've got our suspect values, our last step is to format the data to be more meaningful to an analyst by converting those numeric timestamps into human-readable ones.",Old Passwords In Use - LDAPSearch,Old Passwords In Use - LDAPSearch,,,,,,count,1,Must have Splunk Supporting Add-on for Active Directory Installed,,"The Splunk Supporting Add-on for Active Directory app allows us to query AD environments via LDAP to get everything we need. Check it out -- <a href=""https://splunkbase.splunk.com/app/1151/"" target=""_blank"">app link</a>, <a href=""http://docs.splunk.com/Documentation/SA-LdapSearch/2.1.6/User/AbouttheSplunkSupportingAdd-onforActiveDirectory"" target=""_blank"">docs link</a>.","| rest /services/apps/local | search disabled=0 title=""SA-ldapsearch"" | stats count","| ldapsearch search=""(&(objectclass=user)(!(objectClass=computer)))"" attrs=""sAMAccountName,pwdLastSet,lastLogonTimestamp,whenCreated,badPwdCount,logonCount"" | fields - _raw host _time 
| convert timeformat=""%Y-%m-%dT%H:%M:%S.%6QZ"" mktime(pwdLastSet) mktime(lastLogonTimestamp) | convert timeformat=""%Y%m%d%H%M%S.0Z""  mktime(whenCreated) 
| where pwdLastSet < relative_time(now(), ""-120d"") AND lastLogonTimestamp > relative_time(now(), ""-30d"") 
| convert ctime(lastLogonTimestamp) ctime(whenCreated) ctime(pwdLastSet)"
,,,"First we load our Windows Update data, filtered for just Windows Update messages that are related to specific KB we know we need to focus on.
Now we have a large number of events and we will want to roll them up into something more usable. stats is great at that!
And finally we can filter for exactly the events we care about.",Windows Successful Updates Install - Live,Windows Successful Updates Install - Live,,,,,,count,1,Must have Windows Update logs,,This data is provided by the Windows TA. Consider using the TA for a better experience.,| metasearch index=* sourcetype=WindowsUpdateLog earliest=-1h latest=now | stats count,"index=* (sourcetype=""WinEventLog:System"" OR sourcetype=""WindowsUpdateLog"") (KB4012598 OR KB4012212 OR KB4012215 OR KB4012213 OR KB4012216 OR KB4012214 OR KB4012217 OR KB4012606 OR KB4013198 OR KB4013429) 
| stats latest(status) as lastStatus by _time, dest, signature, signature_id 
| search lastStatus=installed"
,,,"First we load our basic demo data
Next we filter for the specific message that NetBackup sends for failed backups
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the day.
Finally we can look at the hosts that failed to back up over time, thanks to stats.",Unsuccesful backups - Demo,Unsuccesful backups - Demo,,,,,,UC_unsuccessful_backups.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| inputlookup UC_unsuccessful_backups 
| search MESSAGE=""An error occurred, failed to backup."" 
| bucket _time span=1d
| stats values(COMPUTERNAME) as ""Systems Backed Up"" by _time, MESSAGE"
7,,,"Here, tstats gives us a super-fast count of account creation events or account deletion events.
Next we rename the fields make it easier to work with them.
Transaction will now group everything together so that we can see multiple events occurring to the same username.
Now we can filter for transactions with both events.",Short Lived Accounts - Accelerated,Short Lived Accounts - Accelerated,,,,,,"count
count","1
1","Must have an accelerated Change Analysis data model
Change Analysis data model must have Local Account Management Logs (Event ID 4720 and 4726)",,"Begin ingesting Windows Security Logs
Turn on Group Management Audit Logs in your Local Windows Security Policy (<a href=""http://whatevernetworks.com/?p=21"">docs</a>)","| tstats summariesonly=t allow_old_summaries=t count from datamodel=Change_Analysis where earliest=-2d
| tstats summariesonly=t allow_old_summaries=t count from datamodel=Change_Analysis where earliest=-30d (All_Changes.result_id=4720 OR All_Changes.result_id=4726) ","| tstats summariesonly=t allow_old_summaries=t count from datamodel=Change_Analysis where All_Changes.result_id=4720 OR All_Changes.result_id=4726 by All_Changes.result_id All_Changes.user All_Changes.dest _time 
| rename All_Changes.* as *  
| transaction user maxspan=180m  
| search result_id=4720 result_id=4726"
7,,,"First we bring in our basic dataset. In this case, Windows logs.
Next we use the magic of stats+eval to count how many events there are where the action is success, or the action is failure
Finally we filter for where there is at least one success, and more than 100 failures.
Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR.",Brute Force Slow and Low GDPR - Live,Brute Force Slow and Low GDPR - Live,,,,,,"count
count
count","1
1
1","Must have Windows Security data
Must have Logon Success Data
Must have the user field defined",,"This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.
You should log logon events. There are many event IDs that we look for in the underlying logs, but they should all fall into the Audit Successful (or Failed) Logon events in your Windows Audit Policy. (<a href=""https://technet.microsoft.com/en-us/library/cc431373.aspx"">docs</a>)
You should have a field called ""user"" defined in your Windows Security logs. This is provided by the Splunk TA for Windows. Consider adding that TA to make for a better experience!","| metasearch earliest=-2h latest=now sourcetype=""*WinEventLog:Security"" index=* | head 100 | stats count 
sourcetype=""*WinEventLog:Security"" (4624 OR 4647 OR 4648 OR 551 OR 552 OR 540 OR 528 OR 4768 OR 4769 OR 4770 OR 4771 OR 4768 OR 4774 OR 4776 OR 4778 OR 4779 OR 672 OR 673 OR 674 OR 675 OR 678 OR 680 OR 682 OR 683) index=* | head 100 | stats count
sourcetype=""*WinEventLog:Security"" earliest=-2h index=* (4624 OR 4647 OR 4648 OR 551 OR 552 OR 540 OR 528 OR 4768 OR 4769 OR 4770 OR 4771 OR 4768 OR 4774 OR 4776 OR 4778 OR 4779 OR 672 OR 673 OR 674 OR 675 OR 678 OR 680 OR 682 OR 683) | head 100 | stats dc(user) as count","index=* (sourcetype=win*security OR sourcetype=linux_secure OR tag=authentication) user=* user!=""""  
| stats count(eval(action=""success"")) as successes count(eval(action=""failure"")) as failures by src dest  
| where successes>0 AND failures>100  
| lookup gdpr_system_category host as dest | search category=* "
7,,,"First we ask tstats to give us a list of source addresses for emails (with a count), and we rename it so that it's easier to work with.
Next we are going to extract the domain.
Now we aggregate per actual domain we will analyze, for performance reasons
Let's filter out any domains that our organization owns and expects to receive email from. You can have several domains here (I recommend no more than 10-20 -- eventually urltoolbox will get tired and stop doing adding levenshtein fields, so you can look for null ut_levenshtein later if you are pushing this boundary).
Now we use the free URL Toolbox app to parse out subdomains from the top level domains. We want to analyze each one, so that an attacker can't send mycompany.yourithelpdesk.com and get through, or mail.mycampany.com.
The field we are going to pass to the levenshtein algorithm is domain_detected, so let's add each subdomain to the multi-value field domain_detected.
This step is not required, but I like to filter down the list of fields mid-search just to make it easier for me to read and track it. URL Toolbox adds a *lot* of fields, but these four are the only fields I care about from now on.
Last piece of prep -- let's simplify everything exactly the two fields that URL Toolbox's levenshtein algorithm is expecting.
Now the real magic: URL Toolbox is given two multi-value fields, and it does the cross checking to calculate the levenshtein score for each combination. We pull out the lowest score from this group.
Now we filter for a levenshtein score less than three (so two or fewer changes required to go from the domain to one of our standard domains). Those who have used levenshtein are likely thinking: ""Wait, what about the > 0 that we always use?"" -- we accomplished that by filtering out standard domains way back at the start.
Finally we do some | fields and | rename so that everything looks nice and friendly for analysts to understand what we're looking at.",Emails With Lookalike Domains - Accelerated,Emails With Lookalike Domains - Accelerated,,,,,,"count
count
count
count","1
1
1
1","Must have URL Toolbox Installed (provides Levenshtein lookalike detection)
Must have an Email data model
Must have an accelerated Email data model
Must have Sender Email Addresses (src_user) in your accelerated Email data model",,"The URL Toolbox app, written by Cedric Le Roux, not only provides effective URL Parsing but also Levenshtein similarity checking (e.g., typo detection) and Shannon entropy detection (random characters). Download <a href=""https://splunkbase.splunk.com/app/2734/"">here</a>.
This search requires an Email data. This is dependent on the <a href=""https://splunkbase.splunk.com/app/1621/"">Common Information Model</a> being present, and also having your data mapped to CIM via appropriate TAs, usually with the out of the box field extractions from the <a href=""https://splunkbase.splunk.com/app/1761/"">Cisco ESA TA</a>, the <a href=""https://splunkbase.splunk.com/app/3225/"">Splunk Add-on for Microsoft Exchange</a>, etc.
This search requires an accelerated Email data. In order to run a fast accelerated search, you should accelerate your data model. (<a href=""https://docs.splunk.com/Documentation/Splunk/latest/HadoopAnalytics/Configuredatamodelacceleration#Accelerate_the_data_model"">docs</a>)
This search assumes that you have actual source email addresses -- check your field extractions for src_user and then rebuild your data models if not.","| rest /services/apps/local | search disabled=0 label=""URL Toolbox"" | stats count
| tstats summaries_only=f allow_old_summaries=t count from datamodel=Email where earliest=-1h
| tstats summaries_only=t allow_old_summaries=t count from datamodel=Email where earliest=-1h
| tstats summaries_only=t allow_old_summaries=t dc(All_Email.src_user) as count from datamodel=Email where earliest=-1h","| tstats summaries_only=t allow_old_summaries=t count from datamodel=Email by All_Email.src_user | rename All_Email.src_user as src_user 
| rex field=src_user ""\@(?<domain_detected>.*)"" 
| stats sum(count) as count by domain_detected 
| eval domain_detected=mvfilter(domain_detected!=""mycompany.com"" AND domain_detected!=""company.com"" AND domain_detected!=""mycompanylovestheenvironment.com"") 
| eval list=""mozilla"" | `ut_parse_extended(domain_detected, list)` 
| foreach ut_subdomain_level* [eval orig_domain=domain_detected, domain_detected=mvappend(domain_detected, '<<FIELD>>' . ""."" . ut_tld)] 
| fields orig_domain domain_detected ut_domain count   
| eval word1=mvappend(domain_detected, ut_domain), word2 = mvappend(""mycompany.com"", ""company.com"", ""mycompanylovestheenvironment.com"") 
| lookup ut_levenshtein_lookup word1 word2 | eval ut_levenshtein= min(ut_levenshtein) 
| where ut_levenshtein < 3 
| fields - domain_detected ut_domain | rename orig_domain as top_level_domain_in_incoming_email word1 as domain_names_analyzed word2 as company_domains_used count as num_occurrences ut_levenshtein as Levenshtein_Similarity_Score"
7,,,"First we bring in our basic dataset. This dataset includes logins from Windows Security logs.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same hour.
Now we summarize the number of logins by user, or destination.
Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR.
Next we look up the user in the GDPR categorization lookup.
Because a user or host can belong to many different categories, we use mvexpand to split them into a multi-value field.
Finally we look for users who don't have a matching GDPR category, or who aren't authorized for any GDPR information at all.",Login to New System GDPR - Live,Login to New System GDPR - Live,,,,,,"count
count
count","1
1
1","Must have Windows Security data
Must have Logon Success Data
Must have the user field defined",,"This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.
You should log logon events. There are many event IDs that we look for in the underlying logs, but they should all fall into the Audit Successful (or Failed) Logon events in your Windows Audit Policy. (<a href=""https://technet.microsoft.com/en-us/library/cc431373.aspx"">docs</a>)
You should have a field called ""user"" defined in your Windows Security logs. This is provided by the Splunk TA for Windows. Consider adding that TA to make for a better experience!","| metasearch earliest=-2h latest=now sourcetype=""*WinEventLog:Security"" index=* | head 100 | stats count 
sourcetype=""*WinEventLog:Security"" (4624 OR 4647 OR 4648 OR 551 OR 552 OR 540 OR 528 OR 4768 OR 4769 OR 4770 OR 4771 OR 4768 OR 4774 OR 4776 OR 4778 OR 4779 OR 672 OR 673 OR 674 OR 675 OR 678 OR 680 OR 682 OR 683) index=* | head 100 | stats count
sourcetype=""*WinEventLog:Security"" earliest=-2h index=* (4624 OR 4647 OR 4648 OR 551 OR 552 OR 540 OR 528 OR 4768 OR 4769 OR 4770 OR 4771 OR 4768 OR 4774 OR 4776 OR 4778 OR 4779 OR 672 OR 673 OR 674 OR 675 OR 678 OR 680 OR 682 OR 683) | head 100 | stats dc(user) as count","index=* sourcetype=win*security user=* dest=* action=success 
| bucket _time span=1d 
| stats count by user, dest 
| lookup gdpr_system_category.csv host as dest OUTPUT category as dest_category | search dest_category=* 
| lookup gdpr_user_category user OUTPUT category as user_category
| makemv delim=""|"" dest_category | makemv delim=""|"" user_category 
| where isnull(user_category) OR user_category != dest_category"
7,,,"First we bring in our basic dataset. In this case, showing successful logins.
Next we check a lookup that shows the user status (this would typically be pulled from SA-ldapsearch or ADMon).
Now we can filter for users where the expiration is at least a day ago (timezones are hard), or that are disabled.
Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR.",Activity Expired Identity GDPR - Live,Activity Expired Identity GDPR - Live,,,,,,"count
count
count","1
1
1","Must have Windows Security data
Must have Logon Success Data
Must have the user field defined",,"This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.
You should log logon events. There are many event IDs that we look for in the underlying logs, but they should all fall into the Audit Successful (or Failed) Logon events in your Windows Audit Policy. (<a href=""https://technet.microsoft.com/en-us/library/cc431373.aspx"">docs</a>)
You should have a field called ""user"" defined in your Windows Security logs. This is provided by the Splunk TA for Windows. Consider adding that TA to make for a better experience!","| metasearch earliest=-2h latest=now sourcetype=""*WinEventLog:Security"" index=* | head 100 | stats count 
sourcetype=""*WinEventLog:Security"" (4624 OR 4647 OR 4648 OR 551 OR 552 OR 540 OR 528 OR 4768 OR 4769 OR 4770 OR 4771 OR 4768 OR 4774 OR 4776 OR 4778 OR 4779 OR 672 OR 673 OR 674 OR 675 OR 678 OR 680 OR 682 OR 683) index=* | head 100 | stats count
sourcetype=""*WinEventLog:Security"" earliest=-2h index=* (4624 OR 4647 OR 4648 OR 551 OR 552 OR 540 OR 528 OR 4768 OR 4769 OR 4770 OR 4771 OR 4768 OR 4774 OR 4776 OR 4778 OR 4779 OR 672 OR 673 OR 674 OR 675 OR 678 OR 680 OR 682 OR 683) | head 100 | stats dc(user) as count","index=* (sourcetype=win*security OR sourcetype=linux_secure OR tag=authentication) user=* user!="""" action=success 
| lookup user_account_status.csv user 
| where _time > relative_time(terminationDate, ""+1d"") 
| lookup gdpr_system_category.csv host as dest OUTPUT category | search category=*"
7,,,"First we bring in our basic demo dataset. In this case, showing Interactive Logins. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
Next we check a lookup that shows the user status (this would typically be pulled from SA-ldapsearch or ADMon).
Now we can filter for users where the expiration is at least a day ago (timezones are hard), or that are disabled.",Basic Expired Account - Demo,Basic Expired Account - Demo,,,,,,anon_interactive_logons.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(Interactive Logins)` 
| lookup user_account_status user 
| where _time > relative_time(terminationDate, ""+1d"") "
7,,,"First we bring in our basic dataset, Palo Alto Networks logs, filtered for access to PII data (Workday), over any non-HTTPS port. You can also detect this with any mechanism that allows you to analyze whether content is encrypted.
Finally, we format just the data that users want to see.",Access to Inscope Resources Unencrypted GDPR - Live,Access to Inscope Resources Unencrypted GDPR - Live,,,,,,"count
count
count","1
1
1","Must have Firewall data
Must have an app field
Must have workday in this data source (otherwise adjust the search)",,"This search requires Firewall or Netflow data to run. By default, we're checking for Common Information Model compliant data, and then also manually specifying the standard sourcetypes for Check Point, Palo Alto Networks, and Cisco ASAs. You should specify your particular index and sourcetype in the actual search to improve performance (or better yet, accelerate with the common information model!)
This search is also looking for firewall logs, but with the added filter of making sure that an app is defined.
This search is also looking for firewall logs, but with the added filter of making sure that the workday app exists in the data","(tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)| head 100 | stats count 
((tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)) app=* | head 100 | stats count 
((tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)) app=workday* | head 100 | stats count ","((tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa) app=workday* dest_port!=443 
| table _time user app bytes* src_ip dest_ip dest_port"
7,,,"First we bring in our dataset. This is the output of an exceedingly complicated macro that pulls from Splunk's authorization information, which was grabbed for Data Governance.
Next we enrich the index field with the GDPR category.
Because this is a GDPR search, we search for just the GDPR in scope indexes.
Next we enrich the user field with the User's GDPR status.
Because a user or index can belong to many different categories, we use mvexpand to split them into a multi-value field.
Finally we look for users who don't have a matching GDPR category, or who aren't authorized for any GDPR information at all.",Splunk Role Check GDPR - Live,Splunk Role Check GDPR - Live,,,,,,,,,,,,"| `User_to_Index_Provisioning_From_Data_Governance_App`
| lookup gdpr_splunk_index_category index as accessible_indexes OUTPUT category as index_category 
| search index_category=* 
| lookup gdpr_user_category user OUTPUT category as user_category
| makemv delim=""|"" index_category | makemv delim=""|"" user_category 
| where isnull(user_category) OR user_category != index_category "
7,,,"First we bring in our basic dataset. In this case, Windows logs.
Next we use the magic of stats+eval to count how many events there are where the action is success, or the action is failure
Next we filter for where there is at least one success, and more than 100 failures.
Finally we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR.",Brute Force GDPR - Live,Brute Force GDPR - Live,,,,,,"count
count","1
1","Must have Windows Security data
Must have the user field defined",,"This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.
You should have a field called ""user"" defined in your Windows Security logs. This is provided by the Splunk TA for Windows. Consider adding that TA to make for a better experience!","| metasearch earliest=-2h latest=now sourcetype=""*WinEventLog:Security"" index=* | head 100 | stats count 
sourcetype=""*WinEventLog:Security"" earliest=-2h index=* | head 100 | stats dc(user) as count","index=* (sourcetype=win*security OR sourcetype=linux_secure OR tag=authentication) user=* user!=""""  
| stats count(eval(action=""success"")) as successes count(eval(action=""failure"")) as failures by src dest 
| where successes>0 AND failures>100  
| lookup gdpr_system_category host as dest| search category=* "
,,,"First we load our Sysmon demo data
From sysmon data, we care primarily about file writes (code 11) or timestamp changes (code 2), so we filter for that
Next we use the rex command to extract file extensions using a moderately complex regular expression.
Now that we have our file extensions, we want to look them up. Splunk has a capability of looking up data in a CSV file through the lookup command. This will take the file extension we just extracted, ""look it up"" in the csv file, and then add any new fields.
The field from the lookup is ""Name"" so we can now search for any true Name field.
And finally we can pull out all the filenames and put them into a usable format via the stats command.",Ransomware Extensions - Demo,Ransomware Extensions - Demo,,,,,,UC_ransomware_extentions.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah "," | inputlookup UC_ransomware_extentions 
| search EventCode=2 OR EventCode=11 
| rex field=TargetFilename ""(?<file_extension>\.[^\.]+)$"" 
| lookup ransomware_extensions_lookup Extensions AS extension 
| search Name!=false  
| stats values(TargetFilename) AS ""Files Written"" by _time, host, Image, Name, extension"
7,,,"First we bring in our basic dataset. In this case, we are using risk events from Symantec Endpoint Protection.
To detect uncleaned Malware, we look for where the action taken is not the primary or secondary action expected.
Finally we put everything in a nice and usable table!",Endpoint Uncleaned Malware - Live,Endpoint Uncleaned Malware - Live,,,,,,count,1,Must have Symantec AV data,,"For simplicity, this search is written specifically for Symantec AV data, but it can be easily modified for other sources. ",| metasearch earliest=-24h latest=now index=* sourcetype=symantec:ep:* | head 100 | stats count ,"index=* sourcetype=symantec:ep:*:file  
| where Actual_Action!=Requested_Action AND Actual_Action!=Secondary_Action  
| table _time Action_Action Requested_Action Secondary_Action Risk_Name File_Path Computer_Name"
7,,,"First we pull in our accelerated dataset, which comes from Firewall Logs and targets scenarios where we will have a small number of DNS connections with a large amount of volume. tstats here is giving us the number of bytes sent per source_ip, filtered for dest_port 53
Eventstats then allows us to calculate all manner of statistics. This is one of the more complicated stats syntaxes that you will see, but it's actually not that complicated. The big component here is leveraging stats+eval, where we can embed the flexible logic of eval inside of stats. In this case, when we are calculating our average and standard deviation, we really want to exclude the most recent values (that which we're concerned about), so that they don't sway our average (imagine you churn along at 1 kb per hour, then in the last hour it's 150 MB.. you really want your normal baseline to be 1 KB). One other note here -- we are doing two different eventstats, one on a global basis, one on a per host basis. That's so we can try to identify a host that's always at the top of the charts, while overall looking across the org, giving us a good balance across servers with static IPs and DHCP hosts that move around.
Here's where we really start doing the important work. Our lengthy eventstats gave us all these fields that we can filter on and interpret based on. (When testing this out, feel free to remove this line and those that follow, so you can see the raw fields coming out of eventstats.) Now we need to filter for hosts that are substantially above the norm.
From our last line, we have focused in to just hosts that are behaving abnormally. Here, we are using eval to add another field to the results -- not one focused on detection logic, but to try to add context and summarize some of the maths for an analyst to see why we are surfacing this host.
Finally, we clear up some of the nonsense fields we don't care that much about, again to make things clearer for the analyst.",Huge Volume of DNS Traffic - Accelerated,Huge Volume of DNS Traffic - Accelerated,,,,,,"count
count
count","1
1
1","Must have a Network Traffic data model
Must have an accelerated Network Traffic data model
Network Traffic data model must have a src_ip and bytes_out",,"This search requires Firewall or Netflow data to run. We are searching here for the common information model network traffic data model.
In addition to searching for the common information model network traffic data model, we are telling Splunk to only visit accelerated data models.
In addition to searching for the accelerated common information model network traffic data model, we are telling Splunk to verify that there is a src_ip, and that bytes_out is greater than zero bytes in this data set.","| tstats count from datamodel=Network_Traffic where earliest=-1h 
| tstats summariesonly=t allow_old_summaries=t count from datamodel=Network_Traffic where earliest=-1h 
| tstats summariesonly=t allow_old_summaries=t sum(All_Traffic.bytes_out) as bytes dc(All_Traffic.src_ip) as src from datamodel=Network_Traffic where earliest=-1h | eval count = bytes * src","| tstats summariesonly=t allow_old_summaries=t sum(All_Traffic.bytes_out) as count from datamodel=Network_Traffic where All_Traffic.dest_port=53 by All_Traffic.src_ip _time span=1d 
| eventstats max(_time) as maxtime avg(bytes_out) as avg_bytes_out stdev(bytes_out) as stdev_bytes_out | eventstats count as num_data_samples avg(eval(if(_time < relative_time(maxtime, ""@h""),bytes_out,null))) as per_source_avg_bytes_out stdev(eval(if(_time < relative_time(maxtime, ""@h""),bytes_out,null))) as per_source_stdev_bytes_out by src_ip  
| where num_data_samples >=4 AND bytes_out > avg_bytes_out + 3 * stdev_bytes_out AND bytes_out > per_source_avg_bytes_out + 3 * per_source_stdev_bytes_out AND _time >= relative_time(maxtime, ""@h"") 
| eval num_standard_deviations_away_from_org_average = round(abs(bytes_out - avg_bytes_out) / stdev_bytes_out,2), num_standard_deviations_away_from_per_source_average = round(abs(bytes_out - per_source_avg_bytes_out) / per_source_stdev_bytes_out,2) 
| fields - maxtime per_source* avg* stdev*"
7,,,"First we bring in our basic demo dataset, Symantec Endpoint Operational Logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
Next we use a relatively complicated stats command to track the time of the last update, and the time of the last error.
Next we filter for the events where the time of the last update was more than three days ago, or where the last error was more recent than the last update.
Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR.
Finally, we format the timestamps in a human readable way.",Outdated Malware Definitions GDPR - Demo,Outdated Malware Definitions GDPR - Demo,,,,,,anon_interactive_logons.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(Symantec Endpoint Protection Operations)` 
|stats max(eval(if(like(Event_Description, ""%LiveUpdate session ran successfully%"") , _time, null))) as LatestUpdate max(_time) as LatestMessage max(eval(if(tag=""error"", _time, null))) as LatestError by Host_Name  
| where LatestUpdate < relative_time(LatestMessage, ""-3d"") OR LatestError > LatestUpdate 
| lookup gdpr_system_category host as Host_Name | search category=*
| convert ctime(LatestUpdate) ctime(LatestMessage) ctime(LatestError)"
,,,"First we load our Windows Event Log data and filter for the Event Codes that indicate the Windows event log is being cleared. You can see there are a few possibilities. 
Then, because we respect analysts, we put it in a nice easy-to-consume table.",Windows Event Log Clearing Events - Live,Windows Event Log Clearing Events - Live,,,,,,"count
count
count","1
1
1","Must have Windows Security Log data
Must have Windows System Log data
Must have audit log events (EventCode=1100 or EventCode=1102 or EventCode=104)",,"This search requires Windows Security Log data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.
This search requires Windows System Log data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.
Verify that you are generating and receiving logs with these EventCodes. This is provided by the Splunk TA for Windows. Consider adding that TA to make for a better experience!","| metasearch earliest=-2h latest=now sourcetype=""*win*security"" index=* | stats count
| metasearch earliest=-2h latest=now sourcetype=""*win*system"" index=* | stats count
(sourcetype=""*win*system"" OR sourcetype=""*win*security"") (EventCode=1100 OR EventCode=1102 OR EventCode=104) index=* | head 100 | stats count","index=* ((sourcetype=wineventlog:security OR XmlWinEventlog:Security) AND (EventCode=1102 OR EventCode=1100)) OR ((sourcetype=wineventlog:system OR XmlWinEventlog:System) AND EventCode=104) 
| stats count by _time EventCode sourcetype host"
7,,,"First we bring in our basic demo dataset, Symantec Endpoint Operational Logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
Next we use a relatively complicated stats command to track the time of the last update, and the time of the last error.
Next we filter for the events where the time of the last update was more than three days ago, or where the last error was more recent than the last update.
Finally, we format the timestamps in a human readable way.",Outdated Malware Definitions - Demo,Outdated Malware Definitions - Demo,,,,,,anon_interactive_logons.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(Symantec Endpoint Protection Operations)` 
|stats max(eval(if(like(Event_Description, ""%LiveUpdate session ran successfully%"") , _time, null))) as LatestUpdate max(_time) as LatestMessage max(eval(if(tag=""error"", _time, null))) as LatestError by Host_Name  
| where LatestUpdate < relative_time(LatestMessage, ""-3d"") OR LatestError > LatestUpdate 
| convert ctime(LatestUpdate) ctime(LatestMessage) ctime(LatestError)"
,,,"First we load our NGFW data (we include many different options here -- you should specify the index and sourcetype for the device that exists in your environment. Then we filter for where the firewall detects the presence of tor, and where we know the source_ip involved.
Finally we put everything in a table so that it's easy to use.",Monitor TOR traffic - Live,Monitor TOR traffic - Live,,,,,,"count
count","1
1","Must have network traffic.
Make sure the app field is populated.",,"Ingest network traffic logs, consider using Splunk Stream.
Use a NGFW or another tool to identify the application layer protocol.","index=* ((tag=network tag=communicate) OR (sourcetype=pan*traffic OR sourcetype=opsec OR sourcetype=cisco:asa OR sourcetype=stream*)) earliest=-1h latest=now | stats count
index=* ((tag=network tag=communicate) OR (sourcetype=pan*traffic OR sourcetype=opsec OR sourcetype=cisco:asa OR sourcetype=stream*)) earliest=-1h latest=now | stats dc(app) as count","index=* ((tag=network tag=communicate) OR (sourcetype=pan*traffic OR sourcetype=opsec OR sourcetype=cisco:asa OR sourcetype=stream*)) app=tor src_ip=* 
| table _time src_ip src_port dest_ip dest_port bytes app"
,,,"First we load our Windows Update data, filtered for just Windows Update messages that are related to specific KB we know we need to focus on.
Now we have a large number of events and we will want to roll them up into something more usable. stats is great at that!
And finally we can filter for exactly the events we care about.",Windows Updates Install Failure - Live,Windows Updates Install Failure - Live,,,,,,count,1,Must have Windows Update logs,,This data is provided by the Windows TA. Consider using the TA for a better experience.,| metasearch index=* sourcetype=WindowsUpdateLog earliest=-1h latest=now | stats count,"index=* sourcetype=WindowsUpdateLog (KB4012598 OR KB4012212 OR KB4012215 OR KB4012213 OR KB4012216 OR KB4012214 OR KB4012217 OR KB4012606 OR KB4013198 OR KB4013429) 
| stats latest(status) as lastStatus by _time, dest, signature, signature_id 
| search lastStatus=failure"
7,,,"First we pull in our basic dataset, which consists of XML format Sysmon logs from the endpoints (ingested via the sysmon TA). This could be any EDR data source that provides the full CLI string. Because we're looking for process launches, we then filter for EventCode=1 (the sysmon Process Launch code).
Eventstats is like stats, but just adds the results to your existing dataset. So this will give us the avg and stdev of the command line length per host.
Finally we use stats to roll up multiple process launches, giving us the length of that cli string alongside the avg and stdev of the host, per host, per cli string.
With that setup, we can find processes that have substantially longer cli strings (4 * the standard deviation) than the average on this system.",Find Unusually Long CLI Commands - Live,Find Unusually Long CLI Commands - Live,,,,,,count,1,Must have Microsoft sysmon logs,,"Sysmon is a free Microsoft tool that provides all kinds of great value. Consider pulling the data in via our <a href=""https://splunkbase.splunk.com/app/1914/"">Splunk App</a>. Check out the <a href=""http://conf.splunk.com/files/2016/slides/splunking-the-endpoint-hands-on.pdf"">Splunking the Endpoint</a> .conf presentation to see what you can do with this data!","| metasearch index=* sourcetype=""xmlwineventlog:microsoft-windows-sysmon/operational""  earliest=-1h latest=now  | stats count ","index=* sourcetype=""xmlwineventlog:microsoft-windows-sysmon/operational"" EventCode=1  
|eval cmdlen=len(CommandLine) | eventstats stdev(cmdlen) as stdev,avg(cmdlen) as avg by host
| stats max(cmdlen) as maxlen, values(stdev) as stdevperhost, values(avg) as avgperhost by host,CommandLine 
| where maxlen>4*stdevperhost+avgperhost"
7,,,"First we bring in our basic dataset, Symantec Endpoint Protection Risks, over the last thirty days.
Next we use stats to calculate the distance between the earliest and latest time of an infection with the range() function. This will give us a number in seconds.
Then we filter for a range of greater than 30 minutes (so at least 30 minutes between infections).
Finally we do some formatting to provide usable numbers, so that no one has to calculate seconds to days.",Recurring Infection on Host - Live,Recurring Infection on Host - Live,,,,,,count,1,Must have Symantec AV data,,"For simplicity, this search is written specifically for Symantec AV data, but it can be easily modified for other sources. ",| metasearch earliest=-24h latest=now index=* sourcetype=symantec:ep:* | head 100 | stats count ,"index=* sourcetype=symantec:* earliest=-30d 
| stats count range(_time) as TimeRange by Risk_Name, Computer_Name 
| where TimeRange>1800 
| eval TimeRange_In_Hours = round(TimeRange/3600,2), TimeRange_In_Days = round(TimeRange/3600/24,2)"
7,,,"First we bring in our basic dataset, which in this case is Windows Security EventID 4688 Process Launch Logs, but could come from any EDR data source. We use index=* here (though you should replace with the index you use for Windows Security events, like index=oswinsec) and specify the standard sourcetype of Windows Security events pulled from the Splunk TA for Windows. Finally, we look for EventCode 4688, which are the Windows Process Launch Logs.
From line one we have our process launch logs, now we need to filter that down to just the potential attack tools. We do this via a subsearch. A subsearch goes and runs another search, and then takes those results and inserts them into the main search. You can copy-paste that subsearch into a new search window and see what the results look like -- there's a single field called ""search"" that has a bunch of file names with ORs between them. That will effectively be inserted into our main search, giving us a really long search string without having to maintain a really long search.
From Line 1-2, we have a list of suspicious process launches. Now we want to see if many of those fire around the same time. Transaction is great for that -- it lets us group together events that all have the same value for a field, in this case the same host. maxpause=5m lets us continue grouping together any events that have no more than 5 minutes between each one.
From line 1-3, we have grouping of suspicious process launches, but also transaction has added a few new fields, such as duration and eventcount. Eventcount lets us see how many process launches are in each transaction (each grouping of suspicious process launches), so we can filter for when there are at least 4 launch events together.
Finally we clear up a few fields that Transaction adds.",Series of Hacker Filenames - Live,Series of Hacker Filenames - Live,,,,,,"count
count","1
1","Must have Windows Security Logs
Must have Process Launch Logs (Event ID 4688)",,"Begin ingesting Windows Security Logs
Turn on Process Tracking in your Windows Audit logs (<a href=""https://technet.microsoft.com/en-us/library/cc976411.aspx"">docs</a>)","| metasearch index=* earliest=-2h latest=now sourcetype=""WinEventLog:Security"" | stats count 
earliest=-2h latest=now index=* sourcetype=""WinEventLog:Security"" EventCode=4688 | head 100 | stats count ","index=* sourcetype=""WinEventLog:Security"" EventCode=4688 
[| inputlookup tools.csv WHERE discovery_or_attack=attack | stats values(filename) as search | format] 
| transaction host maxpause=5m 
| where eventcount>=4
| fields - _raw closed_txn field_match_sum linecount"
7,,,"First we bring in our basic dataset, Palo Alto Networks logs, filtered for access to PII data (Workday, though you can apply to any other data sources).
Finally, we format just the data that users want to see.",Access to Inscope Resources GDPR - Live,Access to Inscope Resources GDPR - Live,,,,,,"count
count
count","1
1
1","Must have Firewall data
Must have an app field
Must have workday in this data source (otherwise adjust the search)",,"This search requires Firewall or Netflow data to run. By default, we're checking for Common Information Model compliant data, and then also manually specifying the standard sourcetypes for Check Point, Palo Alto Networks, and Cisco ASAs. You should specify your particular index and sourcetype in the actual search to improve performance (or better yet, accelerate with the common information model!)
This search is also looking for firewall logs, but with the added filter of making sure that an app is defined.
This search is also looking for firewall logs, but with the added filter of making sure that the workday app exists in the data","(tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)| head 100 | stats count 
((tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)) app=* | head 100 | stats count 
((tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)) app=workday* | head 100 | stats count ","((tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa) app=workday* 
| table _time user app bytes* src_ip dest_ip"
7,,,"First we start by pulling a count of events overall by host. (Note that when using back-to-back tstats, your field names need to be different, but you can't us the familiar 'count as count1' syntax, so here we use 'count(host)' to distinguish from the 'count' in the next line.
Now we pull a count of events for our in scope security source(s) by host, here Windows Security logs.
Next we use stats to combine our two prestats=t tstats commands into one usable stats. Whenever you use tstats prestats=t, you need a stats, chart, or similiar to pull the hidden prestats fields into the light so that you can use them.
From lines 1-3 we have a count of events per source (all vs security) per host per day, now we can calculate a percentage of logs that were WinSecurity on each of those days (for each of those hosts).
Technically this line isn't really required because we should be able to use now() in the next line, but I typically use it for uniformity with the demo datasets and in case you have some weird timezone hijinks in your environment.
Now we start the tricky piece. How do we figure out the business logic for when we want to be alerted to a security log going silent? At this stage, we're collecting a number of data points that we could use. For example, past_instances_of_no_logs will tell us how many days in the past we had no logs from this host -- if this is non-zero, then no logs today is much more likely to be benign. Similarly, we can use avg and stdev to calculate how wide a distribution there typically is -- if sometimes you have tons of security events, sometimes you have none, then this is could be just chance.
Finally we apply our filtering. In testing, the most reliable metric seemed to be looking for hosts where we do have a baseline (at least ten days, so we know something about this host), we have some historical data (isnotnull(avg)), and we've never seen a day with zero events before.",Hosts That Stop Reporting Sourcetypes - Live,Hosts That Stop Reporting Sourcetypes - Live,,,,,,,,,,,,"| tstats prestats=t count(host) where index=* groupby host _time span=1d 
| tstats prestats=t append=t count where index=* sourcetype=""WinEventLog:Security"" by host  _time span=1d 
| stats count(host) as all_logs count as win_logs by host _time 
| eval win_perc=round(100*(win_logs / all_logs), 2) 
| eventstats max(_time) as maxtime 
| stats count as num_data_samples avg(eval(if(_time<relative_time(maxtime, ""-1d@d""), win_perc, null))) as avg sum(eval(if(_time<relative_time(maxtime, ""-1d@d"") AND win_perc=0, 1, null))) as past_instances_of_no_logs max(eval(if(_time>=relative_time(maxtime, ""-1d@d""), win_perc, null))) as latest by host 
| where isnotnull(avg) AND num_data_samples>10 AND isnull(past_instances_of_no_logs) AND latest=0"
7,,,"First we pull in our demo dataset. This could be any EDR data source that provides file hash information.
From line one we have our process launch logs, now we need to filter that down to just the potential attack tools. We do this via a subsearch. A subsearch goes and runs another search, and then takes those results and inserts them into the main search. You can copy-paste that subsearch into a new search window and see what the results look like -- there's a single field called ""search"" that has a bunch of file hashes with ORs between them. That will effectively be inserted into our main search, giving us a really long search string without having to maintain a really long search.
From Line 1-2, we have a list of suspicious process launches. Now we want to see if many of those fire around the same time. Transaction is great for that -- it lets us group together events that all have the same value for a field, in this case the same host. maxpause=5m lets us continue grouping together any events that have no more than 5 minutes between each one.
From line 1-3, we have grouping of suspicious process launches, but also transaction has added a few new fields, such as duration and eventcount. Eventcount lets us see how many process launches are in each transaction (each grouping of suspicious process launches), so we can filter for when there are at least 4 launch events together.
Finally we clear up a few fields that Transaction adds.",Series of Hacker Hashes - Demo,Series of Hacker Hashes - Demo,,,,,,generic_sysmon_service_launch_logs.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| `Load_Sample_Log_Data(""Generic Sysmon Process Launches"")` 
 |search [|inputlookup tools.csv | search discovery_or_attack=attack | eval hash=""sha1="" . hash | stats values(hash) as search | eval search=mvjoin(search, "" OR "")]
 | transaction host maxpause=5m
 | where eventcount>=4 
| fields - _raw closed_txn field_match_sum linecount"
,,,"First we load our basic demo data
Next we look for any instances of WMIC (Windows Management Instrumentation Command-line) being launched (EventCode 1 indicates a process launch), and filter to make sure our suspicious fields are in the CommandLine string.
Then we put the data into a table because that's the easiest thing to use.",Detecting WMI Remote Process Creation - Demo,Detecting WMI Remote Process Creation - Demo,,,,,,UC_wmi.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| inputlookup UC_wmi 
| search EventCode=1 Image=*wmic* CommandLine=*node* CommandLine=""*process call create*"" 
| table _time host Image CommandLine"
7,,,"First we pull in our demo dataset. This could be any EDR data source that provides file hash information.
Earlier versions of sysmon didn't extract a filename by default, so we are adding that in here. We also make it lowercase so that Windows' lack of case sensitivity doesn't mess with our analysis.
Next we use stats to calculate how many different filenames that hash ran as, including the filenames and the full cli strings for contextual data.
Finally, we filter for where the same hash ran as at least two filenames, indicating a rename.",Find Processes with Renamed Executables - Demo,Find Processes with Renamed Executables - Demo,,,,,,generic_sysmon_service_launch_logs.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| `Load_Sample_Log_Data(""Generic Sysmon Process Launches"")` 
| rex field=Image ""[\\\/](?<filename>[^\\\/]*)$"" | eval filename=lower(filename) 
| stats dc(filename) as NumFilenames values(filename) as Filenames values(Image) as Images by sha1 
| where NumFilenames>1"
,,,"First we bring in AWS Cloudtrail logs, filtering for the PutBucketAcl events that occur when bucket permissions are changed, and filtering for any that include AllUsers.
Next, we extract the User who made the change, via the spath search command that will traverse the JSON easily.
Similarly, we extract the bucket name.
Here is where things get tricky -- AWS uses JSON arrays to show multiple permissions in one message. What we're doing here is extracting that block of ACLs -- spath will return a multi-value field to us. Then we can expand that into multiple events (so if before there were 2 events with 3 ACLs defined in each, we would end up with six events -- three copies of each original event, but the grantee field would be different). Finally, we can use spath to extract the values from each grantee field.
Next, we can search for just those individual permissions apply to all users (unauthenticated users). While we're missing some context here about who else has permissions, we can follow-on to investigtate.
Last, we format the data a bit to meet our needs.
Now we calculate the distance using an approximation for the curvature of the earth. Easy, right? I do not understand it, I copy-pasted from https://answers.splunk.com/answers/317935/calculating-distances-between-points-with-geoip-us.html#answer-568451
Here we pull the date of the event, to make this easier to run over longer time windows.
Finally we use stats to collect all of the values into one line, per user, per day, and per set of locations. We're using some specific AWS data fields here -- if you're using a log source like VPN, then you might choose other fields.",AWS Public Bucket - Live,AWS Public Bucket - Live,,,,,,count,1,Must have AWS CloudTrail data (though could be applied to other data sources),,"In order to run this search, you must have AWS CloudTrail data onboard. Visit the <a href=""/app/Splunk_Security_Essentials/data_source?technology=AWS%20CloudTrail"">data onboarding guide for AWS CloudTrail in this app</a>, or browse to <a href=""https://splunkbase.splunk.com/app/1876/"">apps.splunk.com</a> for more information.",| tstats count where earliest=-2h latest=now index=* sourcetype=aws:cloudtrail,"index=* sourcetype=aws:cloudtrail AllUsers eventName=PutBucketAcl 
| spath output=userIdentityArn path=userIdentity.arn 
| spath output=bucketName path=""requestParameters.bucketName"" 
| spath output=aclControlList path=""requestParameters.AccessControlPolicy.AccessControlList"" | spath input=aclControlList output=grantee path=Grant{} | mvexpand grantee | spath input=grantee 
| search ""Grantee.URI""=*AllUsers 
| table _time, Permission, Grantee.URI, bucketName, userIdentityArn | sort - _time"
7,,,"First we bring in our basic demo dataset, which is just a count of events per host (see the live search for detail). We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
We're filtering for hosts that have always sent us data, and are not currently sending us data.
Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR.",Expected Host GDPR - Demo,Expected Host GDPR - Demo,,,,,,anon_interactive_logons.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(Raw Event Counts by Host)` 
| where current=0 AND isnotnull(historical) AND historical>1000 
| lookup gdpr_system_category host | search category=*"
7,,,"First we bring in our basic dataset, Symantec Endpoint Operational Logs.
Next we use a relatively complicated stats command to track the time of the last update, and the time of the last error.
Next we filter for the events where the time of the last update was more than three days ago, or where the last error was more recent than the last update.
Finally, we format the timestamps in a human readable way.",Outdated Malware Definitions - Live,Outdated Malware Definitions - Live,,,,,,count,1,Must have Symantec AV data,,"For simplicity, this search is written specifically for Symantec AV data, but it can be easily modified for other sources. ",| metasearch earliest=-24h latest=now index=* sourcetype=symantec:ep:* | head 100 | stats count ," index=* sourcetype=symantec:* 
|stats max(eval(if(like(Event_Description, ""%LiveUpdate session ran successfully%"") , _time, null))) as LatestUpdate max(_time) as LatestMessage max(eval(if(tag=""error"", _time, null))) as LatestError by Host_Name 
| where LatestUpdate < relative_time(LatestMessage, ""-3d"") OR LatestError > LatestUpdate 
| convert ctime(LatestUpdate) ctime(LatestMessage) ctime(LatestError)"
7,,,"First we pull in our demo dataset. We are filtering here to just process launches in the Users directory, so that we are focusing in on what unprivileged users can run (and not getting noise from things like software updates).
Next we use the Shannon Entropy algorithm provided by the free app URL Toolbox to calculate a very basic randomness score for this string.
Shannon Entropy gives a numeric score, you will usually want to filter on values above of 3.5 or 4.
Finally we use stats to put everything in a convenient table.
And of course we use rename to provide field names that will make sense to analysts.",Processes With High Entropy Names in Users Directory - Demo,Processes With High Entropy Names in Users Directory - Demo,,,,,,"STE_Win4688.csv
count","1
1","Must have Demo Lookup
Must have URL Toolbox Installed (provides Shannon entropy checking)",,"Verify that lookups installed with Splunk Security Essentials is present
The URL Toolbox app, written by Cedric Le Roux, not only provides effective URL Parsing but also Levenshtein similarity checking (e.g., typo detection) and Shannon entropy detection. Download <a href=""https://splunkbase.splunk.com/app/2734/"">here</a>.","| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah 
| rest /services/apps/local | search disabled=0 label=""URL Toolbox"" | stats count","| `Load_Sample_Log_Data(""Windows 4688 Data"")` | search New_Process_Name=*\Users\*
| lookup ut_shannon_lookup word as New_Process_Name 
| where ut_shannon > 4.5 
| stats  values(ut_shannon)  as ""Shannon Entropy Score"" by New_Process_Name,host 
| rename  New_Process_Name as Process,host as Endpoint "
7,,,"This is one of the longest searches in Splunk Security Essentials, but we'll break it down for you. We start easy: first we pull in our demo dataset.
Earlier versions of sysmon didn't extract a filename by default, so we are adding that in here.
Ultimately when we're analyzing filenames, we are going to want to have a stats by filename, but with the context to understand the broader incident. So here we're adding the hosts that ran that filename, and the Image (file path) that it ran from.
We are about to start the process of comparing multiple standard Windows processes against each process that runs, and recording the scores in a field called ut_levenshtein. To make that work, we first have to initialize the field, so that we can continually add to it.
In this line, we are comparing the filename we see to svchost.exe using the Levenshtein algorithm that comes packaged in the free URL Toolbox app. Levenshtein will compare two strings and count the number of edits it would take to make them match, e.g., svchost.exe vs ssvchost.exe would be a difference of 1 because you'd need to add one character. It is known as an edit distance algorithm. At the end, we are using eval's mvappend command to add the score to the field levenshtein_scores, which by the end will have four different values.
This one is a neat SPL trick, though it only works in this one scenario. When you have an eval (and not an eval inside of a | foreach, or anything like that), you specify another variable in curly braces and it will insert the value of that variable. So here, ut_levenshtein contains a numeric score.. let's suppose it is a 2 for argument sake. What we're going to end up doing is assigning the value of comparisonterm (svchost.exe) to a field called score2. Note that this only works when it's on the left hand side in an eval statement (e.g., you can't create pointers, for those coming from C/C++ land), and it doesn't work inside of things like foreach. But still, it can allow you to do some awesome things you probably didn't expect.
Now we're going to repeat lines 4-6 for the term iexplore.exe, again adding the score to levenshtein_scores.
Now we're going to repeat lines 4-6 for the term ipconfig.exe, again adding the score to levenshtein_scores.
Now we're going to repeat lines 4-6 for the term explorer.exe, again adding the score to levenshtein_scores.
Before we filter, we're going to grab the total number of hosts in our environment, so we can filter out files that are seen by all of them (unlikely to be malware).
Now we filter out noise. Generally with Levenshtein, we look for scores that are greater then 0 (i.e., not an exact match), but less than 3.
Great, we now just have suspicious process launches, so it's now time to start making this usable for an analyst. To start with, let's grab that lowest levenshtein_scores value so we can tell them that.
Now we need to pull out the matching suspicious filename. This uses foreach, which basically iterates over anything starting with score (remember that SPL trick from line 6?), and if the score is less than 3, it will add it to the suspect_files field.
Next we calculate what percentage of the environment shows this filename.
Finally! Finally, we create a nice table",Processes With Lookalike Filenames - Demo,Processes With Lookalike Filenames - Demo,,,,,,"generic_sysmon_service_launch_logs.csv
count","1
1","Must have Demo Lookup
Must have URL Toolbox Installed (provides Levenshtein lookalike detection)",,"Verify that lookups installed with Splunk Security Essentials is present
The URL Toolbox app, written by Cedric Le Roux, not only provides effective URL Parsing but also Levenshtein similarity checking (e.g., typo detection) and Shannon entropy detection (random characters). Download <a href=""https://splunkbase.splunk.com/app/2734/"">here</a>.","| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah 
| rest /services/apps/local | search disabled=0 label=""URL Toolbox"" | stats count","| `Load_Sample_Log_Data(""Generic Sysmon Process Launches"")` 
| rex field=Image ""(?<filename>[^\\\\/]*$)"" 
| stats values(host) as hosts dc(host) as num_hosts values(Image) as Images by filename 
| eval levenshtein_scores=null 
| eval comparisonterm=""svchost.exe""  | lookup ut_levenshtein_lookup word1 as filename, word2 as comparisonterm | eval levenshtein_scores=mvappend(levenshtein_scores, ut_levenshtein)
| eval score{ut_levenshtein} = comparisonterm 
| eval comparisonterm=""iexplore.exe"" | lookup ut_levenshtein_lookup word1 as filename, word2 as comparisonterm | eval levenshtein_scores=mvappend(levenshtein_scores, ut_levenshtein) | eval score{ut_levenshtein} = comparisonterm 
| eval comparisonterm=""ipconfig.exe"" | lookup ut_levenshtein_lookup word1 as filename, word2 as comparisonterm | eval levenshtein_scores=mvappend(levenshtein_scores, ut_levenshtein) | eval score{ut_levenshtein} = comparisonterm 
| eval comparisonterm=""explorer.exe"" | lookup ut_levenshtein_lookup word1 as filename, word2 as comparisonterm | eval levenshtein_scores=mvappend(levenshtein_scores, ut_levenshtein) | eval score{ut_levenshtein} = comparisonterm 
| eventstats max(num_hosts) as max_num_hosts 
| where isnull(mvfilter(levenshtein_scores=""0"")) AND min(levenshtein_scores) <3 
| eval lowest_levenshtein_score=min(levenshtein_scores) 
| eval suspect_files = null | foreach score* [eval temp = ""<<FIELD>>"" | rex field=temp ""(?<num>\d*)$"" | eval suspect_files=if(num<3,mvappend('<<FIELD>>', suspect_files),suspect_files) | fields - temp ""<<FIELD>>""] 
| eval percentage_of_hosts_affected = round(100*num_hosts/max_num_hosts,2)
| table filename lowest_levenshtein_score suspect_files Images hosts num_hosts percentage_of_hosts_affected"
,,,"First we load our basic Windows Registry data and filter for some of the most common AutoRuns keys that are seen in the wild
Finally we put everything in a table that is easy for analysts to read",Suspicious Windows Registry activity - Live,Suspicious Windows Registry activity - Live,,,,,,count,1,Must have registry data from Windows TA,,This data is provided by the Windows TA. Consider using the TA for a better experience.,| metasearch index=* sourcetype=WinRegistry earliest=-1h latest=now | stats count,"index=* sourcetype=WinRegistry process_image=""*AppData\\*"" key_path=""*currentversion\\run*"" 
| table _time, host, process_image, key_path"
7,,,"First we start by pulling our demo email logs, where we have a source address (this could also work for proxy logs!)
This is an intensive exercise, so let's start by aggregating per source address, so we don't end up running over the same email many times
Next we are going to extract the domain -- probably this should actually occur before the last stats, but the performance is similar and this way it matches the accelerated search where this step is required.
Now we aggregate per actual domain we will analyze, for performance reasons
Let's filter out any domains that our organization owns and expects to receive email from. You can have several domains here (I recommend no more than 10-20 -- eventually urltoolbox will get tired and stop doing adding levenshtein fields, so you can look for null ut_levenshtein later if you are pushing this boundary).
Now we use the free URL Toolbox app to parse out subdomains from the top level domains. We want to analyze each one, so that an attacker can't send mycompany.yourithelpdesk.com and get through, or mail.mycampany.com.
The field we are going to pass to the levenshtein algorithm is domain_detected, so let's add each subdomain to the multi-value field domain_detected.
This step is not required, but I like to filter down the list of fields mid-search just to make it easier for me to read and track it. URL Toolbox adds a *lot* of fields, but these four are the only fields I care about from now on.
Last piece of prep -- let's simplify everything exactly the two fields that URL Toolbox's levenshtein algorithm is expecting.
Now the real magic: URL Toolbox is given two multi-value fields, and it does the cross checking to calculate the levenshtein score for each combination. We pull out the lowest score from this group.
Now we filter for a levenshtein score less than three (so two or fewer changes required to go from the domain to one of our standard domains). Those who have used levenshtein are likely thinking: ""Wait, what about the > 0 that we always use?"" -- we accomplished that by filtering out standard domains way back at the start.
Finally we do some | fields and | rename so that everything looks nice and friendly for analysts to understand what we're looking at.",Emails With Lookalike Domains - Demo,Emails With Lookalike Domains - Demo,,,,,,"Anonymized_Email_Logs.csv
count","1
1","Must have Demo Lookup
Must have URL Toolbox Installed (provides Levenshtein lookalike detection and domain parsing)",,"Verify that lookups installed with Splunk Security Essentials is present
The URL Toolbox app, written by Cedric Le Roux, not only provides effective URL Parsing but also Levenshtein similarity checking (e.g., typo detection) and Shannon entropy detection (random characters). Download <a href=""https://splunkbase.splunk.com/app/2734/"">here</a>.","| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah 
| rest /services/apps/local | search disabled=0 label=""URL Toolbox"" | stats count","| `Load_Sample_Log_Data(""Email Logs"")`
| stats count by Sender 
| rex field=Sender ""\@(?<domain_detected>.*)"" 
| stats sum(count) as count by domain_detected 
| eval domain_detected=mvfilter(domain_detected!=""mycompany.com"" AND domain_detected!=""company.com"" AND domain_detected!=""mycompanylovestheenvironment.com"") 
| eval list=""mozilla"" | `ut_parse_extended(domain_detected, list)` 
| foreach ut_subdomain_level* [eval orig_domain=domain_detected, domain_detected=mvappend(domain_detected, '<<FIELD>>' . ""."" . ut_tld)] 
| fields orig_domain domain_detected ut_domain count   
| eval word1=mvappend(domain_detected, ut_domain), word2 = mvappend(""mycompany.com"", ""company.com"", ""mycompanylovestheenvironment.com"") 
| lookup ut_levenshtein_lookup word1 word2 | eval ut_levenshtein= min(ut_levenshtein) 
| where ut_levenshtein < 3 
| fields - domain_detected ut_domain | rename orig_domain as top_level_domain_in_incoming_email word1 as domain_names_analyzed word2 as company_domains_used count as num_occurrences ut_levenshtein as Levenshtein_Similarity_Score"
7,,,"First we bring in our basic demo dataset, the output of the ldapsearch search command against an AD environment. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
Active Directory gives us timestamps in a couple of formats, none of which allow us to do comparisons. To solve this, we use the convert search command to convert those other time formats into Unix epoch time (a nice and clean number) that we can do math on.
This step is only required for demo data -- eval has a now() function that will tell us the current epoch time, but because we're using demo data that gets more out of date with every day that passes, we have to calculate that manually. You won't see this in the live queries.
Now we look for any accounts where the password is more than 120 days old, and the login is in the last 30 days. Why 120 days? If someone's password expires while they're on vacation, we don't want to alert, so this gives us a month of buffer. We also are filtering for only accounts that are actively used, so that we don't trigger on old disabled accounts (though that's also a worthwhile detection!). 
We've got our suspect values, our last step is to format the data to be more meaningful to an analyst by converting those numeric timestamps into human-readable ones. We also clear out the unnecessary maxtime field.",Old Passwords In Use - Demo,Old Passwords In Use - Demo,,,,,,UC_active_directory_search.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(LDAPSearch Output)` 
| convert timeformat=""%Y-%m-%dT%H:%M:%S.%6QZ"" mktime(pwdLastSet) mktime(lastLogonTimestamp) | convert timeformat=""%Y%m%d%H%M%S.0Z""  mktime(whenCreated) 
| eventstats max(lastLogonTimestamp) as maxtime
| where pwdLastSet < relative_time(maxtime, ""-120d"") AND lastLogonTimestamp > relative_time(maxtime, ""-30d"") 
| convert ctime(lastLogonTimestamp) ctime(whenCreated) ctime(pwdLastSet) | fields - maxtime"
,,,"First we load our basic demo data
Next we filter our search for just Windows Update messages that are related to specific KB we know we need to focus on.
Now we have a large number of events and we will want to roll them up into something more usable. stats is great at that!
And finally we can filter for exactly the events we care about.",Windows Updates Install Failure - Demo,Windows Updates Install Failure - Demo,,,,,,UC_unsuccessful_windows_updates.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| inputlookup UC_unsuccessful_windows_updates 
| search signature_id=KB4012598 OR signature_id=KB4012212 OR signature_id=KB4012215 OR signature_id=KB4012213 OR signature_id=KB4012216 OR signature_id=KB4012214 OR signature_id=KB4012217 OR signature_id= KB4012606 OR signature_id=KB4013198 OR signature_id= KB4013429 
| stats last(status) as lastStatus by _time, dest, signature, signature_id 
| search lastStatus=failure"
,,,"First we load our NetBackup data and filter for the specific message that NetBackup sends for successful backups
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the day.
Finally we can look at the hosts that are successfully backed up over time, thanks to stats.",Succesful Backups - Live,Succesful Backups - Live,,,,,,count,1,Must have Netbackup Logs,,Consider ingesting your logs from your Netbackup systems.,| metasearch index=* sourcetype=netbackup_logs earliest=-1d latest=now | stats count,"index=* sourcetype=""netbackup_logs"" ""Disk/Partition backup completed successfully."" 
| bucket _time span=1d 
| stats values(COMPUTERNAME) as ""Systems Backed Up"" by _time, MESSAGE"
7,,,"First we pull in our demo dataset.
This line won't exist in production, it is just so that we can format the demo data (coming from a CSV file) correctly.
Next we filter to make sure we're looking for just account creation events or account changes with group membership events.
Transaction will now group everything together so that we can see multiple events occurring to the same username.
We can now filter for users where both event IDs occurred.
Finally we can display everything in a nice table for the user to consume.",New Local Admin - Demo,New Local Admin - Demo,,,,,,Local_Short_Lived_Account.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","|`Load_Sample_Log_Data(""Local Short-Lived Account"")` 
| rex mode=sed field=Security_ID ""s/
/;/g"" | makemv Security_ID delim="";""| makemv Account_Name delim="";"" 
| search EventCode=4720 OR (EventCode=4732 Administrators) 
| transaction Security_ID maxspan=180m 
| search EventCode=4720 EventCode=4732
| table _time EventCode Security_ID Group_Name Account_Name Message "
7,,,"First we pull in our demo dataset, which comes from Firewall Logs and targets scenarios where we will have a small number of DNS connections with a large amount of volume.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the last hour.
Now we are looking at the number of bytes sent per source ip per hour, over our time range (usually the last day).
Eventstats then allows us to calculate all manner of statistics. This is one of the more complicated stats syntaxes that you will see, but it's actually not that complicated. The big component here is leveraging stats+eval, where we can embed the flexible logic of eval inside of stats. In this case, when we are calculating our average and standard deviation, we really want to exclude the most recent values (that which we're concerned about), so that they don't sway our average (imagine you churn along at 1 kb per hour, then in the last hour it's 150 MB.. you really want your normal baseline to be 1 KB). One other note here -- we are doing two different eventstats, one on a global basis, one on a per host basis. That's so we can try to identify a host that's always at the top of the charts, while overall looking across the org, giving us a good balance across servers with static IPs and DHCP hosts that move around.
Here's where we really start doing the important work. Our lengthy eventstats gave us all these fields that we can filter on and interpret based on. (When testing this out, feel free to remove this line and those that follow, so you can see the raw fields coming out of eventstats.) Now we need to filter for hosts that are substantially above the norm.
From our last line, we have focused in to just hosts that are behaving abnormally. Here, we are using eval to add another field to the results -- not one focused on detection logic, but to try to add context and summarize some of the maths for an analyst to see why we are surfacing this host.
Finally, we clear up some of the nonsense fields we don't care that much about, again to make things clearer for the analyst.",Huge Volume of DNS Traffic - Demo,Huge Volume of DNS Traffic - Demo,,,,,,dns_data_anon.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","|  `Load_Sample_Log_Data(""DNS Logs"")`
| bucket _time span=1h 
| stats sum(bytes*) as bytes* by src_ip _time 
| eventstats max(_time) as maxtime avg(bytes_out) as avg_bytes_out stdev(bytes_out) as stdev_bytes_out | eventstats count as num_data_samples avg(eval(if(_time < relative_time(maxtime, ""@h""),bytes_out,null))) as per_source_avg_bytes_out stdev(eval(if(_time < relative_time(maxtime, ""@h""),bytes_out,null))) as per_source_stdev_bytes_out by src_ip 
 | where num_data_samples >=4 AND bytes_out > avg_bytes_out + 3 * stdev_bytes_out AND bytes_out > per_source_avg_bytes_out + 3 * per_source_stdev_bytes_out AND _time >= relative_time(maxtime, ""@h"") 
| eval num_standard_deviations_away_from_org_average = round(abs(bytes_out - avg_bytes_out) / stdev_bytes_out,2), num_standard_deviations_away_from_per_source_average = round(abs(bytes_out - per_source_avg_bytes_out) / per_source_stdev_bytes_out,2) 
| fields - maxtime per_source* avg* stdev*"
7,,,"First we bring in our basic demo dataset, Symantec Endpoint Protection Risks. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
While there are several approaches to grouping events, and stats is the fastest, we're using transaction because it's the easiest. This will let us group all the events based on the Risk_Name.
Finally, we can look to see if there are more than three different computers that have been affected.",Basic Malware Outbreak - Demo,Basic Malware Outbreak - Demo,,,,,,anon_interactive_logons.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(Symantec Endpoint Protection Risks)` 
| transaction maxpause=1d Risk_Name 
| where mvcount(Computer_Name)>3"
,,,"First we load our Sysmon data. We care primarily about file writes (code 11) or timestamp changes (code 2), so we filter for that
Next we use the rex command to extract file extensions using a moderately complex regular expression.
Now that we have our file extensions, we want to look them up. Splunk has a capability of looking up data in a CSV file through the lookup command. This will take the file extension we just extracted, ""look it up"" in the csv file, and then add any new fields.
The field from the lookup is ""Name"" so we can now search for any true Name field.
And finally we can pull out all the filenames and put them into a usable format via the stats command.",Ransomware Extensions - Live,Ransomware Extensions - Live,,,,,,"count
count","1
1","Must have Microsoft sysmon logs
Must have file events (EventCode=2 or EventCode=11)",,"Sysmon is a free Microsoft tool that provides all kinds of great value. Consider pulling the data in via our <a href=""https://splunkbase.splunk.com/app/1914/"">Splunk App</a>. Check out the <a href=""http://conf.splunk.com/files/2016/slides/splunking-the-endpoint-hands-on.pdf"">Splunking the Endpoint</a> .conf presentation to see what you can do with this data!
Check your sysmon configuration file to ensure you are not filtering out EventCode 2 and EventCode 11 events.","| metasearch index=* sourcetype=""xmlwineventlog:microsoft-windows-sysmon/operational""  earliest=-1h latest=now | stats count
sourcetype=""xmlwineventlog:microsoft-windows-sysmon/operational"" (EventCode=2 OR EventCode=11) index=* | head 100 | stats count","index=* sourcetype=XmlWinEventLog:Microsoft-Windows-Sysmon/Operational EventCode=2 OR EventCode=11 
| rex field=TargetFilename ""^\S+(?<extension>\.\S+)$"" 
| lookup ransomware_extensions_lookup Extensions AS extension 
| search Name!=false  
| stats values(TargetFilename) AS ""Files Written"" by _time, host, Image, Name, extension"
,,,"First we load our basic demo data
Next we are going to look for any process launches (Sysmon EventCode 1) that are being launched from the standard Windows x86 or x64 system directories.
In order for us to see if those filenames are typically associated with Windows processes, we need to get the filename alone. Here, the rex command allows us to do a relatively simple regex to extract that filename (though Splunk has other mechanisms).
Now that we have our filenames, we want to look them up. Splunk has a capability of looking up data in a CSV file through the lookup command. This will take the filename we just extracted, ""look it up"" in the csv file, and then add any new fields.
The field from the lookup is ""systemFile"" so we can now search for any true systemFile field.
And finally we can pull out all the filenames and put them into a usable format via the table command.",Fake Windows Processes - Demo,Fake Windows Processes - Demo,,,,,,UC_fake_win_process.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah "," | inputlookup UC_fake_win_process 
| search EventCode=1 Image!=*Windows\\System32* Image!=*Windows\\SysWOW64* 
| rex field=Image .*\\\(?<filename>\S+)\s?$ 
| lookup isWindowsSystemFile_lookup filename 
| search systemFile=true 
| table _time dest host user Image"
,,,"First we load our Vuln Scanning demo data
Then we filter the specific CVEs that we care about (in this case, for WannaCry related exploits
Vuln data is refreshed over time, so we will want that context. Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to that day.
Finally, we can use stats to put this data in a usable format, showing the CVEs per host, per status, per day.",Vulnerabilities Exploited by Ransomwares - Demo,Vulnerabilities Exploited by Ransomwares - Demo,,,,,,UC_ransomware_vulnerabilities.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| inputlookup UC_ransomware_vulnerabilities 
| search (cve = cve-2017-0143 OR cve = cve-2017-0144 OR cve = cve-2017-0145 OR cve = cve-2017-0146 OR cve = cve-2017-0147 OR cve = cve-2017-0148 OR cve = cve-2014-6332 OR cve = cve-2012-0158 OR cve = cve-2014-4114 OR cve = cve-2014-1761 OR cve = cve-2013-3906 OR cve = cve-2015-1641) 
| bucket _time span=1d 
| stats values(cve) as CVEs by _time, signature, netbios-name, hostname"
7,,,"First we bring in our basic dataset, proxy logs, over the last 10 minutes. 
Then we just filter for any events that are larger than about 35 MB.
Finally we put things in a nice table so that it's easy to read.",Large Web Upload - Live,Large Web Upload - Live,,,,,,count,1,Must have Proxy data,,"Proxy data can come in many forms, including from Palo Alto Networks and other NGFWs, dedicated proxies like BlueCoat, or network monitoring tools like Splunk Stream or bro.",| metasearch earliest=-2h latest=now index=* sourcetype=pan:threat OR (sourcetype=opsec URL Filtering) OR sourcetype=bluecoat:proxysg* OR sourcetype=websense* | head 100 | stats count ,"index=* sourcetype=pan:threat OR (tag=web tag=proxy) earliest=-10m 
| where bytes_out>35000000 
| table _time src_ip user bytes* app uri "
,,,"First we pull in our basic dataset, which comes from Firewall Logs for SMB connections.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the last day.
Now we are looking at the number of connections per source / dest IP pair per day, over our time range",Detect SMB Traffic Allowed - Live,Detect SMB Traffic Allowed - Live,,,,,,count,1,Must have network traffic.,,"Ingest network traffic logs, consider using Splunk Stream.",index=* ((tag=network tag=communicate) OR (sourcetype=pan*traffic OR sourcetype=opsec OR sourcetype=cisco:asa OR sourcetype=stream*)) earliest=-1h latest=now | stats count,"index=* ((tag=network tag=communicate) OR (sourcetype=pan*traffic OR sourcetype=opsec OR sourcetype=cisco:asa OR sourcetype=stream* )) action=allowed (app=smb OR dest_port=139 OR dest_port=445) 
| bucket _time span=1d 
| stats count by _time src_ip dest_ip dest_port"
7,,,"First we pull in our basic dataset, which comes from Firewall Logs and targets scenarios where we will have a large number of DNS connections with a small amount of volume each.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the last hour.
Now we are looking at the number of bytes sent per source ip per hour, over our time range (usually the last day).
Eventstats then allows us to calculate all manner of statistics. This is one of the more complicated stats syntaxes that you will see, but it's actually not that complicated. The big component here is leveraging stats+eval, where we can embed the flexible logic of eval inside of stats. In this case, when we are calculating our average and standard deviation, we really want to exclude the most recent values (that which we're concerned about), so that they don't sway our average (imagine you churn along at 1 kb per hour, then in the last hour it's 150 MB.. you really want your normal baseline to be 1 KB). One other note here -- we are doing two different eventstats, one on a global basis, one on a per host basis. That's so we can try to identify a host that's always at the top of the charts, while overall looking across the org, giving us a good balance across servers with static IPs and DHCP hosts that move around.
Here's where we really start doing the important work. Our lengthy eventstats gave us all these fields that we can filter on and interpret based on. (When testing this out, feel free to remove this line and those that follow, so you can see the raw fields coming out of eventstats.) Now we need to filter for hosts that are substantially above the norm.
From our last line, we have focused in to just hosts that are behaving abnormally. Here, we are using eval to add another field to the results -- not one focused on detection logic, but to try to add context and summarize some of the maths for an analyst to see why we are surfacing this host.
Finally, we clear up some of the nonsense fields we don't care that much about, again to make things clearer for the analyst.",Huge Volume of DNS Requests - Live,Huge Volume of DNS Requests - Live,,,,,,"count
count","1
1","Must have Firewall data
Must have a src_ip and bytes_out field",,"This search requires Firewall or Netflow data to run. By default, we're checking for Common Information Model compliant data, and then also manually specifying the standard sourcetypes for Check Point, Palo Alto Networks, and Cisco ASAs. You should specify your particular index and sourcetype in the actual search to improve performance (or better yet, accelerate with the common information model!)
This search is also looking for firewall logs, but with the added filter of making sure that a src_ip defined and bytes_out>0.","(tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)| head 100 | stats count 
((tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)) src_ip=* bytes_out>0| head 100 | stats count ","((tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa) ) dest_port=53 
| bucket _time span=1h 
| stats count by src_ip _time 
| eventstats max(_time) as maxtime avg(count) as avg_count stdev(count) as stdev_count | eventstats count as num_data_samples avg(eval(if(_time < relative_time(maxtime, ""@h""),count,null))) as per_source_avg_count stdev(eval(if(_time < relative_time(maxtime, ""@h""),count,null))) as per_source_stdev_count by src_ip  
| where num_data_samples >=4 AND count > avg_count + 3 * stdev_count AND count > per_source_avg_count + 3 * per_source_stdev_count AND _time >= relative_time(maxtime, ""@h"") 
| eval num_standard_deviations_away_from_org_average = round(abs(count - avg_count) / stdev_count,2), num_standard_deviations_away_from_per_source_average = round(abs(count - per_source_avg_count) / per_source_stdev_count,2) 
| fields - maxtime per_source* avg* stdev*"
7,,,"First we bring in our basic demo dataset, Symantec Endpoint Protection Risks. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
Next we use stats to calculate the distance between the earliest and latest time of an infection with the range() function. This will give us a number in seconds.
Then we filter for a range of greater than 30 minutes (so at least 30 minutes between infections).
Finally we do some formatting to provide usable numbers, so that no one has to calculate seconds to days.",Recurring Infection on Host - Demo,Recurring Infection on Host - Demo,,,,,,anon_interactive_logons.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(Symantec Endpoint Protection Risks)` 
| stats count range(_time) as TimeRange by Risk_Name, Computer_Name 
| where TimeRange>1800 
| eval TimeRange_In_Hours = round(TimeRange/3600,2), TimeRange_In_Days = round(TimeRange/3600/24,2)"
7,,,"First we bring in our basic demo dataset. In this case, we are using risk events from Symantec Endpoint Protection. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
To detect uncleaned Malware, we look for where the action taken is not the primary or secondary action expected.
Finally we put everything in a nice and usable table!",Endpoint Uncleaned Malware - Demo,Endpoint Uncleaned Malware - Demo,,,,,,anon_interactive_logons.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(Symantec Endpoint Protection Risks)` 
| where Actual_Action!=Requested_Action AND Actual_Action!=Secondary_Action  
| table _time Action_Action Requested_Action Secondary_Action Risk_Name File_Path Computer_Name"
,,,"First we load our basic demo data
Next we look for any instances of fsutil being launched (EventCode 1 indicates a process launch), and filter to make sure our suspicious fields re in the CommandLine string.
Then we put the data into a table because that's the easiest thing to use.",Deleting USN Journal Log - Demo,Deleting USN Journal Log - Demo,,,,,,UC_fsutil.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| inputlookup UC_fsutil 
| search EventCode=1 Image=*fsutil* CommandLine=*usn* CommandLine=*deletejournal* 
| table _time host Image CommandLine"
,,,"First we load our basic demo data
Then we filter for some of the most common AutoRuns keys that are seen in the wild
Finally we put everything in a table that is easy for analysts to read",Suspicious Windows Registry activity - Demo,Suspicious Windows Registry activity - Demo,,,,,,UC_autorun_reg_keys.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| inputlookup UC_autorun_reg_keys 
| search process_image=""*AppData\\*"" key_path=""*currentversion\\run*"" 
| table _time, host, process_image, key_path"
7,,,"First we use tstats to grab events that were indexed in the last half hour, with timestamps ranging from the last half hour to the distant future.
Next we look for the time ranges from a single host. This is a quick and dirty baseline, because if we're constantly running this search and an attacker changes the system time from now to a year from now, there will be some period when there is a massive range of timestamps coming from that host.
Finally we filter for those large ranges.",Systems with Timestamps Far Into the Future - Live,Systems with Timestamps Far Into the Future - Live,,,,,,,,,,,,"| tstats count where index=* _index_earliest=-30m earliest=-30m latest=+16y groupby host _time span=10m 
| stats range(_time) as time_range by host 
| where time_range>3600"
,,,"First we load our basic demo data
Next we look for any instances of wevtutil being launched (EventCode 1 indicates a process launch), and filter to make sure our suspicious fields are in the CommandLine string.
Then we put the data into a table because that's the easiest thing to use.",Log Clearing With wevtutil - Demo,Log Clearing With wevtutil - Demo,,,,,,UC_wevtutil.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| inputlookup UC_wevtutil 
| search EventCode=1 Image=*wevtutil* CommandLine=*cl* (CommandLine=*System* OR CommandLine=*Security* OR CommandLine=*Setup* OR CommandLine=*Application*) 
| table _time host Image CommandLine"
7,,,"First we bring in our historical dataset, going back about 7 days.
Then we pull in the number of events seen per host from the last couple of hours.
Now we can use stats to pull those two tstats (with prestats=t) into the light, giving us the recent number, and the historical number per host.
We're filtering for hosts that have always sent us data, and are not currently sending us data.
Finally we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR.",Expected Host GDPR - Live,Expected Host GDPR - Live,,,,,,,,,,,,"| tstats prestats=t count(host) where latest=-2h earliest=-7d index=* groupby host 
| tstats prestats=t append=t count where earliest=-2h index=* groupby host  
| stats count(host) as historical count as current by host 
| where current=0 AND isnotnull(historical) AND historical>1000 
| lookup gdpr_system_category host | search category=*"
7,,,"First we bring in our basic demo dataset. In this case, anonymized Palo Alto Networks logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
Next we filter for access to PII data (Workday though you can apply to any other sources).
Finally, we format just the data that users want to see.",Access to Inscope Resources GDPR - Demo,Access to Inscope Resources GDPR - Demo,,,,,,od_splunklive_fw_data.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(Sample Firewall Data)` 
| search app=workday* 
| table _time user app bytes* src_ip dest_ip"
7,,,"First we pull in our basic dataset, which consists of XML format Sysmon logs from the endpoints (ingested via the sysmon TA). This could be any EDR data source that provides file hash information. Because we're looking for process launches, we then filter for EventCode=1 (the sysmon Process Launch code).
Earlier versions of sysmon didn't extract a filename by default, so we are adding that in here. We also make it lowercase so that Windows' lack of case sensitivity doesn't mess with our analysis.
Next we use stats to calculate how many different filenames that hash ran as, including the filenames and the full cli strings for contextual data.
Finally, we filter for where the same hash ran as at least two filenames, indicating a rename.",Find Processes with Renamed Executables - Live,Find Processes with Renamed Executables - Live,,,,,,count,1,Must have Microsoft sysmon logs,,"Sysmon is a free Microsoft tool that provides all kinds of great value. Consider pulling the data in via our <a href=""https://splunkbase.splunk.com/app/1914/"">Splunk App</a>. Check out the <a href=""http://conf.splunk.com/files/2016/slides/splunking-the-endpoint-hands-on.pdf"">Splunking the Endpoint</a> .conf presentation to see what you can do with this data!","| metasearch index=* sourcetype=""xmlwineventlog:microsoft-windows-sysmon/operational""  earliest=-1h latest=now  | stats count ","index=* sourcetype=""xmlwineventlog:microsoft-windows-sysmon/operational"" EventCode=1 
| rex field=Image ""[\\\/](?<filename>[^\\\/]*)$"" | eval filename=lower(filename)
| stats dc(filename) as NumFilenames values(filename) as Filenames values(Image) as Images by sha1 
| where NumFilenames>1"
,,,"First we load our NGFW demo data
We filter for where the firewall detects the presence of tor, and where we know the source_ip involved.
Finally we put everything in a table so that it's easy to use.",Monitor TOR traffic - Demo,Monitor TOR traffic - Demo,,,,,,UC_tor_traffic,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| inputlookup UC_tor_traffic 
| search app=tor src_ip=* 
| table _time src_ip src_port dest_ip dest_port bytes app"
7,,,"First we bring in our basic dataset. In this case, Windows logs.
Next we use the magic of stats+eval to count how many events there are where the action is success, or the action is failure
Finally we filter for where there is at least one success, and more than 100 failures.",Basic Brute Force - Live,Basic Brute Force - Live,,,,,,"count
count","1
1","Must have Windows Security data
Must have the user field defined",,"This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.
You should have a field called ""user"" defined in your Windows Security logs. This is provided by the Splunk TA for Windows. Consider adding that TA to make for a better experience!","| metasearch earliest=-2h latest=now sourcetype=""*WinEventLog:Security"" index=* | head 100 | stats count 
sourcetype=""*WinEventLog:Security"" earliest=-2h index=* | head 100 | stats dc(user) as count","index=* (sourcetype=win*security OR sourcetype=linux_secure OR tag=authentication) user=* user!=""""  
| stats count(eval(action=""success"")) as successes count(eval(action=""failure"")) as failures by src dest 
| where successes>0 AND failures>100"
,,,"First we load our basic demo data
Next we filter our search for just Windows Update messages that are related to specific KB we know we need to focus on.
Now we have a large number of events and we will want to roll them up into something more usable. stats is great at that!
And finally we can filter for exactly the events we care about.",Windows Successful Updates Install - Demo,Windows Successful Updates Install - Demo,,,,,,UC_successful_windows_updates.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| inputlookup UC_successful_windows_updates 
| search signature_id=KB4012598 OR signature_id=KB4012212 OR signature_id=KB4012215 OR signature_id=KB4012213 OR signature_id=KB4012216 OR signature_id=KB4012214 OR signature_id=KB4012217 OR signature_id= KB4012606 OR signature_id=KB4013198 OR signature_id= KB4013429 
| stats last(status) as lastStatus by _time, dest, signature, signature_id 
| search lastStatus=installed"
,,,"First we pull in our basic dataset, which consists of XML format Sysmon logs from the endpoints (ingested via the sysmon TA). This could be any EDR data source that provides the full CLI string. Because we're looking for process launches, we then filter for EventCode=1 (the sysmon Process Launch code).
Next we use eval to calcuate how long the command line (file path + command line args) is.
Eventstats is like stats, but just adds the results to your existing dataset. So this will give us the avg and stdev of the command line length per host.
Finally we use stats to roll up multiple process launches, giving us the length of that cli string alongside the avg and stdev of the host, per host, per cli string.
With that setup, we can find processes that have substantially longer cli strings (10 * the standard deviation) than the average on this system.",Command line length statistical analysis - Live,Command line length statistical analysis - Live,,,,,,"count
count","1
1","Must have Microsoft sysmon logs
Must have process start events (EventCode=1)",,"Sysmon is a free Microsoft tool that provides all kinds of great value. Consider pulling the data in via our <a href=""https://splunkbase.splunk.com/app/1914/"">Splunk App</a>. Check out the <a href=""http://conf.splunk.com/files/2016/slides/splunking-the-endpoint-hands-on.pdf"">Splunking the Endpoint</a> .conf presentation to see what you can do with this data!
Check your sysmon configuration file to ensure you are not filtering out EventCode 1 events.","| metasearch index=* sourcetype=""xmlwineventlog:microsoft-windows-sysmon/operational""  earliest=-1h latest=now | stats count
sourcetype=""xmlwineventlog:microsoft-windows-sysmon/operational"" EventCode=1 index=* | head 100 | stats count","index=* sourcetype=""xmlwineventlog:microsoft-windows-sysmon/operational"" EventCode=1 
| eval cmdlen=len(CommandLine) 
| eventstats stdev(cmdlen) as stdev,avg(cmdlen) as avg by host 
| stats max(cmdlen) as maxlen, values(stdev) as stdevperhost, values(avg) as avgperhost by host,CommandLine 
| where maxlen> ((10*stdevperhost) + avgperhost)"
7,,,"First we bring in our basic dataset. In this case, firewall logs. We filter for connections using the default rule.
Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR.",FW Default Rules - Live,FW Default Rules - Live,,,,,,count,1,Must have Palo Alto Networks Firewall data,,"This search requires Firewall data. For simplicity, we've hardcoded the pattern of the PAN default rule names, but you could apply the same logic to other firewalls by adding your internal default rule naming scheme.",index=pan_logs sourcetype=pan*traffic earliest=-2h| head 100 | stats count ,"index=pan_logs sourcetype=pan*traffic rule=""*-default"" action=allowed 
| lookup gdpr_system_category.csv host as src_ip | search category=*"
,,,"First we load our Vuln Scanning data and filter the specific CVEs that we care about (in this case, for WannaCry related exploits
Vuln data is refreshed over time, so we will want that context. Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to that day.
Finally, we can use stats to put this data in a usable format, showing the CVEs per host, per status, per day.",Vulnerabilities Exploited by Ransomwares - Live,Vulnerabilities Exploited by Ransomwares - Live,,,,,,count,1,Must have nessus data,,"Ingest your nessus data, use the <a href=""https://splunkbase.splunk.com/app/1710/"">Splunk Add-on for Tenable</a> for best results/",index=* (sourcetype=nessus:scan OR tag=vulnerability) | head 100 | stats count,"index=* (sourcetype=nessus:scan OR tag=vulnerability) (cve = cve-2017-0143 OR cve = cve-2017-0144 OR cve = cve-2017-0145 OR cve = cve-2017-0146 OR cve = cve-2017-0147 OR cve = cve-2017-0148 OR cve = cve-2014-6332 OR cve = cve-2012-0158 OR cve = cve-2014-4114 OR cve = cve-2014-1761 OR cve = cve-2013-3906 OR cve = cve-2015-1641) 
| bucket _time span=1d 
| stats values(cve) as CVEs by _time, signature, netbios-name, hostname"
7,,,"First we pull in our demo dataset. This could be any EDR data source that provides file hash information.
From line one we have our process launch logs, now we need to filter that down to just the potential discovery tools. We do this via a subsearch. A subsearch goes and runs another search, and then takes those results and inserts them into the main search. You can copy-paste that subsearch into a new search window and see what the results look like -- there's a single field called ""search"" that has a bunch of file hashes with ORs between them. That will effectively be inserted into our main search, giving us a really long search string without having to maintain a really long search.
From Line 1-2, we have a list of suspicious process launches. Now we want to see if many of those fire around the same time. Transaction is great for that -- it lets us group together events that all have the same value for a field, in this case the same host. maxpause=5m lets us continue grouping together any events that have no more than 5 minutes between each one.
From line 1-3, we have grouping of suspicious process launches, now we're going to look and see how many different unique programs were launched using mvcount, which gives us the # of events for a multi-value field.
Finally we clean up a few fields that transaction adds, so that we get a nice clean display.",Series of Discovery Hashes - Demo,Series of Discovery Hashes - Demo,,,,,,generic_sysmon_service_launch_logs.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| `Load_Sample_Log_Data(""Generic Sysmon Process Launches"")` 
 |search [|inputlookup tools.csv | search discovery_or_attack=discovery | eval hash=""sha1="" . hash | stats values(hash) as search | eval search=mvjoin(search, "" OR "")] 
| transaction host maxpause=5m 
| where mvcount(Image)>=6
 | fields - _raw closed_txn field_match_sum linecount"
7,,,"First we bring in our basic demo dataset. In this case, showing Interactive Logins. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
Next we check a lookup that shows the user status (this would typically be pulled from SA-ldapsearch or ADMon).
Now we can filter for users where the expiration is at least a day ago (timezones are hard), or that are disabled.
Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR.",Activity Expired Identity GDPR - Demo,Activity Expired Identity GDPR - Demo,,,,,,anon_interactive_logons.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(Interactive Logins)` 
| lookup user_account_status user 
| where _time > relative_time(terminationDate, ""+1d"") 
| lookup gdpr_system_category host as dest OUTPUT category | search category=*"
7,,,"First we bring in our basic dataset, Symantec Endpoint Protection Risks, over the last 24 hours.
While there are several approaches to grouping events, and stats is the fastest, we're using transaction because it's the easiest. This will let us group all the events based on the Computer_Name.
Finally we can filter for if there are at least three events and they spanned at least a few minutes.",Multiple Infections on Host - Live,Multiple Infections on Host - Live,,,,,,count,1,Must have Symantec AV data,,"For simplicity, this search is written specifically for Symantec AV data, but it can be easily modified for other sources. ",| metasearch earliest=-24h latest=now index=* sourcetype=symantec:ep:* | head 100 | stats count ,"index=* sourcetype=symantec:* earliest=-24h 
| transaction maxpause=1h Computer_Name 
| where eventcount >=3 AND duration>240"
7,,,"First we start by pulling our email logs, where we have a source address (this could also work for proxy logs!)
This is an intensive exercise, so let's start by aggregating per source address, so we don't end up running over the same email many times
Next we are going to extract the domain -- probably this should actually occur before the last stats, but the performance is similar and this way it matches the accelerated search where this step is required.
Now we aggregate per actual domain we will analyze, for performance reasons
Let's filter out any domains that our organization owns and expects to receive email from. You can have several domains here (I recommend no more than 10-20 -- eventually urltoolbox will get tired and stop doing adding levenshtein fields, so you can look for null ut_levenshtein later if you are pushing this boundary).
Now we use the free URL Toolbox app to parse out subdomains from the top level domains. We want to analyze each one, so that an attacker can't send mycompany.yourithelpdesk.com and get through, or mail.mycampany.com.
The field we are going to pass to the levenshtein algorithm is domain_detected, so let's add each subdomain to the multi-value field domain_detected.
This step is not required, but I like to filter down the list of fields mid-search just to make it easier for me to read and track it. URL Toolbox adds a *lot* of fields, but these four are the only fields I care about from now on.
Last piece of prep -- let's simplify everything exactly the two fields that URL Toolbox's levenshtein algorithm is expecting.
Now the real magic: URL Toolbox is given two multi-value fields, and it does the cross checking to calculate the levenshtein score for each combination. We pull out the lowest score from this group.
Now we filter for a levenshtein score less than three (so two or fewer changes required to go from the domain to one of our standard domains). Those who have used levenshtein are likely thinking: ""Wait, what about the > 0 that we always use?"" -- we accomplished that by filtering out standard domains way back at the start.
Finally we do some | fields and | rename so that everything looks nice and friendly for analysts to understand what we're looking at.",Emails With Lookalike Domains - Live,Emails With Lookalike Domains - Live,,,,,,"count
count","1
1","Must have Email Data
Must have URL Toolbox Installed (provides Levenshtein lookalike detection)",,"This search requires Email data. The out of the box field extractions support the Common Information Model, including Cisco ESA/Ironport and Microsoft Exchange. If you don't have this data today, we highly recommend ingesting it with the <a href=""https://splunkbase.splunk.com/app/1761/"">Cisco ESA TA</a> or the <a href=""https://splunkbase.splunk.com/app/3225/"">Splunk Add-on for Microsoft Exchange</a>. For best performance, accelerate the email data model from the <a href=""https://splunkbase.splunk.com/app/1621/"">Common Information Model</a>!
The URL Toolbox app, written by Cedric Le Roux, not only provides effective URL Parsing but also Levenshtein similarity checking (e.g., typo detection) and Shannon entropy detection (random characters). Download <a href=""https://splunkbase.splunk.com/app/2734/"">here</a>.","| tstats count where index=* sourcetype=cisco:esa* OR sourcetype=MSExchange*:MessageTracking OR tag=email earliest=-4h
| rest /services/apps/local | search disabled=0 label=""URL Toolbox"" | stats count","index=* sourcetype=cisco:esa* OR sourcetype=MSExchange*:MessageTracking OR tag=email src_user=*
| stats count by src_user 
| rex field=src_user ""\@(?<domain_detected>.*)"" 
| stats sum(count) as count by domain_detected 
| eval domain_detected=mvfilter(domain_detected!=""mycompany.com"" AND domain_detected!=""company.com"" AND domain_detected!=""mycompanylovestheenvironment.com"") 
| eval list=""mozilla"" | `ut_parse_extended(domain_detected, list)` 
| foreach ut_subdomain_level* [eval orig_domain=domain_detected, domain_detected=mvappend(domain_detected, '<<FIELD>>' . ""."" . ut_tld)] 
| fields orig_domain domain_detected ut_domain count   
| eval word1=mvappend(domain_detected, ut_domain), word2 = mvappend(""mycompany.com"", ""company.com"", ""mycompanylovestheenvironment.com"") 
| lookup ut_levenshtein_lookup word1 word2 | eval ut_levenshtein= min(ut_levenshtein) 
| where ut_levenshtein < 3 
| fields - domain_detected ut_domain | rename orig_domain as top_level_domain_in_incoming_email word1 as domain_names_analyzed word2 as company_domains_used count as num_occurrences ut_levenshtein as Levenshtein_Similarity_Score"
7,,,"First we bring in our basic dataset. In this case, successful logins.
Next we check a lookup that shows the user status (this would typically be pulled from SA-ldapsearch or ADMon).
Now we can filter for users where the expiration is at least a day ago (timezones are hard), or that are disabled.",Basic Expired Account - Live,Basic Expired Account - Live,,,,,,"count
count
count","1
1
1","Must have Windows Security data
Must have Logon Success Data
Must have the user field defined",,"This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.
You should log logon events. There are many event IDs that we look for in the underlying logs, but they should all fall into the Audit Successful (or Failed) Logon events in your Windows Audit Policy. (<a href=""https://technet.microsoft.com/en-us/library/cc431373.aspx"">docs</a>)
You should have a field called ""user"" defined in your Windows Security logs. This is provided by the Splunk TA for Windows. Consider adding that TA to make for a better experience!","| metasearch earliest=-2h latest=now sourcetype=""*WinEventLog:Security"" index=* | head 100 | stats count 
sourcetype=""*WinEventLog:Security"" (4624 OR 4647 OR 4648 OR 551 OR 552 OR 540 OR 528 OR 4768 OR 4769 OR 4770 OR 4771 OR 4768 OR 4774 OR 4776 OR 4778 OR 4779 OR 672 OR 673 OR 674 OR 675 OR 678 OR 680 OR 682 OR 683) index=* | head 100 | stats count
sourcetype=""*WinEventLog:Security"" earliest=-2h index=* (4624 OR 4647 OR 4648 OR 551 OR 552 OR 540 OR 528 OR 4768 OR 4769 OR 4770 OR 4771 OR 4768 OR 4774 OR 4776 OR 4778 OR 4779 OR 672 OR 673 OR 674 OR 675 OR 678 OR 680 OR 682 OR 683) | head 100 | stats dc(user) as count","index=* (sourcetype=win*security OR sourcetype=linux_secure OR tag=authentication) user=* user!="""" action=success 
| lookup user_account_status.csv user 
| where _time > relative_time(terminationDate, ""+1d"")"
7,,,"First we bring in our basic dataset. In this case, Firewall Data. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
Next we use stats to caluate the earliest and the latest time we've seen this calculation.
Eventstats allows us to look across the entire dataset and determine what the largest ""latest"" time is. This step is only necessary when you don't have fresh data, so it really only applies to the demo data.
Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR.
Finally, we filter for events where the earliest time seen is within a day of the latest time seen (aka, this is the first time we've seen this).",New Connection GDPR - Demo,New Connection GDPR - Demo,,,,,,anon_interactive_logons.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(Sample Firewall Data)`  
| stats count min(_time) as earliest max(_time) as maxtime  by src_ip, dest_ip 
| eventstats max(maxtime) as maxtime 
| lookup gdpr_system_category host as dest_ip | search category=*
| where earliest>relative_time(maxtime, ""-1d@d"")"
7,,,"First we pull in our accelerated dataset, which comes from Firewall Logs and targets scenarios where we will have a large number of DNS connections with a small amount of volume each. tstats here is giving us the number of bytes sent per source_ip, filtered for dest_port 53
Eventstats then allows us to calculate all manner of statistics. This is one of the more complicated stats syntaxes that you will see, but it's actually not that complicated. The big component here is leveraging stats+eval, where we can embed the flexible logic of eval inside of stats. In this case, when we are calculating our average and standard deviation, we really want to exclude the most recent values (that which we're concerned about), so that they don't sway our average (imagine you churn along at 1 kb per hour, then in the last hour it's 150 MB.. you really want your normal baseline to be 1 KB). One other note here -- we are doing two different eventstats, one on a global basis, one on a per host basis. That's so we can try to identify a host that's always at the top of the charts, while overall looking across the org, giving us a good balance across servers with static IPs and DHCP hosts that move around.
Here's where we really start doing the important work. Our lengthy eventstats gave us all these fields that we can filter on and interpret based on. (When testing this out, feel free to remove this line and those that follow, so you can see the raw fields coming out of eventstats.) Now we need to filter for hosts that are substantially above the norm.
From our last line, we have focused in to just hosts that are behaving abnormally. Here, we are using eval to add another field to the results -- not one focused on detection logic, but to try to add context and summarize some of the maths for an analyst to see why we are surfacing this host.
Finally, we clear up some of the nonsense fields we don't care that much about, again to make things clearer for the analyst.",Huge Volume of DNS Requests - Accelerated,Huge Volume of DNS Requests - Accelerated,,,,,,"count
count
count","1
1
1","Must have data in your Network Traffic data model
Must have an accelerated Network Traffic data model
Network Traffic data model must have a src_ip",,"This search requires Firewall or Netflow data to run. We are searching here for the common information model network traffic data model.
In addition to searching for the common information model network traffic data model, we are telling Splunk to only visit accelerated data models.
In addition to searching for the accelerated common information model network traffic data model, we are telling Splunk to verify that there is a src_ip in this data set.","| tstats count from datamodel=Network_Traffic where earliest=-1h 
| tstats summariesonly=t allow_old_summaries=t count from datamodel=Network_Traffic where earliest=-1h 
| tstats summariesonly=t allow_old_summaries=t dc(All_Traffic.src_ip) as count from datamodel=Network_Traffic where earliest=-1h ","| tstats summariesonly=t allow_old_summaries=t count from datamodel=Network_Traffic where All_Traffic.dest_port=53 by All_Traffic.src_ip _time span=1d 
| eventstats max(_time) as maxtime avg(count) as avg_count stdev(count) as stdev_count | eventstats count as num_data_samples avg(eval(if(_time < relative_time(maxtime, ""@h""),count,null))) as per_source_avg_count stdev(eval(if(_time < relative_time(maxtime, ""@h""),count,null))) as per_source_stdev_count by src_ip  
| where num_data_samples >=4 AND count > avg_count + 3 * stdev_count AND count > per_source_avg_count + 3 * per_source_stdev_count AND _time >= relative_time(maxtime, ""@h"") 
| eval num_standard_deviations_away_from_org_average = round(abs(count - avg_count) / stdev_count,2), num_standard_deviations_away_from_per_source_average = round(abs(count - per_source_avg_count) / per_source_stdev_count,2) 
| fields - maxtime per_source* avg* stdev*"
7,,,"First we bring in our basic demo dataset. This dataset includes interactive logins from Windows Security logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR.
Next we look up the user in the GDPR categorization lookup.
Because a user or host can belong to many different categories, we use mvexpand to split them into a multi-value field.
Finally we look for users who don't have a matching GDPR category, or who aren't authorized for any GDPR information at all.",Login to New System GDPR - Demo,Login to New System GDPR - Demo,,,,,,anon_interactive_logons.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(Interactive Logins)`  
| lookup gdpr_system_category.csv host as dest OUTPUT category as dest_category | search dest_category=* 
| lookup gdpr_user_category user OUTPUT category as user_category 
| makemv delim=""|"" dest_category | makemv delim=""|"" user_category 
| where isnull(user_category) OR user_category != dest_category"
7,,,"First we bring in our basic demo dataset. In this case, DNS logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
Next we filter for connections using the default rule.
Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR.",FW Default Rules - Demo,FW Default Rules - Demo,,,,,,anon_interactive_logons.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(DNS Logs)` 
| search rule=""*-default"" 
| lookup gdpr_system_category.csv host as src_ip | search category=*"
,,,"First we load our Windows Process Launch logs (Event ID 4688), or any Sysmon process launch logs (EventCode 1). We are going to look for any process launches that are being launched from the standard Windows x86 or x64 system directories.
Because we have two different potential data with their own filenames, we are using eval to put them together into one. (Splunk's Common Information Model makes this much easier and more automatic.)
In order for us to see if those filenames are typically associated with Windows processes, we need to get the filename alone. Here, the rex command allows us to do a relatively simple regex to extract that filename (though Splunk has other mechanisms).
Now that we have our filenames, we want to look them up. Splunk has a capability of looking up data in a CSV file through the lookup command. This will take the filename we just extracted, ""look it up"" in the csv file, and then add any new fields.
The field from the lookup is ""systemFile"" so we can now search for any true systemFile field.
And finally we can pull out all the filenames and put them into a usable format via the table command.",Fake Windows Processes - Live,Fake Windows Processes - Live,,,,,,"count
count","1
1","Must have Windows Security Logs or Sysmon Installed
Must have Process Launch Logs (Event ID 4688) Or Sysmon process start events (EventCode=1)",,"Begin ingesting Windows Security Logs or install Sysmon
Begin ingesting Windows Security Logs or install Sysmon","| metasearch earliest=-2h latest=now (sourcetype=""*win*security"" OR sourcetype=""xmlwineventlog:microsoft-windows-sysmon/operational"") index=* | stats count
| metasearch earliest=-2h latest=now (sourcetype=""*win*security"" OR sourcetype=""xmlwineventlog:microsoft-windows-sysmon/operational"") (EventCode=4688 OR EventCode=1) index=* | stats count","index=* (sourcetype=*win*security EventCode=4688 New_Process_Name!=*Windows\\System32* New_Process_Name!=*Windows\\SysWOW64*) OR (sourcetype=XmlWinEventLog:Microsoft-Windows-Sysmon/Operational EventCode=1 Image!=*Windows\\System32* Image!=*Windows\\SysWOW64*) 
| eval process=coalesce(Image, New_Process_Name)  
| rex field=process .*\\\(?<filename>\S+)\s?$ 
| lookup isWindowsSystemFile_lookup filename 
| search systemFile=true 
| table _time dest host user process"
7,,,"First we bring in our basic demo dataset. In this case, Windows logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same hour.
Next we use the magic of stats+eval to count how many events there are where the action is success, or the action is failure
Finally we filter for where there is at least one success, and more than 100 failures.",Brute Force Slow and Low GDPR - Demo,Brute Force Slow and Low GDPR - Demo,,,,,,anon_interactive_logons.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(Windows Brute Force)` 
| lookup gdpr_system_category host as dest | search category=* 
| bucket _time span=1d 
| stats count(eval(action=""success"")) as successes count(eval(action=""failure"")) as failures by src _time 
| where successes>0 AND failures>100"
7,,,"First we bring in our basic demo dataset, proxy logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
Then we just filter for any events that are larger than about 35 MB.",Large Web Upload - Demo,Large Web Upload - Demo,,,,,,bots-webproxy-data.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(Web Proxy Logs)` 
| where bytes_out>35000000"
,,,"First we pull in our demo dataset of Firewall logs
Next we filter for just SMB connections.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the last day.
Now we are looking at the number of connections per source / dest IP pair per day, over our time range",Detect SMB Traffic Allowed - Demo,Detect SMB Traffic Allowed - Demo,,,,,,UC_smb_traffic_allowed.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| inputlookup UC_smb_traffic_allowed 
| search action=allowed (app=smb OR dest_port=139 OR dest_port=445) 
| bucket _time span=1d 
| stats count by _time src_ip dest_ip dest_port"
7,,,"First we bring in our basic dataset. In this case, AWS CloudTrail logs.
Next we pull the last src_ip for the same user using streamstats (sorted based on the user).
Next we look up the AccountId in the GDPR categorization lookup. Because we only care about GDPR Accounts for this example, we filter for only the hosts that are in scope for GDPR. (You could also categorize based on any other field in AWS.)
Here we filter for logins that are GDPR in scope, and in a short enough time range that it would be difficult to travel to distant parts of the globe.
Here we resolve the Last src_ip to a physical location, and stick that in a field so that we can conveniently use it.
Now we resolve the *current* src_ip
Now we calculate the distance using an approximation for the curvature of the earth. Easy, right? I do not understand it, I copy-pasted from https://answers.splunk.com/answers/317935/calculating-distances-between-points-with-geoip-us.html#answer-568451
Here we pull the date of the event, to make this easier to run over longer time windows.
Finally we use stats to collect all of the values into one line, per user, per day, and per set of locations. We're using some specific AWS data fields here -- if you're using a log source like VPN, then you might choose other fields.",Land Speed GDPR - Live,Land Speed GDPR - Live,,,,,,count,1,Must have AWS CloudTrail data (though could be applied to other data sources),,"In order to run this search, you must have AWS CloudTrail data onboard. Visit the <a href=""/app/Splunk_Security_Essentials/data_source?technology=AWS%20CloudTrail"">data onboarding guide for AWS CloudTrail in this app</a>, or browse to <a href=""https://splunkbase.splunk.com/app/1876/"">apps.splunk.com</a> for more information.",| tstats count where earliest=-2h latest=now index=* sourcetype=aws:cloudtrail,"index=* sourcetype=aws:cloudtrail user=* 
| sort 0 user, _time | streamstats window=1 current=f values(_time) as last_time values(src_ip) as last_src_ip by user 
| lookup gdpr_aws_category accountId 
| where isnotnull(category) AND last_src_ip != src_ip AND _time - last_time < 8*60*60 
| iplocation last_src_ip | rename lat as last_lat lon as last_lon | eval location = City . ""|"" . Country . ""|"" . Region
| iplocation src_ip 
| eval rlat1 = pi()*last_lat/180, rlat2=pi()*lat/180, rlat = pi()*(lat-last_lat)/180, rlon= pi()*(lon-last_lon)/180 | eval a = sin(rlat/2) * sin(rlat/2) + cos(rlat1) * cos(rlat2) * sin(rlon/2) * sin(rlon/2) | eval c = 2 * atan2(sqrt(a), sqrt(1-a)) | eval distance = 6371 * c, time_difference_hours = round((_time - last_time) / 3600,2), speed=round(distance/ ( time_difference_hours),2) | fields - rlat* a c 
| eval day=strftime(_time, ""%m/%d/%Y"")
| stats values(accountId) values(awsRegion) values(eventName) values(distance) values(eval(mvappend(last_Country, Country))) as Country values(eval(mvappend(last_City, City))) as City values(eval(mvappend(last_Region, Region))) as Region  values(lat) values(lon)  values(userAgent) max(speed) as max_speed_kph min(time_difference_hours) as min_time_difference_hours by day user distance"
7,,,"First we bring in our basic demo dataset, the output of the ldapsearch search command against an AD environment. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
Active Directory gives us timestamps in a couple of formats, none of which allow us to do comparisons. To solve this, we use the convert search command to convert those other time formats into Unix epoch time (a nice and clean number) that we can do math on.
Now we look for any accounts where the password is more than 120 days old, and the login is in the last 30 days. Why 120 days? If someone's password expires while they're on vacation, we don't want to alert, so this gives us a month of buffer. We also are filtering for only accounts that are actively used, so that we don't trigger on old disabled accounts (though that's also a worthwhile detection!). 
We've got our suspect values, our last step is to format the data to be more meaningful to an analyst by converting those numeric timestamps into human-readable ones.",Old Passwords In Use - CSV,Old Passwords In Use - CSV,,,,,,UC_active_directory_search.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| inputlookup UC_active_directory_search.csv 
| convert timeformat=""%Y-%m-%dT%H:%M:%S.%6QZ"" mktime(pwdLastSet) mktime(lastLogonTimestamp) | convert timeformat=""%Y%m%d%H%M%S.0Z""  mktime(whenCreated) 
| where pwdLastSet < relative_time(now(), ""-120d"") AND lastLogonTimestamp > relative_time(now(), ""-30d"") 
| convert ctime(lastLogonTimestamp) ctime(whenCreated) ctime(pwdLastSet)"
7,,,"First we bring in our basic dataset, Symantec Endpoint Operational Logs.
Next we use a relatively complicated stats command to track the time of the last update, and the time of the last error.
Next we filter for the events where the time of the last update was more than three days ago, or where the last error was more recent than the last update.
Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR.
Finally, we format the timestamps in a human readable way.",Outdated Malware Definitions GDPR - Live,Outdated Malware Definitions GDPR - Live,,,,,,count,1,Must have Symantec AV data,,"For simplicity, this search is written specifically for Symantec AV data. Some other AV vendors will report similar information.",| metasearch earliest=-24h latest=now index=* sourcetype=symantec:ep:* | head 100 | stats count ," index=* sourcetype=symantec:* 
|stats max(eval(if(like(Event_Description, ""%LiveUpdate session ran successfully%"") , _time, null))) as LatestUpdate max(_time) as LatestMessage max(eval(if(tag=""error"", _time, null))) as LatestError by Host_Name 
| where LatestUpdate < relative_time(LatestMessage, ""-3d"") OR LatestError > LatestUpdate 
| lookup gdpr_system_category Host_Name | search category=* 
| convert ctime(LatestUpdate) ctime(LatestMessage) ctime(LatestError)"
7,,,There is.. lamentably no way to really show the search here. Use the Live Search to get the extreme detail of managing AWS logs!,AWS Public Bucket - Demo,AWS Public Bucket - Demo,,,,,,UC_aws_public_buckets.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ",| `Load_Sample_Log_Data(AWS CloudTrail Public Bucket)` 
7,,,"First we pull in our basic dataset, which consists of XML format Sysmon logs from the endpoints (ingested via the sysmon TA). This could be any EDR data source that provides file hash information. Because we're looking for process launches, we then filter for EventCode=1 (the sysmon Process Launch code).
From line one we have our process launch logs, now we need to filter that down to just the potential attack tools. We do this via a subsearch. A subsearch goes and runs another search, and then takes those results and inserts them into the main search. You can copy-paste that subsearch into a new search window and see what the results look like -- there's a single field called ""search"" that has a bunch of file hashes with ORs between them. That will effectively be inserted into our main search, giving us a really long search string without having to maintain a really long search.
From Line 1-2, we have a list of suspicious process launches. Now we want to see if many of those fire around the same time. Transaction is great for that -- it lets us group together events that all have the same value for a field, in this case the same host. maxpause=5m lets us continue grouping together any events that have no more than 5 minutes between each one.
From line 1-3, we have grouping of suspicious process launches, but also transaction has added a few new fields, such as duration and eventcount. Eventcount lets us see how many process launches are in each transaction (each grouping of suspicious process launches), so we can filter for when there are at least 4 launch events together.
Finally we clear up a few fields that Transaction adds.",Series of Hacker Hashes - Live,Series of Hacker Hashes - Live,,,,,,count,1,Must have Microsoft sysmon logs,,"Sysmon is a free Microsoft tool that provides all kinds of great value. Consider pulling the data in via our <a href=""https://splunkbase.splunk.com/app/1914/"">Splunk App</a>. Check out the <a href=""http://conf.splunk.com/files/2016/slides/splunking-the-endpoint-hands-on.pdf"">Splunking the Endpoint</a> .conf presentation to see what you can do with this data!","| metasearch index=* sourcetype=""xmlwineventlog:microsoft-windows-sysmon/operational""  earliest=-1h latest=now  | stats count ","index=* sourcetype=""xmlwineventlog:microsoft-windows-sysmon/operational"" EventCode=1 
 [|inputlookup tools.csv | search discovery_or_attack=attack | stats values(hash) as search | eval search=mvjoin(search, "" OR "")] 
| transaction host maxpause=5m 
| where eventcount>=4 
| fields - _raw closed_txn field_match_sum linecount"
7,,,"First we pull in our basic dataset, which comes from Firewall Logs and targets scenarios where we will have a small number of DNS connections with a large amount of volume.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the last hour.
Now we are looking at the number of bytes sent per source ip per hour, over our time range (usually the last day).
Eventstats then allows us to calculate all manner of statistics. This is one of the more complicated stats syntaxes that you will see, but it's actually not that complicated. The big component here is leveraging stats+eval, where we can embed the flexible logic of eval inside of stats. In this case, when we are calculating our average and standard deviation, we really want to exclude the most recent values (that which we're concerned about), so that they don't sway our average (imagine you churn along at 1 kb per hour, then in the last hour it's 150 MB.. you really want your normal baseline to be 1 KB). One other note here -- we are doing two different eventstats, one on a global basis, one on a per host basis. That's so we can try to identify a host that's always at the top of the charts, while overall looking across the org, giving us a good balance across servers with static IPs and DHCP hosts that move around.
Here's where we really start doing the important work. Our lengthy eventstats gave us all these fields that we can filter on and interpret based on. (When testing this out, feel free to remove this line and those that follow, so you can see the raw fields coming out of eventstats.) Now we need to filter for hosts that are substantially above the norm.
From our last line, we have focused in to just hosts that are behaving abnormally. Here, we are using eval to add another field to the results -- not one focused on detection logic, but to try to add context and summarize some of the maths for an analyst to see why we are surfacing this host.
Finally, we clear up some of the nonsense fields we don't care that much about, again to make things clearer for the analyst.",Huge Volume of DNS Traffic - Live,Huge Volume of DNS Traffic - Live,,,,,,"count
count","1
1","Must have Firewall data
Must have a src_ip and bytes_out field",,"This search requires Firewall or Netflow data to run. By default, we're checking for Common Information Model compliant data, and then also manually specifying the standard sourcetypes for Check Point, Palo Alto Networks, and Cisco ASAs. You should specify your particular index and sourcetype in the actual search to improve performance (or better yet, accelerate with the common information model!)
This search is also looking for firewall logs, but with the added filter of making sure that a src_ip defined and bytes_out>0.","(network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)| head 100 | stats count 
((tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)) src_ip=* bytes_out>0| head 100 | stats count ","(tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)  
| bucket _time span=1h 
| stats sum(bytes*) as bytes* by src_ip _time 
| eventstats max(_time) as maxtime avg(bytes_out) as avg_bytes_out stdev(bytes_out) as stdev_bytes_out | eventstats count as num_data_samples avg(eval(if(_time < relative_time(maxtime, ""@h""),bytes_out,null))) as per_source_avg_bytes_out stdev(eval(if(_time < relative_time(maxtime, ""@h""),bytes_out,null))) as per_source_stdev_bytes_out by src_ip  
| where num_data_samples >=4 AND bytes_out > avg_bytes_out + 3 * stdev_bytes_out AND bytes_out > per_source_avg_bytes_out + 3 * per_source_stdev_bytes_out AND _time >= relative_time(maxtime, ""@h"") 
| eval num_standard_deviations_away_from_org_average = round(abs(bytes_out - avg_bytes_out) / stdev_bytes_out,2), num_standard_deviations_away_from_per_source_average = round(abs(bytes_out - per_source_avg_bytes_out) / per_source_stdev_bytes_out,2) 
| fields - maxtime per_source* avg* stdev*"
,,,"First we load our Sysmon data (though any EDR / process launch data containing the command line string would suffice). We are looking for any instances of fsutil being launched (EventCode 1 indicates a process launch), and filtering to make sure our suspicious fields re in the CommandLine string.
Then we put the data into a table because that's the easiest thing to use.",Deleting USN Journal Log - Live,Deleting USN Journal Log - Live,,,,,,"count
count","1
1","Must have Microsoft sysmon logs
Must have process start events (EventCode=1)",,"Sysmon is a free Microsoft tool that provides all kinds of great value. Consider pulling the data in via our <a href=""https://splunkbase.splunk.com/app/1914/"">Splunk App</a>. Check out the <a href=""http://conf.splunk.com/files/2016/slides/splunking-the-endpoint-hands-on.pdf"">Splunking the Endpoint</a> .conf presentation to see what you can do with this data!
Check your sysmon configuration file to ensure you are not filtering out EventCode 1 events.","| metasearch index=* sourcetype=""xmlwineventlog:microsoft-windows-sysmon/operational""  earliest=-1h latest=now | stats count
sourcetype=""xmlwineventlog:microsoft-windows-sysmon/operational"" EventCode=1 index=* | head 100 | stats count","index=* sourcetype=XmlWinEventLog:Microsoft-Windows-Sysmon/Operational EventCode=1 Image=*fsutil* CommandLine=*usn* CommandLine=*deletejournal* 
| table _time host Image CommandLine"
,,,"First we pull in our demo dataset. This could be any EDR data source that provides the full CLI string.
Because we just care about process launches, we filter for EventCode 1, which is how sysmon denotes a process launch.
Next we use eval to calcuate how long the command line (file path + command line args) is.
Eventstats is like stats, but just adds the results to your existing dataset. So this will give us the avg and stdev per each host.
Finally we use stats to roll up multiple process launches, giving us the length of that cli string alongside the avg and stdev of the host, per host, per cli string.
With that setup, we can find processes that have substantially longer cli strings (10 * the standard deviation) than the average on this system.",Command line length statistical analysis - Demo,Command line length statistical analysis - Demo,,,,,,UC_malicious_cmdline.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| inputlookup UC_malicious_cmdline 
| search EventCode=1 
| eval cmdlen=len(CommandLine) 
| eventstats stdev(cmdlen) as stdev,avg(cmdlen) as avg by host 
| stats max(cmdlen) as maxlen, values(stdev) as stdevperhost, values(avg) as avgperhost by host,CommandLine 
| where maxlen> ((10*stdevperhost) + avgperhost)"
7,,,"First we bring in our basic dataset, Firewall Logs, from the last hour.
Next, stats gives us the distinct count (aka unique count) of ips and ports that were used per source IP.
Finally we can filter for more than 1000 src_ips or dest_ports.",Basic Scanning - Live,Basic Scanning - Live,,,,,,"count
count","1
1","Must have Firewall data
Must have a dest_ip and dest_port field",,"This search requires Firewall or Netflow data to run. By default, we're checking for Common Information Model compliant data, and then also manually specifying the standard sourcetypes for Check Point, Palo Alto Networks, and Cisco ASAs. You should specify your particular index and sourcetype in the actual search to improve performance (or better yet, accelerate with the common information model!)
This search is also looking for firewall logs, but with the added filter of making sure that a dest_ip and dest_port defined.","(tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)| head 100 | stats count 
((tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa)) dest_ip=* dest_port=*| head 100 | stats count ","(tag=network tag=communicate) OR (index=pan_logs sourcetype=pan*traffic) OR (index=* sourcetype=opsec) OR (index=* sourcetype=cisco:asa) earliest=-1h 
| stats dc(dest_port) as num_dest_port dc(dest_ip) as num_dest_ip by src_ip 
| where num_dest_port > 1000 OR num_dest_ip > 1000"
7,,,"First we bring in our basic demo dataset, Firewall Logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same hour.
Next, stats gives us the distinct count (aka unique count) of ips and ports that were used per source IP, per hour.
Finally we can filter for more than 1000 src_ips or dest_ports.",Basic Scanning - Demo,Basic Scanning - Demo,,,,,,anon_interactive_logons.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(Sample Firewall Data)` 
| bucket _time span=1h 
| stats dc(dest_port) as num_dest_port dc(dest_ip) as num_dest_ip by src_ip, _time 
| where num_dest_port > 1000 OR num_dest_ip > 1000"
7,,,"First we pull in our demo dataset. This could be any EDR data source that provides the full CLI string.
Next we use eval to calcuate how long the command line (file path + command line args) is.
Eventstats is like stats, but just adds the results to your existing dataset. So this will give us the avg and stdev per each host.
Finally we use stats to roll up multiple process launches, giving us the length of that cli string alongside the avg and stdev of the host, per host, per cli string.
With that setup, we can find processes that have substantially longer cli strings (4 * the standard deviation) than the average on this system.",Find Unusually Long CLI Commands - Demo,Find Unusually Long CLI Commands - Demo,,,,,,STE_Sysmon_commandline.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| `Load_Sample_Log_Data(""Sysmon Data"")` 
|  eval cmdlen=len(CommandLine) 
| eventstats stdev(cmdlen) as stdev,avg(cmdlen) as avg by host
| stats max(cmdlen) as maxlen, values(stdev) as stdevperhost, values(avg) as avgperhost by host,CommandLine 
| where maxlen>4*stdevperhost+avgperhost"
,,,"First we load our NetBackup data and filter for the specific message that NetBackup sends for failed backups
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the day.
Finally we can look at the hosts that failed to back up over time, thanks to stats.",Unsuccesful backups - Live,Unsuccesful backups - Live,,,,,,count,1,Must have Netbackup Logs,,Consider ingesting your logs from your Netbackup systems.,| metasearch index=* sourcetype=netbackup_logs earliest=-1d latest=now | stats count,"index=* sourcetype=""netbackup_logs"" ""An error occurred, failed to backup."" 
| bucket _time span=1d 
| stats values(COMPUTERNAME) as ""Back Up Failures"" by _time, MESSAGE "
,,,"First we load our basic demo data
Next we filter for the Event Codes that indicate the Windows event log is being cleared. You can see there are a few possibilities. 
Finally, because we respect analysts, we put it in a nice easy-to-consume table.",Windows Event Log Clearing Events - Demo,Windows Event Log Clearing Events - Demo,,,,,,UC_windows_event_log.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah "," | inputlookup UC_windows_event_log 
| search ((sourcetype=wineventlog:security OR XmlWinEventlog:Security) AND (EventCode=1102 OR EventCode=1100)) OR ((sourcetype=wineventlog:system OR XmlWinEventlog:System) AND EventCode=104) 
| table _time EventCode Message sourcetype host"
7,,,"This is one of the longest searches in Splunk Security Essentials, but we'll break it down for you. We start easy: first we pull in our dataset of process launch events (here from Windows, but could come from any EDR data source).
Earlier versions of sysmon didn't extract a filename by default, so we are adding that in here.
Ultimately when we're analyzing filenames, we are going to want to have a stats by filename, but with the context to understand the broader incident. So here we're adding the hosts that ran that filename, and the Image (file path) that it ran from.
We are about to start the process of comparing multiple standard Windows processes against each process that runs, and recording the scores in a field called ut_levenshtein. To make that work, we first have to initialize the field, so that we can continually add to it.
In this line, we are comparing the filename we see to svchost.exe using the Levenshtein algorithm that comes packaged in the free URL Toolbox app. Levenshtein will compare two strings and count the number of edits it would take to make them match, e.g., svchost.exe vs ssvchost.exe would be a difference of 1 because you'd need to add one character. It is known as an edit distance algorithm. At the end, we are using eval's mvappend command to add the score to the field levenshtein_scores, which by the end will have four different values.
This one is a neat SPL trick, though it only works in this one scenario. When you have an eval (and not an eval inside of a | foreach, or anything like that), you specify another variable in curly braces and it will insert the value of that variable. So here, ut_levenshtein contains a numeric score.. let's suppose it is a 2 for argument sake. What we're going to end up doing is assigning the value of comparisonterm (svchost.exe) to a field called score2. Note that this only works when it's on the left hand side in an eval statement (e.g., you can't create pointers, for those coming from C/C++ land), and it doesn't work inside of things like foreach. But still, it can allow you to do some awesome things you probably didn't expect.
Now we're going to repeat lines 4-6 for the term iexplore.exe, again adding the score to levenshtein_scores.
Now we're going to repeat lines 4-6 for the term ipconfig.exe, again adding the score to levenshtein_scores.
Now we're going to repeat lines 4-6 for the term explorer.exe, again adding the score to levenshtein_scores.
Before we filter, we're going to grab the total number of hosts in our environment, so we can filter out files that are seen by all of them (unlikely to be malware).
Now we filter out noise. Generally with Levenshtein, we look for scores that are greater then 0 (i.e., not an exact match), but less than 3.
Great, we now just have suspicious process launches, so it's now time to start making this usable for an analyst. To start with, let's grab that lowest levenshtein_scores value so we can tell them that.
Now we need to pull out the matching suspicious filename. This uses foreach, which basically iterates over anything starting with score (remember that SPL trick from line 6?), and if the score is less than 3, it will add it to the suspect_files field.
Next we calculate what percentage of the environment shows this filename.
Finally! Finally, we create a nice table",Processes With Lookalike Filenames - Live,Processes With Lookalike Filenames - Live,,,,,,"count
count
count","1
1
1","Must have Windows Security Logs
Must have Process Launch Logs (Event ID 4688)
Must have URL Toolbox Installed (provides Levenshtein lookalike detection)",,"Begin ingesting Windows Security Logs
Turn on Process Tracking in your Windows Audit logs (<a href=""https://technet.microsoft.com/en-us/library/cc976411.aspx"">docs</a>)
The URL Toolbox app, written by Cedric Le Roux, not only provides effective URL Parsing but also Levenshtein similarity checking (e.g., typo detection) and Shannon entropy detection (random characters). Download <a href=""https://splunkbase.splunk.com/app/2734/"">here</a>.","| metasearch index=* earliest=-2h latest=now sourcetype=""WinEventLog:Security"" | stats count 
earliest=-2h latest=now index=* sourcetype=""WinEventLog:Security"" EventCode=4688 | head 100 | stats count 
| rest /services/apps/local | search disabled=0 label=""URL Toolbox"" | stats count","index=* sourcetype=""WinEventLog:Security"" EventCode=4688  
| rex field=Image ""(?<filename>[^\\\\/]*$)"" 
| stats values(host) as hosts dc(host) as num_hosts values(Image) as Images by filename 
| eval levenshtein_scores=null 
| eval comparisonterm=""svchost.exe""  | lookup ut_levenshtein_lookup word1 as filename, word2 as comparisonterm | eval levenshtein_scores=mvappend(levenshtein_scores, ut_levenshtein) 
| eval score{ut_levenshtein} = comparisonterm 
| eval comparisonterm=""iexplore.exe"" | lookup ut_levenshtein_lookup word1 as filename, word2 as comparisonterm | eval levenshtein_scores=mvappend(levenshtein_scores, ut_levenshtein) | eval score{ut_levenshtein} = comparisonterm 
| eval comparisonterm=""ipconfig.exe"" | lookup ut_levenshtein_lookup word1 as filename, word2 as comparisonterm | eval levenshtein_scores=mvappend(levenshtein_scores, ut_levenshtein) | eval score{ut_levenshtein} = comparisonterm 
| eval comparisonterm=""explorer.exe"" | lookup ut_levenshtein_lookup word1 as filename, word2 as comparisonterm | eval levenshtein_scores=mvappend(levenshtein_scores, ut_levenshtein) | eval score{ut_levenshtein} = comparisonterm 
| eventstats max(num_hosts) as max_num_hosts 
| where isnull(mvfilter(levenshtein_scores=""0"")) AND min(levenshtein_scores) <3 
| eval lowest_levenshtein_score=min(levenshtein_scores) 
| eval suspect_files = null | foreach score* [eval temp = ""<<FIELD>>"" | rex field=temp ""(?<num>\d*)$"" | eval suspect_files=if(num<3,mvappend('<<FIELD>>', suspect_files),suspect_files) | fields - temp ""<<FIELD>>""] 
| eval percentage_of_hosts_affected = round(100*num_hosts/max_num_hosts,2)
| table filename lowest_levenshtein_score suspect_files Images hosts num_hosts percentage_of_hosts_affected"
,,,"First we load our Sysmon EDR data. We look for any instances of wevtutil being launched (EventCode 1 indicates a process launch), and filter to make sure our suspicious fields are in the CommandLine string.
Then we put the data into a table because that's the easiest thing to use.",Log Clearing With wevtutil - Live,Log Clearing With wevtutil - Live,,,,,,"count
count","1
1","Must have Microsoft sysmon logs
Must have process start events (EventCode=1)",,"Sysmon is a free Microsoft tool that provides all kinds of great value. Consider pulling the data in via our <a href=""https://splunkbase.splunk.com/app/1914/"">Splunk App</a>. Check out the <a href=""http://conf.splunk.com/files/2016/slides/splunking-the-endpoint-hands-on.pdf"">Splunking the Endpoint</a> .conf presentation to see what you can do with this data!
Check your sysmon configuration file to ensure you are not filtering out EventCode 1 events.","| metasearch index=* sourcetype=""xmlwineventlog:microsoft-windows-sysmon/operational""  earliest=-1h latest=now | stats count
sourcetype=""xmlwineventlog:microsoft-windows-sysmon/operational"" EventCode=1 index=* | head 100 | stats count","index=* sourcetype=XmlWinEventLog:Microsoft-Windows-Sysmon/Operational EventCode=1 Image=*wevtutil* CommandLine=*cl* (CommandLine=*System* OR CommandLine=*Security* OR CommandLine=*Setup* OR CommandLine=*Application*) 
| table _time host Image CommandLine"
7,,,"First we pull in our dataset, of Windows Security Logs with account creation events or account deletions.
Windows Security Logs by default will have two fields for the Account_Name -- the acting username, and the target username. We want the latter (this isn't a canonical always-guaranteed command, but seems to work correctly in this scenario).
Transaction will now group everything together so that we can see multiple events occurring to the same username.
Finally we can display everything in a nice table for the user to consume.",Short Lived Accounts - Live,Short Lived Accounts - Live,,,,,,"count
count","1
1","Must have Windows Security Logs
Must have Local Account Management Logs (Event ID 4720 and 4726)",,"Begin ingesting Windows Security Logs
Turn on Group Management Audit Logs in your Local Windows Security Policy (<a href=""http://whatevernetworks.com/?p=21"">docs</a>)","| metasearch index=* earliest=-2h latest=now sourcetype=""WinEventLog:Security"" | stats count 
| metasearch earliest=-30d latest=now index=* sourcetype=""WinEventLog:Security"" TERM(eventcode=4726) OR TERM(eventcode=4720) | head | stats count ","index=* sourcetype=""WinEventLog:Security""   EventCode=4726 OR EventCode=4720 
| eval Account_Name=mvindex(Account_Name,1) 
| transaction Account_Name maxspan=180m startswith=""EventCode=4726"" endswith=""EventCode=4720""
| table _time EventCode Account_Name Message"
7,,,"First we pull in our basic dataset, which consists of XML format Sysmon logs from the endpoints (ingested via the sysmon TA). This could be any EDR data source that provides file hash information. Because we're looking for process launches, we then filter for EventCode=1 (the sysmon Process Launch code).
From line one we have our process launch logs, now we need to filter that down to just the potential discovery tools. We do this via a subsearch. A subsearch goes and runs another search, and then takes those results and inserts them into the main search. You can copy-paste that subsearch into a new search window and see what the results look like -- there's a single field called ""search"" that has a bunch of file hashes with ORs between them. That will effectively be inserted into our main search, giving us a really long search string without having to maintain a really long search.
From Line 1-2, we have a list of suspicious process launches. Now we want to see if many of those fire around the same time. Transaction is great for that -- it lets us group together events that all have the same value for a field, in this case the same host. maxpause=5m lets us continue grouping together any events that have no more than 5 minutes between each one.
From line 1-3, we have grouping of suspicious process launches, now we're going to look and see how many different unique programs were launched using mvcount, which gives us the # of events for a multi-value field.
Finally we clean up a few fields that transaction adds, so that we get a nice clean display.",Series of Discovery Hashes - Live,Series of Discovery Hashes - Live,,,,,,count,1,Must have Microsoft sysmon logs,,"Sysmon is a free Microsoft tool that provides all kinds of great value. Consider pulling the data in via our <a href=""https://splunkbase.splunk.com/app/1914/"">Splunk App</a>. Check out the <a href=""http://conf.splunk.com/files/2016/slides/splunking-the-endpoint-hands-on.pdf"">Splunking the Endpoint</a> .conf presentation to see what you can do with this data!","| metasearch index=* sourcetype=""xmlwineventlog:microsoft-windows-sysmon/operational""  earliest=-1h latest=now  | stats count ","index=* sourcetype=""xmlwineventlog:microsoft-windows-sysmon/operational"" EventCode=1  
[|inputlookup tools.csv | search discovery_or_attack=discovery | stats values(hash) as search | eval search=mvjoin(search, "" OR "")] 
| transaction host maxpause=5m 
| where mvcount(Image)>=6
 | fields - _raw closed_txn field_match_sum linecount"
7,,,"First we pull in our demo dataset, which comes from Firewall Logs and targets scenarios where we will have a large number of DNS connections with a small amount of volume each.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the last hour.
Now we are looking at the number of bytes sent per source ip per hour, over our time range (usually the last day).
Eventstats then allows us to calculate all manner of statistics. This is one of the more complicated stats syntaxes that you will see, but it's actually not that complicated. The big component here is leveraging stats+eval, where we can embed the flexible logic of eval inside of stats. In this case, when we are calculating our average and standard deviation, we really want to exclude the most recent values (that which we're concerned about), so that they don't sway our average (imagine you churn along at 1 kb per hour, then in the last hour it's 150 MB.. you really want your normal baseline to be 1 KB). One other note here -- we are doing two different eventstats, one on a global basis, one on a per host basis. That's so we can try to identify a host that's always at the top of the charts, while overall looking across the org, giving us a good balance across servers with static IPs and DHCP hosts that move around.
Here's where we really start doing the important work. Our lengthy eventstats gave us all these fields that we can filter on and interpret based on. (When testing this out, feel free to remove this line and those that follow, so you can see the raw fields coming out of eventstats.) Now we need to filter for hosts that are substantially above the norm.
From our last line, we have focused in to just hosts that are behaving abnormally. Here, we are using eval to add another field to the results -- not one focused on detection logic, but to try to add context and summarize some of the maths for an analyst to see why we are surfacing this host.
Finally, we clear up some of the nonsense fields we don't care that much about, again to make things clearer for the analyst.",Huge Volume of DNS Requests - Demo,Huge Volume of DNS Requests - Demo,,,,,,dns_data_anon.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","|  `Load_Sample_Log_Data(""DNS Logs"")`
| bucket _time span=1h 
| stats count by src_ip _time 
| eventstats max(_time) as maxtime avg(count) as avg_count stdev(count) as stdev_count | eventstats count as num_data_samples avg(eval(if(_time < relative_time(maxtime, ""@h""),count,null))) as per_source_avg_count stdev(eval(if(_time < relative_time(maxtime, ""@h""),count,null))) as per_source_stdev_count by src_ip  
| where num_data_samples >=4 AND count > avg_count + 3 * stdev_count AND count > per_source_avg_count + 3 * per_source_stdev_count AND _time >= relative_time(maxtime, ""@h"") 
| eval num_standard_deviations_away_from_org_average = round(abs(count - avg_count) / stdev_count,2), num_standard_deviations_away_from_per_source_average = round(abs(count - per_source_avg_count) / per_source_stdev_count,2) 
| fields - maxtime per_source* avg* stdev*"
7,,,"First we pull in our demo dataset. This could be any EDR data source that provides process launch information.
From line one we have our process launch logs, now we need to filter that down to just the potential discovery tools. We do this via a subsearch. A subsearch goes and runs another search, and then takes those results and inserts them into the main search. You can copy-paste that subsearch into a new search window and see what the results look like -- there's a single field called ""search"" that has a bunch of file names with ORs between them. That will effectively be inserted into our main search, giving us a really long search string without having to maintain a really long search.
From Line 1-2, we have a list of suspicious process launches. Now we want to see if many of those fire around the same time. Transaction is great for that -- it lets us group together events that all have the same value for a field, in this case the same host. maxpause=5m lets us continue grouping together any events that have no more than 5 minutes between each one.
From line 1-3, we have grouping of suspicious process launches, now we're going to look and see how many different unique programs were launched using mvcount, which gives us the # of events for a multi-value field.
Finally we clean up a few fields that transaction adds, so that we get a nice clean display.",Series of Discovery Filenames - Live,Series of Discovery Filenames - Live,,,,,,"count
count","1
1","Must have Windows Security Logs
Must have Process Launch Logs (Event ID 4688)",,"Begin ingesting Windows Security Logs
Turn on Process Tracking in your Windows Audit logs (<a href=""https://technet.microsoft.com/en-us/library/cc976411.aspx"">docs</a>)","| metasearch index=* earliest=-2h latest=now sourcetype=""WinEventLog:Security"" | stats count 
earliest=-2h latest=now index=* sourcetype=""WinEventLog:Security"" EventCode=4688 | head 100 | stats count ","index=* sourcetype=""WinEventLog:Security"" EventCode=4688 
[|inputlookup tools.csv | search discovery_or_attack=discovery | stats values(filename) as search | eval search=mvjoin(search, "" OR "")] 
| transaction host maxpause=5m 
| where mvcount(Image)>=6
| fields - _raw closed_txn field_match_sum linecount"
7,,,"First we bring in our basic dataset. In this case, Firewall Data.
Next we use stats to caluate the earliest and the latest time we've seen this calculation.
Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR.
Finally, we filter for events where the earliest time is within the last day (aka, this is the first time we've seen this).",New Connection GDPR - Live,New Connection GDPR - Live,,,,,,"count
count","1
1","Must have Firewall data
Must have a dest_ip and src_ip field",,"This search requires Firewall or Netflow data to run. By default, we're checking for Common Information Model compliant data, and then also manually specifying the standard sourcetypes for Check Point, Palo Alto Networks, and Cisco ASAs. You should specify your particular index and sourcetype in the actual search to improve performance (or better yet, accelerate with the common information model!)
This search is also looking for firewall logs, but with the added filter of making sure that a dest_ip and src_ip defined.","index=* ((tag=traffic tag=communicate) OR sourcetype=pan*traffic OR sourcetype=opsec OR sourcetype=cisco*)| head 100 | stats count 
index=* ((tag=traffic tag=communicate) OR sourcetype=pan*traffic OR sourcetype=opsec OR sourcetype=cisco*) dest_ip=* src_ip=*| head 100 | stats count ","index=* ((tag=traffic tag=communicate) OR sourcetype=pan*traffic OR sourcetype=opsec OR sourcetype=cisco*) src_ip=* dest_ip=* 
| stats count min(_time) as earliest max(_time) as maxtime  by src_ip, dest_ip 
| lookup gdpr_system_category host as dest_ip | search category=*
| where earliest>relative_time(now(), ""-1d@d"")"
,,,"First we load our Sysmon data. From sysmon data, we care primarily about file writes (code 11) or timestamp changes (code 2), so we filter for that
From line one we have our process launch logs, now we need to filter that down to just the potential attack tools. We do this via a subsearch. A subsearch goes and runs another search, and then takes those results and inserts them into the main search. You can copy-paste that subsearch into a new search window and see what the results look like -- it will return a single column with the name ""TargetFilename"" that include a number of our search strings. That will effectively be inserted into our main search, giving us a really long search string without having to maintain a really long search.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the day.
And finally we can pull out all the filenames and put them into a usable format via the stats command.",Ransomware Notes - Live,Ransomware Notes - Live,,,,,,"count
count","1
1","Must have Microsoft sysmon logs
Must have file events (EventCode=2 or EventCode=11)",,"Sysmon is a free Microsoft tool that provides all kinds of great value. Consider pulling the data in via our <a href=""https://splunkbase.splunk.com/app/1914/"">Splunk App</a>. Check out the <a href=""http://conf.splunk.com/files/2016/slides/splunking-the-endpoint-hands-on.pdf"">Splunking the Endpoint</a> .conf presentation to see what you can do with this data!
Check your sysmon configuration file to ensure you are not filtering out EventCode 2 and EventCode 11 events.","| metasearch index=* sourcetype=""xmlwineventlog:microsoft-windows-sysmon/operational""  earliest=-1h latest=now | stats count
sourcetype=""xmlwineventlog:microsoft-windows-sysmon/operational"" (EventCode=2 OR EventCode=11) index=* | head 100 | stats count","index=* sourcetype=XmlWinEventLog:Microsoft-Windows-Sysmon/Operational EventCode=2 EventCode=11 
[ | inputlookup ransomware_notes_lookup | rename ransomware_notes as TargetFilename | eval TargetFilename=""*"" . TargetFilename | table TargetFilename]
| bucket _time span=1d 
| stats values(TargetFilename) as filesWritten by _time host Image"
7,,,"First we bring in our basic dataset. This could be any EDR data source that provides process launch logs, including Windows 4688 logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
From line one we have our process launch logs, now we need to filter that down to just the potential attack tools. We do this via a subsearch. A subsearch goes and runs another search, and then takes those results and inserts them into the main search. You can copy-paste that subsearch into a new search window and see what the results look like -- there's a single field called ""search"" that has a bunch of file names with ORs between them. That will effectively be inserted into our main search, giving us a really long search string without having to maintain a really long search.
From Line 1-2, we have a list of suspicious process launches. Now we want to see if many of those fire around the same time. Transaction is great for that -- it lets us group together events that all have the same value for a field, in this case the same host. maxpause=5m lets us continue grouping together any events that have no more than 5 minutes between each one.
From line 1-3, we have grouping of suspicious process launches, but also transaction has added a few new fields, such as duration and eventcount. Eventcount lets us see how many process launches are in each transaction (each grouping of suspicious process launches), so we can filter for when there are at least 4 launch events together.
Finally, we filter out a few fields that we don't need here.",Series of Hacker Filenames - Demo,Series of Hacker Filenames - Demo,,,,,,generic_sysmon_service_launch_logs.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| `Load_Sample_Log_Data(""Generic Sysmon Process Launches"")`  
|search [|inputlookup tools.csv | search discovery_or_attack=attack | eval filename=""Image=\""*\\\\"" . filename . ""\"""" | stats values(filename) as search | eval search=mvjoin(search, "" OR "")] 
| transaction host maxpause=5m 
| where eventcount>=4 
| fields - _raw closed_txn field_match_sum linecount"
7,,,"First we bring in our basic dataset, Symantec Endpoint Protection Risks, over the last 24 hours.
While there are several approaches to grouping events, and stats is the fastest, we're using transaction because it's the easiest. This will let us group all the events based on the Risk_Name.
Finally, we can look to see if there are more than three different computers that have been affected.",Basic Malware Outbreak - Live,Basic Malware Outbreak - Live,,,,,,count,1,Must have Symantec AV data,,"For simplicity, this search is written specifically for Symantec AV data, but it can be easily modified for other sources. ",| metasearch earliest=-24h latest=now index=* sourcetype=symantec:ep:* | head 100 | stats count ,"index=* sourcetype=symantec:*  earliest=-24h
| transaction maxpause=1d Risk_Name 
| where mvcount(Computer_Name)>3"
7,,,"First we bring in our basic demo dataset. In this case, anonymized Palo Alto Networks logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
Next we filter for access to PII data (Workday), over any non-HTTPS port. You can also detect this with any mechanism that allows you to analyze whether content is encrypted.
Finally, we format just the data that users want to see.",Access to Inscope Resources Unencrypted GDPR - Demo,Access to Inscope Resources Unencrypted GDPR - Demo,,,,,,od_splunklive_fw_data.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(Sample Firewall Data)` 
| search app=workday* dest_port!=443 
| table _time user app bytes* src_ip dest_ip dest_port"
7,,,"First we bring in our basic demo dataset. In this case, Windows logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR.
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same hour.
Next we use the magic of stats+eval to count how many events there are where the action is success, or the action is failure
Finally we filter for where there is at least one success, and more than 100 failures.",Brute Force GDPR - Demo,Brute Force GDPR - Demo,,,,,,anon_interactive_logons.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row"" 
| xyseries row title blah ","| `Load_Sample_Log_Data(Windows Brute Force)` 
| lookup gdpr_system_category host as dest | search category=* 
| bucket _time span=1h 
| stats count(eval(action=""success"")) as successes count(eval(action=""failure"")) as failures by src _time 
| where successes>0 AND failures>100"
,,,"Here, tstats is pulling in one command a super-fast count per user, per system, per day of authentications.
It is usually easiest to work with data model acceleration after we've renamed the fields to something a little friendlier.",First Log On to Server - Accelerated,First Log On to Server - Accelerated,,user,dest,,,"count
count
count","1
1
1","Must have an accelerated Authentication data model
Authentication data model must have a field called dest defined
Authentication data model must have the user field defined",,"This search requires an accelerated authentication data model to run. If it is not present, consider ingesting Windows Security data via the Splunk Universal Forwarder, and then accelerating it with the Common Information App from <a href=""http://apps.splunk.com/"">apps.splunk.com</a>.
You should have a field called ""dest"" defined in your accelerated Authentication data model (referenced in tstats as Authentication.dest). If it is not present, consider ingesting Windows Security data via the Splunk Universal Forwarder, and then accelerating it with the Common Information App from <a href=""http://apps.splunk.com/"">apps.splunk.com</a>.
You should have a field called ""user"" defined in your accelerated Authentication data model (referenced in tstats as Authentication.user). If it is not present, consider ingesting Windows Security data via the Splunk Universal Forwarder, and then accelerating it with the Common Information App from <a href=""http://apps.splunk.com/"">apps.splunk.com</a>.","| tstats summariesonly=t allow_old_summaries=t count  from datamodel=Authentication where earliest=-2h 
| tstats summariesonly=t allow_old_summaries=t dc(Authentication.dest) as count  from datamodel=Authentication where earliest=-2h
| tstats summariesonly=t allow_old_summaries=t dc(Authentication.user) as count from datamodel=Authentication where earliest=-2h","| tstats summariesonly=t allow_old_summaries=t count earliest(_time) AS earliest latest(_time) AS latest from datamodel=Authentication  groupby _time span=1d, Authentication.user Authentication.dest
| rename ""Authentication.dest"" AS dest, ""Authentication.user"" as user"
,,,First we pull in our demo dataset.,Accessing New Git Repositories - Demo,Accessing New Git Repositories - Demo,,user,anonymized_git_repo,,,anonymized_git_history.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","|  `Load_Sample_Log_Data(""Source Code Access Logs"")`"
7,,,"First we pull in our dataset of Windows process launch logs filtered to our common Windows executables, care of EventID 4688 documented in this app. Any other EDR solution giving process launch logs will suffice here, as well. Notably, this technique of doing the value and then the field=value can bypass some quirks around field extractions, and make searches faster for very large datasets (though that's an area of active work, and it's less true every year).
Earlier versions of sysmon didn't extract a filename by default, so we are adding that in here.",New Paths for Common Executables - Live,New Paths for Common Executables - Live,,filename,path,,,"count
count","1
1","Must have Windows Security Logs
Must have Process Launch Logs (Event ID 4688)",,"Begin ingesting Windows Security Logs
Turn on Process Tracking in your Windows Audit logs (<a href=""https://technet.microsoft.com/en-us/library/cc976411.aspx"">docs</a>)","| metasearch earliest=-2h latest=now index=* sourcetype=""*WinEventLog:Security"" | stats count
earliest=-2h latest=now sourcetype=""*WinEventLog:Security"" 4688 EventCode=4688 index=* | head 100 | stats count ","index=* sourcetype=""*WinEventLog:Security"" EventCode=4688 (svchost.exe New_Process_Name=*svchost.exe) OR (iexplore.exe New_Process_Name=*iexplore.exe) OR (cmd.exe New_Process_Name=*cmd.exe) OR (firefox.exe New_Process_Name=*firefox.exe) OR (explorer.exe New_Process_Name=*explorer.exe) 
| rex field=New_Process_Name ""^(?<path>.*?)(?<filename>[^\\\/]*)$"" "
,,,First we pull in our demo dataset.,First Connection to Domain Controller - Demo,First Connection to Domain Controller - Demo,,user,anonymized_DomainControllerName,,,anonymized_DC_4776_logs.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","|  `Load_Sample_Log_Data(""Domain Controller Logins (Event ID 4776)"")`"
,,,"Here we start with our basic dataset of WinSecurity authentication logs. Notably, this technique of doing the value and then the field=value can bypass some quirks around field extractions, and make searches faster for very large datasets (though that's an area of active work, and it's less true every year).",First Log On to Server - Live,First Log On to Server - Live,,dest,user,,,"count
count
count","1
1
1","Must have Windows Security data
Must have Logon Success Data
Must have the user field defined",,"This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.
You should log logon events. There are many event IDs that we look for in the underlying logs, but they should all fall into the Audit Successful (or Failed) Logon events in your Windows Audit Policy. (<a href=""https://technet.microsoft.com/en-us/library/cc431373.aspx"">docs</a>)
You should have a field called ""user"" defined in your Windows Security logs. This is provided by the Splunk TA for Windows. Consider adding that TA to make for a better experience!","| metasearch earliest=-2h latest=now sourcetype=""*WinEventLog:Security"" index=* | head 100 | stats count 
sourcetype=""*WinEventLog:Security"" (4624 OR 4647 OR 4648 OR 551 OR 552 OR 540 OR 528 OR 4768 OR 4769 OR 4770 OR 4771 OR 4768 OR 4774 OR 4776 OR 4778 OR 4779 OR 672 OR 673 OR 674 OR 675 OR 678 OR 680 OR 682 OR 683) index=* | head 100 | stats count
sourcetype=""*WinEventLog:Security"" earliest=-2h index=* (4624 OR 4647 OR 4648 OR 551 OR 552 OR 540 OR 528 OR 4768 OR 4769 OR 4770 OR 4771 OR 4768 OR 4774 OR 4776 OR 4778 OR 4779 OR 672 OR 673 OR 674 OR 675 OR 678 OR 680 OR 682 OR 683) | head 100 | stats dc(user) as count","index=* sourcetype=""*WinEventLog:Security"" (4624 OR 4647 OR 4648 OR 551 OR 552 OR 540 OR 528 OR 4768 OR 4769 OR 4770 OR 4771 OR 4768 OR 4774 OR 4776 OR 4778 OR 4779 OR 672 OR 673 OR 674 OR 675 OR 678 OR 680 OR 682 OR 683) (EventCode=4624 OR EventCode=4647 OR EventCode=4648 OR EventCode=551 OR EventCode=552 OR EventCode=540 OR EventCode=528 OR EventCode=4768 OR EventCode=4769 OR EventCode=4770 OR EventCode=4771 OR EventCode=4768 OR EventCode=4774 OR EventCode=4776 OR EventCode=4778 OR EventCode=4779 OR EventCode=672 OR EventCode=673 OR EventCode=674 OR EventCode=675 OR EventCode=678 OR EventCode=680 OR EventCode=682 OR EventCode=683)"
7,,,"First we bring in our basic demo dataset. This dataset includes service status reported via WinHostMon (a part of the Universal Forwarder). We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
Then search for where the service doesn't start automatically",Update Service - Demo,Update Service - Demo,,start_mode,host,,,anon_interactive_logons.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(System Update Service Status)` 
| search start_mode!=Auto"
,,,"Here we start with our basic dataset of email logs, where we have a sender address who uses our company's email addresses, but where the IP address isn't a part of our environment.",New External IPs Sending Company Emails - Live,New External IPs Sending Company Emails - Live,,src_ip,sourcetype,,,count,1,Must have Incoming Address and Sender in Same Event,,"This one is slightly more complex for new Splunk users -- if your email logs show the src_ip (incoming address) in a different event than the sender email address, you will need to join these together via transaction or stats/eventstats. Check out the Cisco ESA version of this search for a working example of this unification, or <a href=""http://docs.splunk.com/Documentation/Splunk/6.5.2/SearchReference/Transaction#4._Email_transactions_based_on_maxevents_and_endswith"">check Splunk Docs</a>.",index=* sourcetype=cisco:esa* OR sourcetype=MSExchange*:MessageTracking OR tag=email | head 100 | eval length_combined = len(src_ip) * len(src_user) | stats sum(length_combined) as count,index=* sourcetype=cisco:esa* OR sourcetype=MSExchange*:MessageTracking OR tag=email src_user=* src_ip=* src_user=*@mycompany.com src_ip!=10.0.0.0/8
7,,,"First we pull in our demo dataset.
Then we filter for where the parent process is svchost.exe and the actual process is wsmprovhost.exe, as that is what will show up for a remote powershell launch.
For our demo data, we don't have a user, so we just default it as ""unknown."" ",Remote Powershell Launches - Demo,Remote Powershell Launches - Demo,,user,host,,,generic_sysmon_service_launch_logs.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","|  `Load_Sample_Log_Data(""Sysmon Process Launch Logs"")`
| search ParentImage=*\\svchost.exe Image=*\\wsmprovhost.exe 
| eval user=""Unknown"""
,,,First we pull in our demo dataset.,New RunAs - Demo,New RunAs - Demo,,Privileged_Account_Name,host,,,event_id_4648_runas.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","|`Load_Sample_Log_Data(""Windows Run As Logs (Event ID 4648)"")` "
,,,"Here, tstats is pulling in one command a super-fast count per system, per day of EventCode 20001 which shows up in some (but not all) systems for USB drive insertion.
It is usually easiest to work with data model acceleration after we've renamed the fields to something a little friendlier.",First USB Usage - Accelerated,First USB Usage - Accelerated,,source,dest,,,"count
count","1
1","Must have an accelerated Change Analysis data model
Change Analysis data model must have EventCode 20001 Events",,"This search requires Windows System data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.
In at least two test environments, EventCode 20001 was found when USB drives were inserted.","| tstats summariesonly=t allow_old_summaries=t count  from datamodel=Change_Analysis where earliest=-24h latest=now (nodename = All_Changes) 
| tstats summariesonly=t allow_old_summaries=t count  from datamodel=Change_Analysis where earliest=-24h latest=now (nodename = All_Changes) (All_Changes.result_id=20001) ","| tstats summariesonly=t allow_old_summaries=t count earliest(_time) AS earliest latest(_time) AS latest from datamodel=Change_Analysis where (nodename = All_Changes) (All_Changes.result_id=20001) groupby _time span=1d, source All_Changes.dest
| rename ""All_Changes.dest"" AS dest "
7,,,"First we bring in our basic demo dataset. In this case, anonymized AWS CloudTrail logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.",AWS New API Call Per User - Demo,AWS New API Call Per User - Demo,,user,eventName,,,aws-cloudtrail-data-anon.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ",| `Load_Sample_Log_Data(AWS CloudTrail)`
7,,,"First we bring in our basic dataset. In this case, AWS CloudTrail logs, filtered for individual APIs that we want to pay close attention to.",AWS New API Call Per User - Live,AWS New API Call Per User - Live,,user,eventName,,,count,1,Must have AWS CloudTrail data,,"In order to run this search, you must have AWS CloudTrail data onboard. Visit the <a href=""/app/Splunk_Security_Essentials/data_source?technology=AWS%20CloudTrail"">data onboarding guide for AWS CloudTrail in this app</a>, or browse to <a href=""https://splunkbase.splunk.com/app/1876/"">apps.splunk.com</a> for more information.",| tstats count where earliest=-2h latest=now index=* sourcetype=aws:cloudtrail,index=* sourcetype=aws:cloudtrail eventType=* NOT errorMessage=* NOT eventName=Describe* NOT eventName=Get* NOT eventName=List*
7,,,"First we bring in our basic demo dataset. In this case, anonymized AWS CloudTrail logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
Then we filter for Instance Creation.",AWS Instance Created by Unusual User - Demo,AWS Instance Created by Unusual User - Demo,,user,eventName,,,aws-cloudtrail-data-anon.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(AWS CloudTrail)` 
| search eventName=RunInstances"
7,,,"First we bring in our basic dataset. In this case, AWS CloudTrail logs, filtered for individual APIs that we want to pay close attention to.",AWS Instance Modified by Unusual User - Live,AWS Instance Modified by Unusual User - Live,,user,eventName,,,count,1,Must have AWS CloudTrail data,,"In order to run this search, you must have AWS CloudTrail data onboard. Visit the <a href=""/app/Splunk_Security_Essentials/data_source?technology=AWS%20CloudTrail"">data onboarding guide for AWS CloudTrail in this app</a>, or browse to <a href=""https://splunkbase.splunk.com/app/1876/"">apps.splunk.com</a> for more information.",| tstats count where earliest=-2h latest=now index=* sourcetype=aws:cloudtrail,index=* sourcetype=aws:cloudtrail eventName=ConsoleLogin OR eventName=CreateImage OR eventName=AssociateAddress OR eventName=AttachInternetGateway OR eventName=AttachVolume OR eventName=StartInstances OR eventName=StopInstances OR eventName=UpdateService OR eventName=UpdateLoginProfile
,,,First we pull in our demo dataset.,Accessing New Git Repositories With Peer - Demo,Accessing New Git Repositories With Peer - Demo,peer_group_for_git_use_case.csv,user,anonymized_git_repo,,,anonymized_git_history.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","|  `Load_Sample_Log_Data(""Source Code Access Logs"")`"
7,,,"First we pull in our dataset of Windows process launch logs filtered to our common Windows executables, care of EventID 4688 documented in this app. Any other EDR solution giving process launch logs will suffice here, as well. Notably, this technique of doing the value and then the field=value can bypass some quirks around field extractions, and make searches faster for very large datasets (though that's an area of active work, and it's less true every year).
Then we use table to include just the processes we're apt to care about. (Technically we need to use | table for this app because we show you the intermediate results, but in production you should drop this line because it will reduce search performance.)",Remote Powershell Launches - Live,Remote Powershell Launches - Live,,user,dest,,,"count
count","1
1","Must have Windows Security Logs
Must have Process Launch Logs (Event ID 4688)",,"Begin ingesting Windows Security Logs
Turn on Process Tracking in your Windows Audit logs (<a href=""https://technet.microsoft.com/en-us/library/cc976411.aspx"">docs</a>)","| metasearch index=* earliest=-2h latest=now sourcetype=""*WinEventLog:Security"" | stats count 
earliest=-2h latest=now index=* sourcetype=""*WinEventLog:Security"" EventCode=4688 | head 100 | stats count ","index=* sourcetype=""*WinEventLog:Security"" EventCode=4688 wsmprovhost svchost ParentImage=*\\svchost.exe Image=*\\wsmprovhost.exe 
| table _time user host EventCode New_Process_Name"
,,,"First we pull in our SFDC dataset and filter for what we're looking for in this use case, specifically export EVENT_TYPEs where the CLIENT_NAME is defined.
Then we enrich to convert the SFDC USER_ID into a friendly username via a lookup.",New Applications Contacting SFDC - Live,New Applications Contacting SFDC - Live,,CLIENT_NAME,USER_NAME,,,count,1,Must have Salesforce Data (assumes index=SFDC),,"This search requires data from the Salesforce Event Log File API. This is an additional fee from Salesforce, and can be effectively ingested and analyzed with the <a href=""https://splunkbase.splunk.com/app/1931"">Splunk App for Salesforce</a>.",| metasearch index=sfdc  earliest=-24h | head 100| stats count,"index=sfdc CLIENT_NAME=* EVENT_TYPE=API OR EVENT_TYPE=BulkAPI OR EVENT_TYPE=RestAPI 
| lookup SFDC_User_Lookup USER_ID"
7,,,"First we bring in our basic dataset. In this case, AWS CloudTrail logs, filtered for individual APIs that we want to pay close attention to.",AWS New API Call Per Peer Group - Live,AWS New API Call Peer Group - Live,peer_group_for_git_use_case.csv,user,eventName,,,count,1,Must have AWS CloudTrail data,,"In order to run this search, you must have AWS CloudTrail data onboard. Visit the <a href=""/app/Splunk_Security_Essentials/data_source?technology=AWS%20CloudTrail"">data onboarding guide for AWS CloudTrail in this app</a>, or browse to <a href=""https://splunkbase.splunk.com/app/1876/"">apps.splunk.com</a> for more information.",| tstats count where earliest=-2h latest=now index=* sourcetype=aws:cloudtrail,index=* sourcetype=aws:cloudtrail eventType=* NOT errorMessage=* NOT eventName=Describe* NOT eventName=Get* NOT eventName=List*
,,,"First we pull in our SFDC dataset and filter for EVENT_TYPEs that can have SOQL queries attached.
Then we extract out the table name from the query.
Finally we filter for queries of sensitive tables (Acount, Contact, or Opportunity), or their derivatives.
Then we enrich to convert the SFDC USER_ID into a friendly username via a lookup.",New SFDC Tables Queried by User - Live,New SFDC Tables Queried by User - Live,,USER_NAME,QUERY_TABLE,,,count,1,Must have Salesforce Data (assumes index=SFDC),,"This search requires data from the Salesforce Event Log File API. This is an additional fee from Salesforce, and can be effectively ingested and analyzed with the <a href=""https://splunkbase.splunk.com/app/1931"">Splunk App for Salesforce</a>.",| metasearch index=sfdc  earliest=-24h | head 100| stats count,"index=sfdc QUERY=* EVENT_TYPE=API OR EVENT_TYPE=BulkAPI OR EVENT_TYPE=RestAPI 
| rex field=QUERY max_match=12 ""\s(?i)from\s*(?<QUERY_TABLE>[\w_]*)"" 
| search QUERY_TABLE=Account* OR QUERY_TABLE=Contact* OR QUERY_TABLE=Opportunity* 
| lookup SFDC_User_Lookup USER_ID"
,,,"Here, tstats is pulling in one command a super-fast count per user, per repo, per day.
(self-explanatory)",Accessing New Git Repositories - Accelerated,Accessing New Git Repositories - Accelerated,,user,git_repo,,,"count
count
count","1
1
1","Must have an accelerated Git data model (not-default)
Must have a user field in accelerated Git data model
Must have a repo field in accelerated Git data model",,"This search accelerated Git data. There is no formal CIM data model for Source Code checkins or checkouts, so we are presuming a custom data model called Git.
The Git data model must have a user field defined.
The Git data model must have a repo field defined.","| tstats summariesonly=t allow_old_summaries=t count from datamodel=Git where earliest=-24h latest=now nodename=Git_View 
| tstats summariesonly=t allow_old_summaries=t dc(user) as count from datamodel=Git where earliest=-24h latest=now nodename=Git_View
| tstats summariesonly=t allow_old_summaries=t dc(repo) as count from datamodel=Git where earliest=-24h latest=now nodename=Git_View","| tstats summariesonly=t allow_old_summaries=t count from datamodel=Git where nodename=Git_View groupby user, repo, _time span=1d 
| eval comment=""<--- We don't have a standard data model that includes git repos, so you will need to build one to leverage data model acceleration""  "
,,,"First we start with our basic dataset of WinSecurity logs with EventCode 4776, which will only originate from a domain controller.
We then rename the ComputerName to DomainController name for clarity
Then we use table to include just the processes we're apt to care about. (Technically we need to use | table for this app because we show you the intermediate results, but in production you should drop this line because it will reduce search performance.)",First Connection to Domain Controller - Live,First Connection to Domain Controller - Live,,user,anonymized_DomainControllerName,,,"count
count
count","1
1
1","Must have Windows Security data
Must have Domain Controller Logon Events (EventCode=4776)
Must have the user field defined",,"This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.
Based on initial research, event ID 4776 (or in a Splunk search, EventCode=4776) gives us the right authentication logs from a domain controller. Make sure you have these logs (or tell us if any other event IDs are valid for this use case!). 
You should have a field called ""user"" defined in your Windows Security logs. This is provided by the Splunk TA for Windows. Consider adding that TA to make for a better experience!","| metasearch earliest=-2h latest=now sourcetype=""*WinEventLog:Security"" index=* | head 100 | stats count 
sourcetype=""*WinEventLog:Security"" 4776 EventCode=4776 index=* | head 100 | stats count
sourcetype=""*WinEventLog:Security"" earliest=-2h 4776 EventCode=4776 index=* | head 100 | stats dc(user) as count","sourcetype=""*WinEventLog:Security"" index=* 4776 EventCode=4776 
| rename ComputerName as DomainControllerName 
| table _time DomainControllerName user"
7,,,"First we bring in our basic demo dataset. In this case, sample Proxy logs from a Palo Alto Networks NGFW. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
Now we use URL Toolbox to parse out the domain from the the URL. Parsing out domains is actually wildly complicated (a regex will not suffice!), but URL Toolbox makes it easy. Check out more detail on Splunk Blogs (<a href=""https://www.splunk.com/blog/2017/09/21/ut-parsing-domains-like-house-slytherin.html"" target=""_blank"">link</a>).
Finally, we exclude IP addresses from our search using the regex filtering command. This is an optional step, but we've found that the value to noise ratio when including IP addresses can be quite high given that some applications will connect to many ephemeral AWS instance IPs for normal operations.",New Domain - Demo,New Domain - Demo,,ut_domain,sourcetype,,,bots-webproxy-data.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(Web Proxy Logs)` 
| eval list=""mozilla"" | `ut_parse_extended(uri, list)` 
| regex ut_domain!=""^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$"""
,,,"First we pull in our basic dataset, which consists of XML format Sysmon logs from the endpoints (ingested via the sysmon TA) with process launches of cmd.exe or regedit.exe. This could be any EDR data source that provides file hash information. Because we're looking for process launches, we then filter for EventCode=1 (the sysmon Process Launch code).
Then we use table to include just the processes we're apt to care about. (Technically we need to use | table for this app because we show you the intermediate results, but in production you should drop this line because it will reduce search performance.)",New Parent Process for cmd.exe or regedit.exe - Live,New Parent Process for cmd.exe or regedit.exe - Live,,Image,ParentImage,,,"count
count","1
1","Must have Sysmon data
Must have Sysmon Process Launch logs (EventCode=1)",,"This search requires Microsoft Sysmon data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder. If you're not using sysmon at all, it's free and highly recommended! <a href=""https://technet.microsoft.com/en-us/sysinternals/sysmon"">Here is the direct link</a>, and <a href=""http://conf.splunk.com/files/2016/slides/splunking-the-endpoint-hands-on.pdf"">here is a .conf presentation on the topic</a>.
These are the logs that will give us process launch data. Some customers will get these logs from other sources, but ensure you have a parent process name as well.","| tstats count where index=* earliest=-2h latest=now sourcetype=*sysmon* 
sourcetype=*sysmon* index=* EventCode=1  | head 100 | stats count","index=* sourcetype=*sysmon* EventCode=1 (cmd.exe Image=*\cmd.exe) OR (regedit.exe Image=*\regedit.exe)  
|table  _time host Image ProcessId ParentImage ParentProcessId sha1"
,,,"First we pull in our demo SFDC dataset.
Then we filter for what we're looking for in this use case, specifically queries of sensitive tables (Account, Contact, or Opportunity), or their derivatives.
Then we enrich to convert the SFDC USER_ID into a friendly username via a lookup.",New SFDC Tables Queried by Peer Group - Demo,New SFDC Tables Queried by Peer Group - Demo,SFDC_User_Lookup.csv,USER_NAME,QUERY_TABLE,,,SFDC_Sample_Data_Anon.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| `Load_Sample_Log_Data(""SFDC Data"")` 
| search QUERY_TABLE=Account* OR QUERY_TABLE=Contact* OR QUERY_TABLE=Opportunity* 
| lookup SFDC_User_Lookup USER_ID"
,,,"First we pull in our SFDC dataset and filter for what we're looking for in this use case, specifically high risk EVENT_TYPEs
Then we enrich to convert the SFDC USER_ID into a friendly username via a lookup.",New Risky Event Types Per User - Live,New Risky Event Types Per User - Live,,USER_NAME,EVENT_TYPE,,,count,1,Must have Salesforce Data (assumes index=SFDC),,"This search requires data from the Salesforce Event Log File API. This is an additional fee from Salesforce, and can be effectively ingested and analyzed with the <a href=""https://splunkbase.splunk.com/app/1931"">Splunk App for Salesforce</a>.",| metasearch index=sfdc  earliest=-24h | head 100| stats count,"index=sfdc EVENT_TYPE=API OR EVENT_TYPE=BulkAPI OR EVENT_TYPE=RestAPI OR EVENT_TYPE=ReportExport 
| lookup SFDC_User_Lookup USER_ID"
7,,,"First we pull in our dataset of Windows process launch logs, care of EventID 4688 documented in this app. Any other EDR solution giving process launch logs will suffice here, as well.
Earlier versions of sysmon didn't extract a filename by default, so we are adding that in here.
This line uses eventstats (which works just like stats except it adds all the additional fields to whatever your incoming dataset was) to let us know how many days of baseline we have for a host. This is important, because it allows us to filter out hosts without much of a history.
This line also uses eventstats to pull out, per host, how many paths a particular filename was executed with.
Finally we look for filenames that were launched with multiple paths.",New Path for Process On Host - Live,New Path for Process On Host - Live,,,,,,"count
count","1
1","Must have Windows Security Logs
Must have Process Launch Logs (Event ID 4688)",,"Begin ingesting Windows Security Logs
Turn on Process Tracking in your Windows Audit logs (<a href=""https://technet.microsoft.com/en-us/library/cc976411.aspx"">docs</a>)","| metasearch earliest=-2h latest=now index=* sourcetype=""*WinEventLog:Security"" | stats count
earliest=-2h latest=now sourcetype=""*WinEventLog:Security"" 4688 EventCode=4688 index=* | head 100 | stats count ","index=* sourcetype=""*WinEventLog:Security"" EventCode=4688 
| rex field=Image ""(?<filename>[^\\\/]*)$"" 
| eval day=strftime(_time, ""%d/%m/%Y"")  | eventstats dc(day) as days_of_baseline by host | where days_of_baseline>7 | fields - day 
| eventstats dc(Image) as NumPaths values(Image) as Paths by filename, host 
| where NumPaths>1"
,,,This string will look in your Windows Security logs for the specific signature of Mimikatz (prior to 2017).,New Domain Name Detected - Live,New Domain Name Detected - Live,,Account_Domain,EventCode,,,"count
count","1
1","Must have Windows Security data
Must have Event Codes for Potentially Suspect Windows Logon Events (EventCode=4624)",,"This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.
Based on research, event ID 4624 (or in a Splunk search, EventCode=4624) gives us the right authentication logs. Make sure you have these logs. Here's a <a href=""https://www.microsoft.com/en-us/download/details.aspx?id=36036"">really detailed whitepaper from Microsoft</a>. Notably, this approach doesn't work with modern Mimikatz, but it's still valuable for anomaly detection.","| metasearch earliest=-2h latest=now sourcetype=""*WinEventLog:Security"" index=* | head 100 | stats count 
sourcetype=""*WinEventLog:Security"" 4624 EventCode=4624 index=* | head 100 | stats count","index=* source=WinEventLog:Security EventCode=4624 Authentication_Package=NTLM Type=Information Account_Name!=""ANONYMOUS LOGON"" Logon_Type=3 Logon_Process=""NtLmSsP"" NOT Security_ID!=""NULL SID"" Key_Length=0"
7,,,"First we bring in our basic demo dataset. This dataset includes service status reported via WinHostMon (a part of the Universal Forwarder). We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
Then search for where the service doesn't start automatically
Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR.",Update Service GDPR - Demo,Update Service GDPR - Demo,,start_mode,host,,,anon_interactive_logons.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(System Update Service Status)` 
| search start_mode!=Auto 
| lookup gdpr_system_category.csv host | search category=*"
7,,,"First we pull in our demo dataset.
Then we filter for the individual launches of common windows filenames.
Earlier versions of sysmon didn't extract a filename by default, so we are adding that in here.",New Paths for Common Executables - Demo,New Paths for Common Executables - Demo,,filename,path,,,generic_sysmon_service_launch_logs.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| `Load_Sample_Log_Data(""Generic Sysmon Process Launches"")`  
| search Image=*svchost.exe OR Image=*iexplore.exe OR Image=*cmd.exe OR Image=*firefox.exe OR Image=*explorer.exe 
| rex field=Image ""^(?<path>.*?)(?<filename>[^\\\/]*)$"" "
,,,"First we pull in our dataset of Windows process launch logs filtered to where services.exe is the Parent Process, care of EventID 4688 documented in this app. Any other EDR solution giving process launch logs will suffice here, as well.
Then we use table to include just the processes we're apt to care about. (Technically we need to use | table for this app because we show you the intermediate results, but in production you should drop this line because it will reduce search performance.)",New Service Creation - Live,New Service Creation - Live,,Image,host,,,"count
count","1
1","Must have Windows Security Logs
Must have Process Launch Logs (Event ID 4688)",,"Begin ingesting Windows Security Logs
Turn on Process Tracking in your Windows Audit logs (<a href=""https://technet.microsoft.com/en-us/library/cc976411.aspx"">docs</a>)","| metasearch index=* earliest=-2h latest=now sourcetype=""*WinEventLog:Security"" | stats count 
earliest=-2h latest=now index=* sourcetype=""*WinEventLog:Security"" EventCode=4688 | head 100 | stats count ","index=* sourcetype=""*WinEventLog:Security"" EventCode=4688 services.exe | search ParentImage=*\\services.exe 
|table _time user host EventCode New_Process_Name ParentImage"
7,,,"First we bring in our basic dataset of service status reported via WinHostMon (a part of the Universal Forwarder). We also search for where the service doesn't start automatically
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Stats summarizes the status of across the entire environment for better performance.",Update Service - Live,Update Service - Live,,StartMode,host,,,"count
count","1
1","Must have service status data
Must have Windows Update status data",,"In order to run this search, you must have data about service status. Visit the <a href=""/app/Splunk_Security_Essentials/data_source?technology=Windows%20Security"">data onboarding guide for Windows Security in this app</a>, or browse to <a href=""https://docs.splunk.com/Documentation/Splunk/7.0.2/Data/MonitorWindowshostinformation"">Splunk Docs</a> for more information.
Inside of your service status data, you must have Windows Update listed (wuauserv.exe)","| tstats count where earliest=-2h latest=now index=* (sourcetype=wmi:service OR (sourcetype=winhostmon Service))
| tstats count where earliest=-2h latest=now index=* (sourcetype=wmi:service OR (sourcetype=winhostmon Service)) wuauserv","index=* (sourcetype=wmi:service OR (sourcetype=winhostmon Type=Service) OR (tag=service tag=os))  (tag=update OR Name=wuauserv) StartMode!=Auto 
| bucket _time span=1d 
| stats latest(Status) as Status latest(StartMode) as StartMode by _time host Name"
7,,,"First we bring in our basic demo dataset. In this case, AWS CloudTrail logs.",Unusual AWS Regions - Live,Unusual AWS Regions - Live,,awsRegion,sourcetype,,,count,1,Must have AWS CloudTrail data,,"In order to run this search, you must have AWS CloudTrail data onboard. Visit the <a href=""/app/Splunk_Security_Essentials/data_source?technology=AWS%20CloudTrail"">data onboarding guide for AWS CloudTrail in this app</a>, or browse to <a href=""https://splunkbase.splunk.com/app/1876/"">apps.splunk.com</a> for more information.",| tstats count where earliest=-2h latest=now index=* sourcetype=aws:cloudtrail,index=* sourcetype=aws:cloudtrail
,,,First we pull in our demo dataset.,First Log On to Server - Demo,First Log On to Server - Demo,,user,anonymized_ComputerName,,,Sampled_AnonymizedLogonActivity.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","|  `Load_Sample_Log_Data(""Windows Logon Activity"")`"
7,,,"First we pull in our dataset of Windows process launch logs, care of EventID 4688 documented in this app. Any other EDR solution giving process launch logs will suffice here, as well.
From line one we have our process launch logs, now we need to filter that down to just the potential attack tools. We do this via a subsearch. A subsearch goes and runs another search, and then takes those results and inserts them into the main search. You can copy-paste that subsearch into a new search window and see what the results look like -- there's a single field called ""search"" that has a bunch of filenames with ORs between them. That will effectively be inserted into our main search, giving us a really long search string without having to maintain a really long search.",Suspicious Command Launch - Live,Suspicious Command Launch - Live,,,,,,"count
count","1
1","Must have Windows Security Logs
Must have Process Launch Logs (Event ID 4688)",,"Begin ingesting Windows Security Logs
Turn on Process Tracking in your Windows Audit logs (<a href=""https://technet.microsoft.com/en-us/library/cc976411.aspx"">docs</a>)","| metasearch earliest=-2h latest=now index=* sourcetype=""*WinEventLog:Security"" | stats count
earliest=-2h latest=now sourcetype=""*WinEventLog:Security"" 4688 EventCode=4688 index=* | head 100 | stats count ","index=* sourcetype=""*WinEventLog:Security"" EventCode=4688 
[|inputlookup tools.csv | search discovery_or_attack=suspicious | stats values(filename) as search | eval search=mvjoin(search, "" OR "")] "
,,,"This line will load a sample CSV. The macro is a wrapper for |inputlookup to make this search look prettier here.
Now we filter for where the user account starts with svc_, which is a common way to notate service accounts.",New Interactive Logon from a Service Account - Demo,New Interactive Logon from a Service Account - Demo,,user,dest,,,anon_interactive_logons.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| `Load_Sample_Log_Data(""Interactive Logins"")` 
 | search user=svc_* "
,,,"First we pull in our demo SFDC dataset.
Then we filter for what we're looking for in this use case, specifically high risk EVENT_TYPEs
Then we enrich to convert the SFDC USER_ID into a friendly username via a lookup.",New Risky Event Types Per User - Demo,New Risky Event Types Per User - Demo,,USER_NAME,EVENT_TYPE,,,SFDC_Sample_Data_Anon.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| `Load_Sample_Log_Data(""SFDC Data"")` 
| search EVENT_TYPE=API OR EVENT_TYPE=BulkAPI OR EVENT_TYPE=RestAPI OR EVENT_TYPE=ReportExport 
| lookup SFDC_User_Lookup USER_ID"
,,,"First we pull in our dataset of Windows Authentication where there is a Logon_Type defined.
Then we use table to include just the logons we're apt to care about. (Technically we need to use | table for this app because we show you the intermediate results, but in production you should drop this line because it will reduce search performance.)",New Logon Type for User - Live,New Logon Type for User - Live,,user,Logon_Type,,,count,1,Must have Windows Security data,,"This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.","| metasearch earliest=-2h latest=now index=* sourcetype=""*WinEventLog:Security"" | head | stats count ","index=* sourcetype=""*WinEventLog:Security"" Logon_Type=* Logon Type TaskCategory=Logon Audit Success 
| table _time user Logon_Type  "
7,,,"First we bring in our basic demo dataset. In this case, AWS CloudTrail logs filtered for Instance Creation.",AWS Instance Created by Unusual User - Live,AWS Instance Created by Unusual User - Live,,user,eventName,,,count,1,Must have AWS CloudTrail data,,"In order to run this search, you must have AWS CloudTrail data onboard. Visit the <a href=""/app/Splunk_Security_Essentials/data_source?technology=AWS%20CloudTrail"">data onboarding guide for AWS CloudTrail in this app</a>, or browse to <a href=""https://splunkbase.splunk.com/app/1876/"">apps.splunk.com</a> for more information.",| tstats count where earliest=-2h latest=now index=* sourcetype=aws:cloudtrail,index=* sourcetype=aws:cloudtrail eventType=* NOT errorMessage=* eventName=RunInstances 
7,,,"First we bring in our basic demo dataset. In this case, anonymized AWS CloudTrail logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
Then we filter for provisioning activities (somewhat broadly)
Next we GeoIP to get the country.",AWS Cloud Provisioning Activity from Unusual Country - Demo,AWS Cloud Provisioning Activity from Unusual Country - Demo,,Country,sourcetype,,,aws-cloudtrail-data-anon.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(AWS CloudTrail)` 
| search eventName=Create* OR eventName=Run* OR eventName=Attach* 
| iplocation src_ip"
,,,First we pull in our demo dataset.,New Logon Type for User - Demo,New Logon Type for User - Demo,,user,Logon_Type_Description,,,anon_interactive_logons.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","|`Load_Sample_Log_Data(""Interactive Logins"")`"
,,,"Here we start with our basic dataset of Cisco ESA (aka Ironport) logs, where we have either an MID (Message ID) or ICID (Incoming Connection ID).
Ironport logs will have several messages with just ICID, and many with just MID. We want to ultimately analyze MIDs, but we need to tag those ICID fields that don't have any MID with the MIDs that came on that connection. Eventstats will let us do that, without otherwise changing the dataset. If two messages (MIDs 159950 and 159951) came across one incoming connect (ICID 102501), after this the events would look the same except all logs with ICID 102501 would now have a multi-value field called MID with the values 159950 and 159951.
Now we can aggregate the source IPs and sender addresses of every incoming message.
Finally we can filter in on those that have a sender domain that belongs to our company, but didn't originate from our environment.",New External IPs Sending Company Emails - Cisco ESA Live,New External IPs Sending Company Emails - Cisco ESA Live,,src_ip,sourcetype,,,"count
count","1
1","Must have Cisco ESA / Ironport Data
Must have Cisco ESA field extractions",,"This search requires Email data. The out of the box field extractions support the Common Information Model, including Cisco ESA/Ironport and Microsoft Exchange. If you don't have this data today, we highly recommend ingesting it with the <a href=""https://splunkbase.splunk.com/app/1761/"">Cisco ESA TA</a> or the <a href=""https://splunkbase.splunk.com/app/3225/"">Splunk Add-on for Microsoft Exchange</a>. For best performance, accelerate the email data model from the <a href=""https://splunkbase.splunk.com/app/1621/"">Common Information Model</a>!
We highly recommend the <a href=""https://splunkbase.splunk.com/app/1761/"">Cisco ESA TA</a> to handle your field extractions.","| tstats count where index=* sourcetype=cisco:esa* OR sourcetype=*ironport* MID OR ICID earliest=-4h
index=* sourcetype=cisco:esa* OR sourcetype=*ironport* MID OR ICID earliest=-4h | head 1000 | stats dc(ICID) as ICID dc(MID) as MID dc(src_ip) as src_ip dc(src_user) as src_user | eval count = ICID * MID * src_ip * src_user","index=* sourcetype=cisco:esa* OR sourcetype=*ironport* MID OR ICID 
| eventstats values(MID) as MID by ICID 
| stats values(src_ip) as src_ip values(src_user) as src_user by MID 
| search src_user=*@mycompany.com src_ip!=10.0.0.0/8"
,,,First we pull in our demo dataset.,New Domain Name Detected - Demo,New Domain Name Detected - Demo,,Account_Domain,EventCode,,,Example_Legacy_Pass_The_Hash.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","|  `Load_Sample_Log_Data(""Example Pass The Hash (Legacy)"")`"
,,,"Here we pull in our dataset of Microsoft Sysmon logs (though an EDR logs will suffice), and filter to when the process is one of the standard windows processes, and the parent process is services.exe",New cmd.exe or regedit or powershell launched by services.exe - Live,New cmd.exe or regedit or powershell launched by services.exe - Live,,Image,host,,,"count
count","1
1","Must have Sysmon data
Must have Sysmon Process Launch logs (EventCode=1)",,"This search requires Microsoft Sysmon data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder. If you're not using sysmon at all, it's free and highly recommended! <a href=""https://technet.microsoft.com/en-us/sysinternals/sysmon"">Here is the direct link</a>, and <a href=""http://conf.splunk.com/files/2016/slides/splunking-the-endpoint-hands-on.pdf"">here is a .conf presentation on the topic</a>.
These are the logs that will give us process launch data. Some customers will get these logs from other sources, but ensure you have a parent process name as well.","| tstats count where index=* earliest=-2h latest=now sourcetype=*sysmon*  
sourcetype=*sysmon* index=* EventCode=1  | head 100 | stats count",index=* sourcetype=*sysmon* EventCode=1 ( (cmd.exe Image=*\\cmd.exe) OR (regedit.exe Image=*\\regedit.exe) OR (powershell.exe Image=*\\powershell.exe) ) (services.exe Image=*\\system32\\services.exe)
,,,"First we pull in our demo SFDC dataset.
Then we filter for what we're looking for in this use case, specifically queries of sensitive tables (Acount, Contact, or Opportunity), or their derivatives.
Then we enrich to convert the SFDC USER_ID into a friendly username via a lookup.",New SFDC Tables Queried by User - Demo,New SFDC Tables Queried by User - Demo,,USER_NAME,QUERY_TABLE,,,SFDC_Sample_Data_Anon.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| `Load_Sample_Log_Data(""SFDC Data"")` 
| search QUERY_TABLE=Account* OR QUERY_TABLE=Contact* OR QUERY_TABLE=Opportunity* 
| lookup SFDC_User_Lookup USER_ID"
,,,"First we pull in our demo dataset.
Then we filter for where the sender is *@mycompany.com, but the incoming_address is not in our internal environment (update this with your environment's details).",New External IPs Sending Company Emails - Demo,New External IPs Sending Company Emails - Demo,,incoming_address,sourcetype,,,Anonymized_Email_Logs.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| `Load_Sample_Log_Data(""Email Logs"")`
| search Sender=*@mycompany.com incoming_address!=10.0.0.0/8  "
,,,"Here we start with our basic dataset of Windows System Logs, filtered for EventCode 20001 which shows up in some (but not all) systems for USB drive insertion. Notably, this technique of doing the value and then the field=value can bypass some quirks around field extractions, and make searches faster for very large datasets (though that's an area of active work, and it's less true every year).",First USB Usage - Live,First USB Usage - Live,,host,source,,,"count
count","1
1","Must have Windows System Events
Must have EventCode 20001 Events",200,"This search requires Windows System data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.
In at least two test environments, EventCode 20001 was found when USB drives were inserted.","| metasearch earliest=-2h latest=now index=* sourcetype=WinEventLog:System | head 100 | stats count 
index=* earliest=-30d sourcetype=WinEventLog:System TERM(20001) EventCode=20001  | head | stats count",index=* sourcetype=WinEventLog:System 20001 EventCode=20001
,,,"First we pull in our SFDC dataset and filter for EVENT_TYPEs that can have SOQL queries attached.
Then we extract out the table name from the query.
Finally we filter for queries of sensitive tables (Account, Contact, or Opportunity), or their derivatives.
Then we enrich to convert the SFDC USER_ID into a friendly username via a lookup.",New SFDC Tables Queried by Peer Group - Live,New SFDC Tables Queried by Peer Group - Live,SFDC_User_Lookup.csv,USER_NAME,QUERY_TABLE,,,count,1,Must have Salesforce Data (assumes index=SFDC),,"This search requires data from the Salesforce Event Log File API. This is an additional fee from Salesforce, and can be effectively ingested and analyzed with the <a href=""https://splunkbase.splunk.com/app/1931"">Splunk App for Salesforce</a>.",| metasearch index=sfdc  earliest=-24h | head 100| stats count,"index=sfdc QUERY=* EVENT_TYPE=API OR EVENT_TYPE=BulkAPI OR EVENT_TYPE=RestAPI 
| rex field=QUERY max_match=12 ""\s(?i)from\s*(?<QUERY_TABLE>[\w_]*)"" 
| search QUERY_TABLE=Account* OR QUERY_TABLE=Contact* OR QUERY_TABLE=Opportunity* 
| lookup SFDC_User_Lookup USER_ID"
,,,"First we pull in our dataset of Windows Authentication specifying Interactive logon types, and filter for where the user account starts with svc_, which is a common way to notate service accounts.
Then we use table to include just the processes we're apt to care about. (Technically we need to use | table for this app because we show you the intermediate results, but in production you should drop this line because it will reduce search performance.)",New Interactive Logon from a Service Account - Live,New Interactive Logon from a Service Account - Live,,user,Logon_Type,,,"count
count","1
1","Must have Email Data
Must have Windows Security data",,"This search requires Email data. The out of the box field extractions support the Common Information Model, including Cisco ESA/Ironport and Microsoft Exchange. If you don't have this data today, we highly recommend ingesting it with the <a href=""https://splunkbase.splunk.com/app/1761/"">Cisco ESA TA</a> or the <a href=""https://splunkbase.splunk.com/app/3225/"">Splunk Add-on for Microsoft Exchange</a>. For best performance, accelerate the email data model from the <a href=""https://splunkbase.splunk.com/app/1621/"">Common Information Model</a>!
This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.","| tstats count where index=* sourcetype=cisco:esa* earliest=-4h
| metasearch earliest=-2h latest=now index=* sourcetype=""*WinEventLog:Security"" | head | stats count ","index=* sourcetype=""*WinEventLog:Security"" Logon_Type=2 OR Logon_Type=10 OR Logon_Type=11 Logon Type TaskCategory=Logon Audit Success svc user=svc_* 
| table _time user dest "
,,,"First, we start with our Atlassian BitBucket access logs (Atlassian is a commercial open-source version of git).
Next we extract the field names we will use. These are the regular expressions that have worked in a couple of environments, but you should verify them in yours.
Finally, we filter for just the logs that include a git_repo field.",Accessing New Git Repositories - Live,Accessing New Git Repositories - Live,,user,git_repo,,,"count
count","1
1","Must have BitBucket / Git data
Must have a user defined in your data",,"In tests so far, Atlassian BitBucket git logs are stored in a file called atlassian-bitbucket-access.log. We're looking for that here.
You should have a field called ""user"" defined in your bitbucket logs. If that's not currently extracted, build an extraction for it (or do an inline rex in the SPL below to work around this).","| metasearch earliest=-24h latest=now index=* source=""*/atlassian-bitbucket-access.log"" | head 100 | stats count 
earliest=-2h latest=now index=* source=""*/atlassian-bitbucket-access.log"" | head 100 | stats dc(user) as count ","index=* source=""*/atlassian-bitbucket-access.log"" 
| rex ""GET /projects/[^/]*/repos/(?<git_repo>[^/]*)"" | rex ""(?<git_repo>[^/]*)\.git"" | rex ""git\.[^ /]{1,}/projects/[^/]*/repos/(?<git_repo>[^/]*)"" | eval comment = ""<-- verify these regex in your env""  
| search  git_repo=""*"""
,,,First we pull in our demo dataset.,First USB Usage - Demo,First USB Usage - Demo,,anonymized_ComputerName,source,,,windows_system_event_id_20001_usb_inserts.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","|  `Load_Sample_Log_Data(""USB Inserts from Windows System Event ID 20001"")`"
,,,"First we pull in our demo dataset.
Earlier versions of sysmon didn't extract a filename by default, so we are adding that in here.
This line uses eventstats (which works just like stats except it adds all the additional fields to whatever your incoming dataset was) to let us know how many days of baseline we have for a host. This is important, because it allows us to filter out hosts without much of a history.
This line also uses eventstats to pull out, per host, how many paths a particular filename was executed with.
Finally we look for files that were launched with multiple paths, and then filter out some known false positives found in our demo dataset (quirk of how it was built out -- shouldn't occur in production).",New Path for Process On Host - Demo,New Path for Process On Host - Demo,,Image,filename,,,generic_sysmon_service_launch_logs.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| `Load_Sample_Log_Data(""Generic Sysmon Process Launches"")`  
| rex field=Image ""(?<filename>[^\\\/]*)$"" 
| eval day=strftime(_time, ""%d/%m/%Y"")  | eventstats dc(day) as days_of_baseline by host | where days_of_baseline>7 | fields - day 
| eventstats dc(Image) as NumPaths values(Image) as Paths by filename, host 
| where NumPaths>1  AND NOT (mvcount(Paths) = 2 AND like(mvindex(Paths, 0), ""%SysWOW64%"") AND like(mvindex(Paths, 1), ""%System32%"")) AND NOT (mvcount(Paths) = 2 AND like(mvindex(Paths, 0), ""%SysWOW64\explorer.exe"") AND like(mvindex(Paths, 1), ""%Windows\explorer.exe""))"
,,,"Here, tstats is pulling in one command a super-fast count per sender, per day of emails where the sender domain is our organization, but the email did not originate from our organization",New External IPs Sending Company Emails - Accelerated,New External IPs Sending Company Emails - Accelerated,,sourcetype,All_Email.src_ip,,,"count
count
count
count","1
1
1
1","Must have an Email data model
Must have an accelerated Email data model
Must have Sender Email Addresses (src_user) in your accelerated Email data model
Must have Source IP Addresses (src_user) in your accelerated Email data model",,"This search requires an Email data. This is dependent on the <a href=""https://splunkbase.splunk.com/app/1621/"">Common Information Model</a> being present, and also having your data mapped to CIM via appropriate TAs, usually with the out of the box field extractions from the <a href=""https://splunkbase.splunk.com/app/1761/"">Cisco ESA TA</a>, the <a href=""https://splunkbase.splunk.com/app/3225/"">Splunk Add-on for Microsoft Exchange</a>, etc.
This search requires an accelerated Email data. In order to run a fast accelerated search, you should accelerate your data model. (<a href=""https://docs.splunk.com/Documentation/Splunk/latest/HadoopAnalytics/Configuredatamodelacceleration#Accelerate_the_data_model"">docs</a>)
This search assumes that you have actual source email addresses -- check your field extractions for src_user and then rebuild your data models if not.
This search assumes that you have actual source email addresses -- check your field extractions for src_user and then rebuild your data models if not.","| tstats summaries_only=f allow_old_summaries=t count from datamodel=Email where earliest=-1h
| tstats summaries_only=t allow_old_summaries=t count from datamodel=Email where earliest=-1h
| tstats summaries_only=t allow_old_summaries=t dc(All_Email.src_user) as count from datamodel=Email where earliest=-1h
| tstats summaries_only=t allow_old_summaries=t dc(All_Email.src_user) as count from datamodel=Email where earliest=-1h",| tstats summaries_only=t allow_old_summaries=t count from datamodel=Email where All_Email.src_user=*@mycompany.com All_Email.src_ip!=10.* by All_Email.src_user  _time span=1d
,,,First we pull in our demo dataset.,New Service Creation - Demo,New Service Creation - Demo,,Image,host,,,generic_sysmon_service_launch_logs.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","|`Load_Sample_Log_Data(""Generic Sysmon Service Launches"")`"
7,,,"First we bring in our proxy dataset, leveraging Common Information Model fields, filtering for just events that actually have a URI.
Now we use URL Toolbox to parse out the domain from the the URL. Parsing out domains is actually wildly complicated (a regex will not suffice!), but URL Toolbox makes it easy. Check out more detail on Splunk Blogs (<a href=""https://www.splunk.com/blog/2017/09/21/ut-parsing-domains-like-house-slytherin.html"" target=""_blank"">link</a>).
Finally, we exclude IP addresses from our search using the regex filtering command. This is an optional step, but we've found that the value to noise ratio when including IP addresses can be quite high given that some applications will connect to many ephemeral AWS instance IPs for normal operations.",New Domain - Live,New Domain - Live,,ut_domain,sourcetype,,,count,1,Must have Proxy data,,"Proxy data can come in many forms, including from Palo Alto Networks and other NGFWs, dedicated proxies like BlueCoat, or network monitoring tools like Splunk Stream or bro.",| metasearch earliest=-2h latest=now index=* (tag=web tag=proxy) | head 100 | stats count ,"tag=web url=* 
| eval list=""mozilla"" | `ut_parse_extended(url,list)`
| regex ut_domain!=""^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$"""
,,,"Here we start with our basic dataset of WinSecurity logs with Event ID 4648 (signifying ""Run As"" events).
Next we filter out the Windows System usernames, where this can occur frequently
Windows Security logs often include two usernames -- the acting username, and the target username. We want the latter (note that this hasn't been proven to work uniformly across all log sources, but it seems to work well for this scenario).
Finally we put it all in a table.",New RunAs - Live,New RunAs - Live,,Privileged_Account_Name,host,,,"count
count","1
1","Must have Windows Security data
Must have Privileged Escalation Events (EventCode=4648)",,"This search requires Windows Security data to run. If it is not present, consider ingesting it via the Splunk Universal Forwarder.
Windows Security Event ID 4648 tracks the explicit use of credentials, as in a runas event or batch login from a scheduled task. You can enable this from your Windows Logon Event policy configuration.","| metasearch earliest=-2h latest=now index=* sourcetype=""*WinEventLog:Security"" | head | stats count 
| metasearch earliest=-30d sourcetype=""*WinEventLog:Security"" index=* TERM(eventcode=4648)  | head | stats count","index=* sourcetype=Wineventlog:security EventCode=4648 
| search NOT Account_Name=*$ 
| eval Privileged_Account_Name=mvindex(Account_Name,1) 
| table _time  host Privileged_Account_Name"
,,,"First we pull in our demo dataset.
Then we filter for process launches of cmd.exe, regedit.exe, or powershell.exe",New Parent Process for cmd.exe or regedit.exe - Demo,New Parent Process for cmd.exe or regedit.exe - Demo,,Image,ParentImage,,,generic_sysmon_service_launch_logs.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| `Load_Sample_Log_Data(""Generic Sysmon Process Launches"")` 
| search Image=*\\cmd.exe OR Image=*\\regedit.exe OR Image=*\\powershell.exe "
7,,,"First we bring in our basic demo dataset. In this case, anonymized AWS CloudTrail logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
Then we filter for individual APIs that we want to pay close attention to.",AWS Instance Modified by Unusual User - Demo,AWS Instance Modified by Unusual User - Demo,,user,eventName,,,aws-cloudtrail-data-anon.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(AWS CloudTrail)` 
|search  eventName=ConsoleLogin OR eventName=CreateImage OR eventName=AssociateAddress OR eventName=AttachInternetGateway OR eventName=AttachVolume OR eventName=StartInstances OR eventName=StopInstances OR eventName=UpdateService OR eventName=UpdateLoginProfile"
,,,"First we pull in our demo dataset.
From line one we have our process launch logs, now we need to filter that down to just the potential attack tools. We do this via a subsearch. A subsearch goes and runs another search, and then takes those results and inserts them into the main search. You can copy-paste that subsearch into a new search window and see what the results look like -- there's a single field called ""search"" that has a bunch of filenames with ORs between them. That will effectively be inserted into our main search, giving us a really long search string without having to maintain a really long search.
For our demo data, we don't have a user, so we just default it as ""unknown.""",Suspicious Command Launch - Demo,Suspicious Command Launch - Demo,,Image,user,,,generic_sysmon_service_launch_logs.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| `Load_Sample_Log_Data(""Generic Sysmon Process Launches"")`  
| search [| inputlookup tools.csv | search discovery_or_attack=suspicious | eval filename=""Image=\""*\\\\"" . filename . ""\"""" | stats values(filename) as search | eval search=mvjoin(search, "" OR "")] 
| eval user=""Unknown"""
7,,,"First we bring in our basic demo dataset. In this case, anonymized AWS CloudTrail logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.
Then we filter for provisioning activities (somewhat broadly).",AWS Cloud Provisioning Activity from Unusual IP - Demo,AWS Cloud Provisioning Activity from Unusual IP - Demo,,src_ip,sourcetype,,,aws-cloudtrail-data-anon.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ","| `Load_Sample_Log_Data(AWS CloudTrail)` 
| search eventName=Create* OR eventName=Run* OR eventName=Attach*"
7,,,"First we bring in our basic demo dataset. In this case, anonymized AWS CloudTrail logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.",AWS New API Call Per Peer Group - Demo,AWS New API Call Per Peer Group - Demo,peer_group_for_git_use_case.csv,user,eventName,,,aws-cloudtrail-data-anon.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ",| `Load_Sample_Log_Data(AWS CloudTrail)`
,,,"First we pull in our demo SFDC dataset.
Then we filter for what we're looking for in this use case, specifically export EVENT_TYPEs where the CLIENT_NAME is defined.
Then we enrich to convert the SFDC USER_ID into a friendly username via a lookup.",New Applications Contacting SFDC - Demo,New Applications Contacting SFDC - Demo,,CLIENT_NAME,USER_NAME,,,SFDC_Sample_Data_Anon.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| `Load_Sample_Log_Data(""SFDC Data"")` 
| search CLIENT_NAME=* EVENT_TYPE=API OR EVENT_TYPE=BulkAPI OR EVENT_TYPE=RestAPI 
| lookup SFDC_User_Lookup USER_ID"
7,,,"First we bring in our basic demo dataset. In this case, anonymized AWS CloudTrail logs. We're using a macro called Load_Sample_Log_Data to wrap around | inputlookup, just so it is cleaner for the demo data.",Unusual AWS Regions - Demo,Unusual AWS Regions - Demo,,awsRegion,sourcetype,,,aws-cloudtrail-data-anon.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"
| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files 
| eval blah=1, row=""row""
| xyseries row title blah ",| `Load_Sample_Log_Data(AWS CloudTrail)`
,,,"First we pull in our demo dataset.
Then we filter to when the process is one of the standard windows processes, and the parent process is services.exe",New cmd.exe or regedit or powershell launched by services.exe - Demo,New cmd.exe or regedit or powershell launched by services.exe - Demo,,Image,host,,,generic_sysmon_service_launch_logs.csv,1,Must have Demo Lookup,,Verify that lookups installed with Splunk Security Essentials is present,"| rest splunk_server=local /servicesNS/-/-/data//lookup-table-files | eval blah=1, row=""row""| xyseries row title blah ","| `Load_Sample_Log_Data(""Generic Sysmon Process Launches"")` 
| search (Image=*\\cmd.exe OR Image=*\\regedit.exe OR Image=*\\powershell.exe) ParentImage=*\\services.exe"
7,,,"First we bring in our basic demo dataset. In this case, AWS CloudTrail logs filtered for provisioning activities.
Then we summarize to get a count per API and source IP address.
Next we GeoIP to get the country.",AWS Cloud Provisioning Activity from Unusual Country - Live,AWS Cloud Provisioning Activity from Unusual Country - Live,,Country,sourcetype,,,count,1,Must have AWS CloudTrail data,,"In order to run this search, you must have AWS CloudTrail data onboard. Visit the <a href=""/app/Splunk_Security_Essentials/data_source?technology=AWS%20CloudTrail"">data onboarding guide for AWS CloudTrail in this app</a>, or browse to <a href=""https://splunkbase.splunk.com/app/1876/"">apps.splunk.com</a> for more information.",| tstats count where earliest=-2h latest=now index=* sourcetype=aws:cloudtrail,"index=* sourcetype=aws:cloudtrail eventName=Create* OR eventName=Run* OR eventName=Attach* 
|stats count by src_ip eventName 
| iplocation src_ip"
7,,,"First we bring in our basic demo dataset. In this case, AWS CloudTrail logs filtered for provisioning activities.",AWS Cloud Provisioning Activity from Unusual IP - Live,AWS Cloud Provisioning Activity from Unusual IP - Live,,src_ip,sourcetype,,,count,1,Must have AWS CloudTrail data,,"In order to run this search, you must have AWS CloudTrail data onboard. Visit the <a href=""/app/Splunk_Security_Essentials/data_source?technology=AWS%20CloudTrail"">data onboarding guide for AWS CloudTrail in this app</a>, or browse to <a href=""https://splunkbase.splunk.com/app/1876/"">apps.splunk.com</a> for more information.",| tstats count where earliest=-2h latest=now index=* sourcetype=aws:cloudtrail,index=* sourcetype=aws:cloudtrail eventName=Create* OR eventName=Run* OR eventName=Attach*
7,,,"First we bring in our basic dataset of service status reported via WinHostMon (a part of the Universal Forwarder). We also search for where the service doesn't start automatically
Bucket (aliased to bin) allows us to group events based on _time, effectively flattening the actual _time value to the same day.
Stats summarizes the status of across the entire environment for better performance.
Next we look up the host in the GDPR categorization lookup. Because we only care about GDPR hosts for this example, we filter for only the hosts that are in scope for GDPR.",Update Service GDPR - Live,Update Service GDPR - Live,,StartMode,host,,,"count
count","1
1","Must have service status data
Must have Windows Update status data",,"In order to run this search, you must have data about service status. Visit the <a href=""/app/Splunk_Security_Essentials/data_source?technology=Windows%20Security"">data onboarding guide for Windows Security in this app</a>, or browse to <a href=""https://docs.splunk.com/Documentation/Splunk/7.0.2/Data/MonitorWindowshostinformation"">Splunk Docs</a> for more information.
Inside of your service status data, you must have Windows Update listed (wuauserv.exe)","| tstats count where earliest=-2h latest=now index=* (sourcetype=wmi:service OR (sourcetype=winhostmon Service))
| tstats count where earliest=-2h latest=now index=* (sourcetype=wmi:service OR (sourcetype=winhostmon Service)) wuauserv","index=* (sourcetype=wmi:service OR (sourcetype=winhostmon Type=Service) OR (tag=service tag=os))  (tag=update OR Name=wuauserv)  StartMode!=Auto  
| bucket _time span=1d 
| stats latest(Status) as Status latest(StartMode) as StartMode by _time host Name 
| lookup gdpr_system_category.csv host | search category=*"
